{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove JaasModule#zkDigestModule, JaasTestUtils#zkSections and its usage\n\nIt also needs to remove zk related properties.","output":"Resolved","metadata":{"issue_id":"KAFKA-18556","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:54:27.000+0000","updated":"2025-01-16T17:28:46.000+0000","resolved":"2025-01-16T17:28:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove JaasModule#zkDigestModule, JaasTestUtils#zkSections and its usage\n\nIt also needs to remove zk related properties.","output":"Major","metadata":{"issue_id":"KAFKA-18556","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:54:27.000+0000","updated":"2025-01-16T17:28:46.000+0000","resolved":"2025-01-16T17:28:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove JaasModule#zkDigestModule, JaasTestUtils#zkSections and its usage\n\nIt also needs to remove zk related properties.","output":"It also needs to remove zk related properties.","metadata":{"issue_id":"KAFKA-18556","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:54:27.000+0000","updated":"2025-01-16T17:28:46.000+0000","resolved":"2025-01-16T17:28:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove JaasModule#zkDigestModule, JaasTestUtils#zkSections and its usage\n\nIt also needs to remove zk related properties.","output":"Fixed","metadata":{"issue_id":"KAFKA-18556","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:54:27.000+0000","updated":"2025-01-16T17:28:46.000+0000","resolved":"2025-01-16T17:28:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Avoid casting MetadataCache to KRaftMetadataCache","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Resolved","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Major","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Since we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Fixed","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Avoid casting MetadataCache to KRaftMetadataCache\n\nSince we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.c\n\nConversation:\nuser: [https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488\nassistant: Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous z","output":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous z","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Since we remove ZkMetadataCache in https://issues.apache.org/jira/browse/KAFKA-18373. There is only one MetadataCache in code. We can consider to use KRaftMetadataCache directly. ref: https://github.com/apache/kafka/pull/18553#issuecomment-2593078120","output":"Avoid casting MetadataCache to KRaftMetadataCache","metadata":{"issue_id":"KAFKA-18555","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T15:47:54.000+0000","updated":"2025-01-25T15:05:55.000+0000","resolved":"2025-01-25T15:02:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17914435","author":{"displayName":"Chia-Ping Tsai"},"body":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","body_raw":"[https://github.com/apache/kafka/pull/18579] removed some casing already, but partition is still using casting. see https://github.com/apache/kafka/pull/18579#issuecomment-2600820488","created":"2025-01-19T11:34:50.664+0000","updated":"2025-01-19T11:34:50.664+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914458","author":{"displayName":"Ismael Juma"},"body":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","body_raw":"Note that the Kraft implementation adds a bunch of methods that are not usually needed (like mutability, etc). So, it makes sense to keep and use the interface and not to combine them. But there may be cases where the methods only exist in the implementation (and not interface) due to the previous zk implementation. In such cases, we should pull up the relevant methods so that no casting is required.","created":"2025-01-19T15:16:42.584+0000","updated":"2025-01-19T15:17:38.847+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.","output":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.","output":"Resolved","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.","output":"Major","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.","output":"The doc of `advertised.listeners` still contains the statement of zk, we should remove it.","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.","output":"Duplicate","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cleanup the doc of SocketServerConfigs#ADVERTISED_LISTENERS_CONFIG\n\nThe doc of `advertised.listeners` still contains the statement of zk, we should remove it.\n\nConversation:\nuser: it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","output":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","metadata":{"issue_id":"KAFKA-18554","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:29:10.000+0000","updated":"2025-02-21T15:35:24.000+0000","resolved":"2025-01-15T16:55:14.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913376","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","created":"2025-01-15T16:46:57.877+0000","updated":"2025-01-15T16:46:57.877+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update javadoc and comment of ConfigType\n\nThe description of this class is outdated.","output":"Resolved","metadata":{"issue_id":"KAFKA-18553","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:27:03.000+0000","updated":"2025-01-20T14:35:48.000+0000","resolved":"2025-01-20T14:35:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update javadoc and comment of ConfigType\n\nThe description of this class is outdated.","output":"Major","metadata":{"issue_id":"KAFKA-18553","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:27:03.000+0000","updated":"2025-01-20T14:35:48.000+0000","resolved":"2025-01-20T14:35:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update javadoc and comment of ConfigType\n\nThe description of this class is outdated.","output":"The description of this class is outdated.","metadata":{"issue_id":"KAFKA-18553","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:27:03.000+0000","updated":"2025-01-20T14:35:48.000+0000","resolved":"2025-01-20T14:35:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update javadoc and comment of ConfigType\n\nThe description of this class is outdated.","output":"Fixed","metadata":{"issue_id":"KAFKA-18553","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T15:27:03.000+0000","updated":"2025-01-20T14:35:48.000+0000","resolved":"2025-01-20T14:35:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove unnecessary version check from KafkaApisTest\n\ntestHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"Remove unnecessary version check from KafkaApisTest","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove unnecessary version check from KafkaApisTest\n\ntestHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"Resolved","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove unnecessary version check from KafkaApisTest\n\ntestHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"Major","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove unnecessary version check from KafkaApisTest\n\ntestHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"testHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove unnecessary version check from KafkaApisTest\n\ntestHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"Fixed","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"testHandleOffsetFetchWithMultipleGroups and testHandleOffsetFetchWithSingleGroup have version check for v0, but the v0 is removed already from protocol. Hence, we can remove the version check now.","output":"Remove unnecessary version check from KafkaApisTest","metadata":{"issue_id":"KAFKA-18552","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T15:16:59.000+0000","updated":"2025-01-16T08:09:24.000+0000","resolved":"2025-01-16T08:09:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinatorTest.testOutdatedCoordinatorAssignment(ConsumerCoordinatorTest.java:1066) at org.apache.kafka.clients.consumer.internals.EagerConsumerCoordinatorTest.some(EagerConsumerCoordinatorTest.java:29) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)}} ``` [https://ge.apache.org/scans/tests?search.names=CI%20workflow,Git%20repository&searc[…]ientTest&tests.test=testAdminClientApisAuthenticationFailure()|https://ge.apache.org/scans/tests?search.names=CI%20workflow,Git%20repository&search.rootProjectNames=kafka&search.startTimeMax=1736954343491&search.startTimeMin=1734460200000&search.tags=github,trunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI,https:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.admin.KafkaAdminClientTest&tests.test=testAdminClientApisAuthenticationFailure()]","output":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.asse","output":"Resolved","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.asse","output":"Minor","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.asse","output":"Found to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.asse","output":"Duplicate","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()\n\nFound to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuild\n\nConversation:\nuser: https://issues.apache.org/jira/browse/KAFKA-15900","output":"https://issues.apache.org/jira/browse/KAFKA-15900","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Found to be failing infrequently over multiple executions: ``` {{org.opentest4j.AssertionFailedError: Expected :1 Actual :2 at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinatorTest.testOutdatedCoordinatorAssignment(ConsumerCoordinatorTest.java:1066) at org.apache.kafka.clients.consumer.internals.EagerConsum","output":"Flaky test - EagerConsumerCoordinatorTest > testOutdatedCoordinatorAssignment()","metadata":{"issue_id":"KAFKA-18551","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:09:29.000+0000","updated":"2025-01-15T16:32:55.000+0000","resolved":"2025-01-15T16:32:55.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913374","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-15900","body_raw":"https://issues.apache.org/jira/browse/KAFKA-15900","created":"2025-01-15T16:32:55.259+0000","updated":"2025-01-15T16:32:55.259+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at org.apache.kafka.clients.admin.KafkaAdminClientTest.lambda$callAdminClientApisAndExpectAnAuthenticationError$38(KafkaAdminClientTest.java:1808) at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:53) at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:35) at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3115) at org.apache.kafka.clients.admin.KafkaAdminClientTest.callAdminClientApisAndExpectAnAuthenticationError(KafkaAdminClientTest.java:1808) at org.apache.kafka.clients.admin.KafkaAdminClientTest.testAdminClientApisAuthenticationFailure(KafkaAdminClientTest.java:1793)}} ``` https://ge.apache.org/scans/tests?search.names=CI%20workflow,Git%20repository&search.rootProjectNames=kafka&search.startTimeMax=1736954418480&search.startTimeMin=1734460200000&search.tags=github,trunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI,https:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.internals.EagerConsumerCoordinatorTest&tests.test=testOutdatedCoordinatorAssignment()","output":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.ka","output":"Resolved","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.ka","output":"Minor","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.ka","output":"Found the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPa","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.ka","output":"Duplicate","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk\n\nFound the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.\n\nConversation:\nuser: https://issues.apache.org/jira/browse/KAFKA-18441","output":"https://issues.apache.org/jira/browse/KAFKA-18441","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Found the test to break infrequently. Tested over multiple executions ``` {{org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createPartitions at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at org.apache.kafka.clients.admin.KafkaAdminClientTest.lambda$callAdminClientApisAndExpectAnAuthenticationError$38(KafkaAdminClientTest.java:1808) at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:53) at org.junit.","output":"Flaky test - KafkaAdminClientTest > testAdminClientApisAuthenticationFailure() in trunk","metadata":{"issue_id":"KAFKA-18550","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Sushant Mahajan"},"created":"2025-01-15T15:08:16.000+0000","updated":"2025-01-15T16:32:13.000+0000","resolved":"2025-01-15T16:32:13.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913373","author":{"displayName":"Sushant Mahajan"},"body":"https://issues.apache.org/jira/browse/KAFKA-18441","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18441","created":"2025-01-15T16:32:13.474+0000","updated":"2025-01-15T16:32:13.474+0000","updateAuthor":{"displayName":"Sushant Mahajan"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Highlight the CI test timout to avoid we only check the partial failed tests\n\n!螢幕快照 2025-01-15 22-53-30.png!","output":"Open","metadata":{"issue_id":"KAFKA-18549","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T14:55:20.000+0000","updated":"2025-01-15T15:08:52.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913339","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take the issue? Thank you.","body_raw":"Hi [~chia7712], may I take the issue? Thank you.","created":"2025-01-15T14:58:19.271+0000","updated":"2025-01-15T14:58:19.271+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913341","author":{"displayName":"Lianet Magrans"},"body":"Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","body_raw":"Thanks [~chia7712] & [~yangpoan] ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","created":"2025-01-15T15:08:52.928+0000","updated":"2025-01-15T15:08:52.928+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Highlight the CI test timout to avoid we only check the partial failed tests\n\n!螢幕快照 2025-01-15 22-53-30.png!","output":"Major","metadata":{"issue_id":"KAFKA-18549","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T14:55:20.000+0000","updated":"2025-01-15T15:08:52.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913339","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take the issue? Thank you.","body_raw":"Hi [~chia7712], may I take the issue? Thank you.","created":"2025-01-15T14:58:19.271+0000","updated":"2025-01-15T14:58:19.271+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913341","author":{"displayName":"Lianet Magrans"},"body":"Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","body_raw":"Thanks [~chia7712] & [~yangpoan] ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","created":"2025-01-15T15:08:52.928+0000","updated":"2025-01-15T15:08:52.928+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Highlight the CI test timout to avoid we only check the partial failed tests\n\n!螢幕快照 2025-01-15 22-53-30.png!","output":"!螢幕快照 2025-01-15 22-53-30.png!","metadata":{"issue_id":"KAFKA-18549","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T14:55:20.000+0000","updated":"2025-01-15T15:08:52.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913339","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take the issue? Thank you.","body_raw":"Hi [~chia7712], may I take the issue? Thank you.","created":"2025-01-15T14:58:19.271+0000","updated":"2025-01-15T14:58:19.271+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913341","author":{"displayName":"Lianet Magrans"},"body":"Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","body_raw":"Thanks [~chia7712] & [~yangpoan] ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","created":"2025-01-15T15:08:52.928+0000","updated":"2025-01-15T15:08:52.928+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Highlight the CI test timout to avoid we only check the partial failed tests\n\n!螢幕快照 2025-01-15 22-53-30.png!\n\nConversation:\nuser: Hi chia7712, may I take the issue? Thank you.\nassistant: Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","output":"Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","metadata":{"issue_id":"KAFKA-18549","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T14:55:20.000+0000","updated":"2025-01-15T15:08:52.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913339","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take the issue? Thank you.","body_raw":"Hi [~chia7712], may I take the issue? Thank you.","created":"2025-01-15T14:58:19.271+0000","updated":"2025-01-15T14:58:19.271+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913341","author":{"displayName":"Lianet Magrans"},"body":"Thanks chia7712 & yangpoan ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","body_raw":"Thanks [~chia7712] & [~yangpoan] ! This was my mistake recently :S, totally my fault overseeing it but this would be helpful for all (along with keeping the bar we've had of green builds)","created":"2025-01-15T15:08:52.928+0000","updated":"2025-01-15T15:08:52.928+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk","output":"Cleanup KRaftConfig documentation","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk","output":"Resolved","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk","output":"Major","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk","output":"We don't need zk related description anymore since we are removing zk","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk","output":"Duplicate","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cleanup KRaftConfig documentation\n\nWe don't need zk related description anymore since we are removing zk\n\nConversation:\nuser: it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","output":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","metadata":{"issue_id":"KAFKA-18548","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-15T14:53:12.000+0000","updated":"2025-02-21T15:35:33.000+0000","resolved":"2025-01-16T00:48:09.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913380","author":{"displayName":"黃竣陽"},"body":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363","body_raw":"it is dulpicate with https://issues.apache.org/jira/browse/KAFKA-18363\r\n\r\n ","created":"2025-01-15T17:12:12.649+0000","updated":"2025-01-15T17:12:12.649+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"Resolved","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"Blocker","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"This test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testPa","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"Fixed","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk\n\nThis test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.t\n\nConversation:\nuser: Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","output":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This test started failing in last 2 days [https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.startTimeMax=1736951652633&search.startTimeMin=1736722800000&search.tags=trunk&search.timeZoneId=Europe%2FBerlin&tests.container=org.apache.kafka.clients.ClientUtilsTest&tests.test=testParseAndValidateAddressesWithReverseLookup()]","output":"Failing test ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup in trunk","metadata":{"issue_id":"KAFKA-18547","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Divij Vaidya"},"created":"2025-01-15T14:34:52.000+0000","updated":"2025-01-15T15:50:07.000+0000","resolved":"2025-01-15T15:50:07.000+0000","resolution":"Fixed","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"17913332","author":{"displayName":"TaiJuWu"},"body":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","body_raw":"Here is a PR to fix [https://github.com/apache/kafka/pull/18549]","created":"2025-01-15T14:40:54.180+0000","updated":"2025-01-15T14:40:54.180+0000","updateAuthor":{"displayName":"TaiJuWu"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"Resolved","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"Major","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"The case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"Fixed","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test\n\nThe case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.\n\nConversation:\nuser: PR: https://github.com/apache/kafka/pull/18565\nassistant: Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","output":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The case ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup fails when example.com changes domain. It's better to change it as mock test.","output":"Change ClientUtilsTest#testParseAndValidateAddressesWithReverseLookup to mock test","metadata":{"issue_id":"KAFKA-18546","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Bruno Cadonna"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T14:13:24.000+0000","updated":"2025-01-17T13:02:09.000+0000","resolved":"2025-01-16T15:20:47.000+0000","resolution":"Fixed","labels":["newbie"],"components":["unit tests"],"comment_count":2,"comments":[{"id":"17913598","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18565","body_raw":"PR: https://github.com/apache/kafka/pull/18565","created":"2025-01-16T08:05:16.927+0000","updated":"2025-01-16T08:05:16.927+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913767","author":{"displayName":"Divij Vaidya"},"body":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","body_raw":"Marking it for backport to all active branches (3.8, 3.9, 4.0) since we will need to fix the test whenever DNS changes in those branches","created":"2025-01-16T15:03:23.087+0000","updated":"2025-01-16T15:03:23.087+0000","updateAuthor":{"displayName":"Divij Vaidya"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove Zookeeper logic from LogManager\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18545","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T14:03:31.000+0000","updated":"2025-04-15T06:08:06.000+0000","resolved":"2025-02-03T17:20:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17923390","author":{"displayName":"Christo Lolov"},"body":"Reviewed and merged in both trunk and 4.0","body_raw":"Reviewed and merged in both trunk and 4.0","created":"2025-02-03T17:20:03.806+0000","updated":"2025-02-03T17:20:03.806+0000","updateAuthor":{"displayName":"Christo Lolov"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove Zookeeper logic from LogManager\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18545","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T14:03:31.000+0000","updated":"2025-04-15T06:08:06.000+0000","resolved":"2025-02-03T17:20:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17923390","author":{"displayName":"Christo Lolov"},"body":"Reviewed and merged in both trunk and 4.0","body_raw":"Reviewed and merged in both trunk and 4.0","created":"2025-02-03T17:20:03.806+0000","updated":"2025-02-03T17:20:03.806+0000","updateAuthor":{"displayName":"Christo Lolov"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove Zookeeper logic from LogManager\n\n","output":"","metadata":{"issue_id":"KAFKA-18545","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T14:03:31.000+0000","updated":"2025-04-15T06:08:06.000+0000","resolved":"2025-02-03T17:20:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17923390","author":{"displayName":"Christo Lolov"},"body":"Reviewed and merged in both trunk and 4.0","body_raw":"Reviewed and merged in both trunk and 4.0","created":"2025-02-03T17:20:03.806+0000","updated":"2025-02-03T17:20:03.806+0000","updateAuthor":{"displayName":"Christo Lolov"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove Zookeeper logic from LogManager\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18545","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T14:03:31.000+0000","updated":"2025-04-15T06:08:06.000+0000","resolved":"2025-02-03T17:20:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17923390","author":{"displayName":"Christo Lolov"},"body":"Reviewed and merged in both trunk and 4.0","body_raw":"Reviewed and merged in both trunk and 4.0","created":"2025-02-03T17:20:03.806+0000","updated":"2025-02-03T17:20:03.806+0000","updateAuthor":{"displayName":"Christo Lolov"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove Zookeeper logic from LogManager\n\n\n\nConversation:\nuser: Reviewed and merged in both trunk and 4.0","output":"Reviewed and merged in both trunk and 4.0","metadata":{"issue_id":"KAFKA-18545","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T14:03:31.000+0000","updated":"2025-04-15T06:08:06.000+0000","resolved":"2025-02-03T17:20:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17923390","author":{"displayName":"Christo Lolov"},"body":"Reviewed and merged in both trunk and 4.0","body_raw":"Reviewed and merged in both trunk and 4.0","created":"2025-02-03T17:20:03.806+0000","updated":"2025-02-03T17:20:03.806+0000","updateAuthor":{"displayName":"Christo Lolov"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove Zookeeper logic from LogConfig\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18544","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:55:25.000+0000","updated":"2025-01-17T06:45:49.000+0000","resolved":"2025-01-17T00:33:19.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913895","author":{"displayName":"黃竣陽"},"body":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","body_raw":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","created":"2025-01-17T00:33:19.232+0000","updated":"2025-01-17T00:37:47.134+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove Zookeeper logic from LogConfig\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18544","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:55:25.000+0000","updated":"2025-01-17T06:45:49.000+0000","resolved":"2025-01-17T00:33:19.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913895","author":{"displayName":"黃竣陽"},"body":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","body_raw":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","created":"2025-01-17T00:33:19.232+0000","updated":"2025-01-17T00:37:47.134+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove Zookeeper logic from LogConfig\n\n","output":"","metadata":{"issue_id":"KAFKA-18544","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:55:25.000+0000","updated":"2025-01-17T06:45:49.000+0000","resolved":"2025-01-17T00:33:19.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913895","author":{"displayName":"黃竣陽"},"body":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","body_raw":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","created":"2025-01-17T00:33:19.232+0000","updated":"2025-01-17T00:37:47.134+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove Zookeeper logic from LogConfig\n\n","output":"Duplicate","metadata":{"issue_id":"KAFKA-18544","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:55:25.000+0000","updated":"2025-01-17T06:45:49.000+0000","resolved":"2025-01-17T00:33:19.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913895","author":{"displayName":"黃竣陽"},"body":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","body_raw":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","created":"2025-01-17T00:33:19.232+0000","updated":"2025-01-17T00:37:47.134+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove Zookeeper logic from LogConfig\n\n\n\nConversation:\nuser: It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","output":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","metadata":{"issue_id":"KAFKA-18544","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:55:25.000+0000","updated":"2025-01-17T06:45:49.000+0000","resolved":"2025-01-17T00:33:19.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913895","author":{"displayName":"黃竣陽"},"body":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","body_raw":"It will handle by https://issues.apache.org/jira/browse/KAFKA-18499 and [https://github.com/apache/kafka/pull/18583]","created":"2025-01-17T00:33:19.232+0000","updated":"2025-01-17T00:37:47.134+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigException: Invalid url in bootstrap.servers {code} Environment: • Kafka Clients Version: 3.8.1 • Apache Camel Version: 4.9.0 • Spring Boot Version: 3.4.1 • Java Version: 17 We are using Apache Camel to connect to Kafka, and Camel simply passes the Spring Boot auto-configured values (including bootstrap.servers) to Kafka. Camel doesn’t modify the configuration itself. Steps to Reproduce: 1. Define bootstrap.servers in application.yaml using multiline syntax: {code:java} camel: component: kafka: brokers: >- broker1:9092, broker2:9092{code} 2. Use the following setup in logback-spring.xml to configure Kafka Appender: {code:java} bootstrap.servers=${brokers} {code} 3. Run the application. The application fails to start with the following error: _*org.apache.camel.RuntimeCamelException: org.apache.kafka.common.config.ConfigException: Invalid url in bootstrap.servers: broker2:9092*_ • The error occurs due to the stricter validation introduced in KAFKA-17584., which does not allow newline characters or improperly formatted values for bootstrap.servers. • Previously, Kafka Clients 3.7.x seemed to accept multiline configurations more leniently. *Is this stricter validation behavior intentional in Kafka Clients 3.8.x, and is it a permanent change?* We would like to understand whether this behavior is expected or if it could be reconsidered in future Kafka versions to support multiline configurations, as this change has introduced compatibility issues with Camel applications. For users using Camel with Spring Boot, this issue is particularly problematic since the brokers’ list often appears on multiple lines in YAML files for better readability. This could make configuration difficult to manage if forced onto a single line. It would be helpful if Kafka allowed multiline configuration for bootstrap.servers to avoid forcing users to put all brokers on a single line. • Similar issue already reported: KAFKA-18171 • When I updated my YAML configuration to write *bootstrap.servers on a single line* (e.g., broker1:9092,broker2:9092), the issue was resolved. • _Applications that use Spring Kafka directly without Camel are not affected by this issue._ Thank you!","output":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigExcep","output":"Resolved","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigExcep","output":"Major","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigExcep","output":"Hello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked f","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigExcep","output":"Resolved","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations\n\nHello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., \n\nConversation:\nuser: Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0\nassistant: Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","output":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Hello, We encountered an issue when upgrading to *Kafka Clients 3.8.x* (from 3.7.2). The problem occurs when the bootstrap.servers configuration is defined in YAML files using multiline syntax (e.g., using >, or as a list of brokers). Previously, with Kafka Clients 3.7.x, this configuration worked fine even with newlines in the brokers’ addresses, but in Kafka Clients 3.8.x, the validation was made stricter, resulting in the following error: {code:java} org.apache.kafka.common.config.ConfigException: Invalid url in bootstrap.servers {code} Environment: • Kafka Clients Version: 3.8.1 • Apache Camel Version: 4.9.0 • Spring Boot Version: 3.4.1 • Java Version: 17 We are using Apache Camel to connect to Kafka, and Camel simply passes the Spring Boot auto-configured values (including bootstrap.s","output":"Strict validation of bootstrap.servers in Kafka Clients 3.8.x causing issues with multiline configurations","metadata":{"issue_id":"KAFKA-18543","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ming-Yen Chung"},"reporter":{"displayName":"Nadina Florea"},"created":"2025-01-15T13:42:58.000+0000","updated":"2025-02-05T08:34:00.000+0000","resolved":"2025-02-05T08:34:00.000+0000","resolution":"Resolved","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17923664","author":{"displayName":"Ming-Yen Chung"},"body":"Hi nadinaf , this issue is resolved in KAFKA-18171 via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","body_raw":"Hi [~nadinaf] , this issue is resolved in KAFKA-18171  via revert strict config validation in 3.8 and 3.9. However, please use comma separated brokers without newline (>- or |- in yaml), since if space or newline used, if will be fail in version 4.0","created":"2025-02-04T11:38:03.233+0000","updated":"2025-02-05T00:31:54.769+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}},{"id":"17923958","author":{"displayName":"Nadina Florea"},"body":"Hi mingyen066, thanks for your response. I have also followed KAFKA-18171. You can close this issue.","body_raw":"Hi [~mingyen066], thanks for your response. I have also followed KAFKA-18171.\r\n\r\nYou can close this issue.\r\n\r\n ","created":"2025-02-05T08:24:18.397+0000","updated":"2025-02-05T08:24:18.397+0000","updateAuthor":{"displayName":"Nadina Florea"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cleanup AlterPartitionManager\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18542","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:14:13.000+0000","updated":"2025-01-16T11:53:50.000+0000","resolved":"2025-01-16T11:53:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913694","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","body_raw":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","created":"2025-01-16T11:53:50.968+0000","updated":"2025-01-16T11:53:50.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cleanup AlterPartitionManager\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18542","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:14:13.000+0000","updated":"2025-01-16T11:53:50.000+0000","resolved":"2025-01-16T11:53:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913694","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","body_raw":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","created":"2025-01-16T11:53:50.968+0000","updated":"2025-01-16T11:53:50.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cleanup AlterPartitionManager\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18542","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:14:13.000+0000","updated":"2025-01-16T11:53:50.000+0000","resolved":"2025-01-16T11:53:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913694","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","body_raw":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","created":"2025-01-16T11:53:50.968+0000","updated":"2025-01-16T11:53:50.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cleanup AlterPartitionManager\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18542","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:14:13.000+0000","updated":"2025-01-16T11:53:50.000+0000","resolved":"2025-01-16T11:53:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913694","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","body_raw":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","created":"2025-01-16T11:53:50.968+0000","updated":"2025-01-16T11:53:50.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cleanup AlterPartitionManager\n\nas title\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","output":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","metadata":{"issue_id":"KAFKA-18542","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T13:14:13.000+0000","updated":"2025-01-16T11:53:50.000+0000","resolved":"2025-01-16T11:53:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913694","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553] 4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","body_raw":"trunk: [https://github.com/apache/kafka/commit/ce1b079884c5b3b8b0c03dea532548f6b2da2553]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/10b24105d8a9ab0ce6594e2602ea81f661ab2cde","created":"2025-01-16T11:53:50.968+0000","updated":"2025-01-16T11:53:50.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:161) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:632) at org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassCorrectMetricsDynamicInstances(KafkaStreamsTelemetryIntegrationTest.java:323){noformat}","output":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.Asser","output":"Resolved","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.Asser","output":"Major","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.Asser","output":"This test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFai","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.Asser","output":"Fixed","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances\n\nThis test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(Assertion\n\nConversation:\nuser: I'm currently looking into this test.","output":"I'm currently looking into this test.","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This test failed 45 times in last month. Most of failure message is: {noformat} org.opentest4j.AssertionFailedError: expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:161) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:632) at org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassCorrectMetricsDynamicInstances(KafkaStreamsTelemetryIntegrationTest.java:323){noformat}","output":"Fix flaky KafkaStreamsTelemetryIntegrationTest#shouldPassCorrectMetricsDynamicInstances","metadata":{"issue_id":"KAFKA-18541","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T13:00:12.000+0000","updated":"2025-01-27T17:45:26.000+0000","resolved":"2025-01-27T17:45:26.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17913495","author":{"displayName":"Bill Bejeck"},"body":"I'm currently looking into this test.","body_raw":"I'm currently looking into this test.","created":"2025-01-15T23:43:43.090+0000","updated":"2025-01-15T23:43:43.090+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"Resolved","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"Major","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"noted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"Fixed","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs\n\nnoted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also\n\nConversation:\nuser: chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.\nassistant: apoorvmittal10 thanks for taking over this jira!\nassistant: trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","output":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"noted that `KRaftMetadataRequestBenchmark` does not create sharePartitionManager and clientMetricsManager so we need to fix that benchmark in this Jira also","output":"sharePartitionManager and clientMetricsManager should be required to KafkaAPIs","metadata":{"issue_id":"KAFKA-18539","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-15T10:53:50.000+0000","updated":"2025-01-15T20:52:23.000+0000","resolved":"2025-01-15T20:52:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17913260","author":{"displayName":"Apoorv Mittal"},"body":"chia7712 I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","body_raw":"[~chia7712] I have assigned it to myself, let me know if you have already started else will fix this as both areas I have worked on.","created":"2025-01-15T11:07:03.001+0000","updated":"2025-01-15T11:07:03.001+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17913261","author":{"displayName":"Chia-Ping Tsai"},"body":"apoorvmittal10 thanks for taking over this jira!","body_raw":"[~apoorvmittal10]  thanks for taking over this jira!","created":"2025-01-15T11:09:55.808+0000","updated":"2025-01-15T11:09:55.808+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913447","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f 4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","body_raw":"trunk: https://github.com/apache/kafka/commit/3fa998475b263fd475730ef8e4ee04cac14d762f\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8b7b5160f35ffe246b33dd07888a10402969276f","created":"2025-01-15T20:52:23.127+0000","updated":"2025-01-15T20:52:23.127+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214) at kafka.log.remote.RemoteIndexCacheTest.testCleanerThreadShutdown(RemoteIndexCacheTest.scala:333){noformat}","output":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.asse","output":"Resolved","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.asse","output":"Major","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.asse","output":"This case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at o","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.asse","output":"Fixed","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown\n\nThis case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: bu\n\nConversation:\nuser: It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","output":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This case failed 13 times in last month. Most of failure message is like following: {noformat} org.opentest4j.AssertionFailedError: Found unexpected 1 threads=remote-log-index-cleaner ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214) at kafka.log.remote.RemoteIndexCacheTest.testCleanerThreadShutdown(RemoteIndexCacheTest.scala:333){noformat}","output":"Fix flaky RemoteIndexCacheTest#testCleanerThreadShutdown","metadata":{"issue_id":"KAFKA-18537","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"HongYi Chen"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T08:02:59.000+0000","updated":"2025-05-05T17:05:38.000+0000","resolved":"2025-05-05T17:05:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17948961","author":{"displayName":"Chia-Ping Tsai"},"body":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results.","body_raw":"It should be test by TestUtils.waitForCondition as `getAllStackTraces` is a snapshot of threads. Hence, it may return out-of-date results. ","created":"2025-05-02T18:48:58.653+0000","updated":"2025-05-02T18:48:58.653+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle\n\nThis case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214) at kafka.server.ReplicationQuotasTest.shouldMatchQuotaReplicatingThroughAnAsymmetricTopology(ReplicationQuotasTest.scala:183) at kafka.server.ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle(ReplicationQuotasTest.scala:79){noformat}","output":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle\n\nThis case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org","output":"Open","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle\n\nThis case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org","output":"Major","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle\n\nThis case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org","output":"This case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFail","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle\n\nThis case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.As\n\nConversation:\nuser: yangpoan Can I pick this up for fixing the flaky behavior?\nassistant: Yes, feel free to assign to yourself. Thanks.","output":"Yes, feel free to assign to yourself. Thanks.","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This case failed 19 times in last month and most of failure is like following: {noformat} org.opentest4j.AssertionFailedError: Expected 8927 > 9000.0 ==> expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214) at kafka.server.ReplicationQuotasTest.shouldMatchQuotaReplicatingThroughAnAsymmetricTopology(ReplicationQuotasTest.scala:183) at kafka.server.ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle(ReplicationQuotasTest.scala:","output":"Flaky ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle","metadata":{"issue_id":"KAFKA-18536","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:50:00.000+0000","updated":"2025-05-23T01:29:12.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17953573","author":{"displayName":"Varun Upadhyay"},"body":"yangpoan Can I pick this up for fixing the flaky behavior?","body_raw":"[~yangpoan] Can I pick this up for fixing the flaky behavior?","created":"2025-05-23T00:56:20.914+0000","updated":"2025-05-23T00:56:20.914+0000","updateAuthor":{"displayName":"Varun Upadhyay"}},{"id":"17953574","author":{"displayName":"PoAn Yang"},"body":"Yes, feel free to assign to yourself. Thanks.","body_raw":"Yes, feel free to assign to yourself. Thanks.","created":"2025-05-23T01:29:12.548+0000","updated":"2025-05-23T01:29:12.548+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation\n\nThis case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:161) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:632) at app//kafka.api.PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation(PlaintextConsumerPollTest.scala:128) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:197) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base@23.0.1/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:807) at java.base@23.0.1/java.util.stream.ReferencePipeline$7$1FlatMap.accept(ReferencePipeline.java:294) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base@23.0.1/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:570) at java.base@23.0.1/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:560) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base@23.0.1/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:265) at java.base@23.0.1/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:636) at java.base@23.0.1/java.util.stream.ReferencePipeline$7$1FlatMap.accept(ReferencePipeline.java:294) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1709) at java.base@23.0.1/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:570) at java.base@23.0.1/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:560) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base@23.0.1/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:265) at java.base@23.0.1/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:636) at java.base@23.0.1/java.util.stream.ReferencePipeline$7$1FlatMap.accept(ReferencePipeline.java:294) at java.base@23.0.1/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1709) at java.base@23.0.1/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:570) at java.base@23.0.1/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:560) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base@23.0.1/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:265) at java.base@23.0.1/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:636) at java.base@23.0.1/java.util.ArrayList.forEach(ArrayList.java:1597) at java.base@23.0.1/java.util.ArrayList.forEach(ArrayList.java:1597) Suppressed: org.apache.kafka.common.KafkaException: Failed to close kafka consumer at app//org.apache.kafka.clients.consumer.internals.AsyncKafkaConsumer.close(AsyncKafkaConsumer.java:1364) at app//org.apache.kafka.clients.consumer.internals.AsyncKafkaConsumer.close(AsyncKafkaConsumer.java:1251) at app//org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1795) at app//kafka.api.IntegrationTestHarness.$anonfun$tearDown$3(IntegrationTestHarness.scala:243) at app//kafka.api.IntegrationTestHarness.$anonfun$tearDown$3$adapted(IntegrationTestHarness.scala:243) at app//scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619) at app//scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617) at app//scala.collection.AbstractIterable.foreach(Iterable.scala:935) at app//kafka.api.IntegrationTestHarness.tearDown(IntegrationTestHarness.scala:243) ... 45 more Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition topic-0 could be determined{noformat}","output":"Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation","metadata":{"issue_id":"KAFKA-18535","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:41:11.000+0000","updated":"2025-05-17T12:29:49.000+0000","resolved":null,"labels":["integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation\n\nThis case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at app//org.","output":"Open","metadata":{"issue_id":"KAFKA-18535","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:41:11.000+0000","updated":"2025-05-17T12:29:49.000+0000","resolved":null,"labels":["integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation\n\nThis case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at app//org.","output":"Major","metadata":{"issue_id":"KAFKA-18535","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:41:11.000+0000","updated":"2025-05-17T12:29:49.000+0000","resolved":null,"labels":["integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation\n\nThis case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at app//org.","output":"This case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(Asse","metadata":{"issue_id":"KAFKA-18535","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:41:11.000+0000","updated":"2025-05-17T12:29:49.000+0000","resolved":null,"labels":["integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This case failed 6 times with a same error since from Jan 10 2025. {noformat} org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:161) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:632) at app//kafka.api.PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation(PlaintextConsumerPollTest.scala:128) at java.base@23.0.1/java.lang.reflect.","output":"Flaky PlaintextConsumerPollTest.testMaxPollIntervalMsDelayInRevocation","metadata":{"issue_id":"KAFKA-18535","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-15T07:41:11.000+0000","updated":"2025-05-17T12:29:49.000+0000","resolved":null,"labels":["integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/kafka.Kafka /opt/kafka/ 0.0s => ERROR [stage-1 4/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/kraft/reconfig-server.properties /etc/kafka/docker/ 0.0s => ERROR [stage-1 5/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/log4j2.yaml /etc/kafka/docker/ 0.0s => ERROR [stage-1 6/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/tools-log4j2.yaml /etc/kafka/docker/ 0.0s ------ > [stage-1 4/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/kraft/reconfig-server.properties /etc/kafka/docker/: ------ ------ > [stage-1 5/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/log4j2.yaml /etc/kafka/docker/: ------ ------ > [stage-1 6/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/tools-log4j2.yaml /etc/kafka/docker/: ------ Dockerfile:68 -------------------- 66 | COPY --chown=appuser:root --from=build-native-image /app/kafka/config/kraft/reconfig-server.properties /etc/kafka/docker/ 67 | COPY --chown=appuser:root --from=build-native-image /app/kafka/config/log4j2.yaml /etc/kafka/docker/ 68 | >>> COPY --chown=appuser:root --from=build-native-image /app/kafka/config/tools-log4j2.yaml /etc/kafka/docker/ 69 | COPY --chown=appuser:root resources/common-scripts /etc/kafka/docker/ 70 | COPY --chown=appuser:root launch /etc/kafka/docker/ -------------------- ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref e873af20-6895-4203-a36b-f2e96f726025::nmrt29mmksjf1mty1yveutk0q: \"/app/kafka/config/tools-log4j2.yaml\": not found Traceback (most recent call last): File \"/home/tjwu/kafka/docker/common.py\", line 42, in build_docker_image_runner execute(command.split()) File \"/home/tjwu/kafka/docker/common.py\", line 26, in execute raise SystemError(\"Failure in executing following command:- \", \" \".join(command)) SystemError: ('Failure in executing following command:- ', 'docker build -f /tmp/tmpwqticp8w/native/Dockerfile -t kafka/test:3.8.0 --build-arg kafka_url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz --build-arg build_date=2025-01-15 /tmp/tmpwqticp8w/native') During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/tjwu/kafka/docker/docker_build_test.py\", line 79, in build_docker_image(args.image, args.tag, args.kafka_url, args.image_type) File \"/home/tjwu/kafka/docker/docker_build_test.py\", line 46, in build_docker_image build_docker_image_runner(f\"docker build -f $DOCKER_FILE -t \\{image} --build-arg kafka_url=\\{kafka_url} --build-arg build_date=\\{date.today()} $DOCKER_DIR\", image_type) File \"/home/tjwu/kafka/docker/common.py\", line 44, in build_docker_image_runner raise SystemError(\"Docker Image Build failed\") SystemError: Docker Image Build failed ```","output":"docker build fail because log4j","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-","output":"Resolved","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-","output":"Major","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-","output":"cmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.or","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-","output":"Duplicate","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"docker build fail because log4j\n\ncmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-imag\n\nConversation:\nuser: see KAFKA-18340","output":"see KAFKA-18340","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"cmd: python docker_build_test.py kafka/test --image-tag=3.8.0 --image-type=native --kafka-url=https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz The error throw ``` => [build-native-image 5/5] RUN mkdir /app/kafka; microdnf install wget; wget -nv -O kafka.tgz \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0.tgz\"; wget -nv -O kafka.tgz.asc \"https://archive.apache.org/dist/kafka/3.8.0/kafka_2.13-3.8.0 107.3s => CACHED [stage-1 3/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/kafka.Kafka /opt/kafka/ 0.0s => ERROR [stage-1 4/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/kraft/reconfig-server.properties /etc/kafka/docker/ 0.0s => ERROR [stage-1 5/8] COPY --chown=appuser:root --from=build-native-image /app/kafka/config/log4","output":"docker build fail because log4j","metadata":{"issue_id":"KAFKA-18534","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-15T07:00:28.000+0000","updated":"2025-02-21T07:58:04.000+0000","resolved":"2025-02-21T07:58:04.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17929057","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18340","body_raw":"see KAFKA-18340","created":"2025-02-21T07:58:04.433+0000","updated":"2025-02-21T07:58:04.433+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KafkaConfig zookeeper related logic\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18533","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T04:07:50.000+0000","updated":"2025-02-06T05:59:52.000+0000","resolved":"2025-01-25T14:52:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17916964","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","body_raw":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","created":"2025-01-25T20:32:50.251+0000","updated":"2025-01-25T20:32:50.251+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KafkaConfig zookeeper related logic\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18533","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T04:07:50.000+0000","updated":"2025-02-06T05:59:52.000+0000","resolved":"2025-01-25T14:52:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17916964","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","body_raw":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","created":"2025-01-25T20:32:50.251+0000","updated":"2025-01-25T20:32:50.251+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KafkaConfig zookeeper related logic\n\n","output":"","metadata":{"issue_id":"KAFKA-18533","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T04:07:50.000+0000","updated":"2025-02-06T05:59:52.000+0000","resolved":"2025-01-25T14:52:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17916964","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","body_raw":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","created":"2025-01-25T20:32:50.251+0000","updated":"2025-01-25T20:32:50.251+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KafkaConfig zookeeper related logic\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18533","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T04:07:50.000+0000","updated":"2025-02-06T05:59:52.000+0000","resolved":"2025-01-25T14:52:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17916964","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","body_raw":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","created":"2025-01-25T20:32:50.251+0000","updated":"2025-01-25T20:32:50.251+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove KafkaConfig zookeeper related logic\n\n\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","output":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","metadata":{"issue_id":"KAFKA-18533","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T04:07:50.000+0000","updated":"2025-02-06T05:59:52.000+0000","resolved":"2025-01-25T14:52:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17916964","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9 4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","body_raw":"trunk: https://github.com/apache/kafka/commit/c40e7a13414dd1ae9941d9be5d92f3452d971fa9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/3ee4508bb6288fddb3db135f8a2bfd90da5b9027","created":"2025-01-25T20:32:50.251+0000","updated":"2025-01-25T20:32:50.251+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZooKeeperClientException\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18531","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:25:13.000+0000","updated":"2025-01-15T03:50:22.000+0000","resolved":"2025-01-15T03:50:22.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913154","author":{"displayName":"TengYao Chi"},"body":"Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?","body_raw":"Is this ticket a duplicate of [18427|https://issues.apache.org/jira/browse/KAFKA-18427] ?","created":"2025-01-15T03:41:04.890+0000","updated":"2025-01-15T03:41:04.890+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913157","author":{"displayName":"黃竣陽"},"body":"You are right I missed it","body_raw":"You are right I missed it","created":"2025-01-15T03:49:44.201+0000","updated":"2025-01-15T03:49:44.201+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZooKeeperClientException\n\n","output":"Blocker","metadata":{"issue_id":"KAFKA-18531","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:25:13.000+0000","updated":"2025-01-15T03:50:22.000+0000","resolved":"2025-01-15T03:50:22.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913154","author":{"displayName":"TengYao Chi"},"body":"Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?","body_raw":"Is this ticket a duplicate of [18427|https://issues.apache.org/jira/browse/KAFKA-18427] ?","created":"2025-01-15T03:41:04.890+0000","updated":"2025-01-15T03:41:04.890+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913157","author":{"displayName":"黃竣陽"},"body":"You are right I missed it","body_raw":"You are right I missed it","created":"2025-01-15T03:49:44.201+0000","updated":"2025-01-15T03:49:44.201+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZooKeeperClientException\n\n","output":"","metadata":{"issue_id":"KAFKA-18531","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:25:13.000+0000","updated":"2025-01-15T03:50:22.000+0000","resolved":"2025-01-15T03:50:22.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913154","author":{"displayName":"TengYao Chi"},"body":"Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?","body_raw":"Is this ticket a duplicate of [18427|https://issues.apache.org/jira/browse/KAFKA-18427] ?","created":"2025-01-15T03:41:04.890+0000","updated":"2025-01-15T03:41:04.890+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913157","author":{"displayName":"黃竣陽"},"body":"You are right I missed it","body_raw":"You are right I missed it","created":"2025-01-15T03:49:44.201+0000","updated":"2025-01-15T03:49:44.201+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZooKeeperClientException\n\n","output":"Duplicate","metadata":{"issue_id":"KAFKA-18531","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:25:13.000+0000","updated":"2025-01-15T03:50:22.000+0000","resolved":"2025-01-15T03:50:22.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913154","author":{"displayName":"TengYao Chi"},"body":"Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?","body_raw":"Is this ticket a duplicate of [18427|https://issues.apache.org/jira/browse/KAFKA-18427] ?","created":"2025-01-15T03:41:04.890+0000","updated":"2025-01-15T03:41:04.890+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913157","author":{"displayName":"黃竣陽"},"body":"You are right I missed it","body_raw":"You are right I missed it","created":"2025-01-15T03:49:44.201+0000","updated":"2025-01-15T03:49:44.201+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZooKeeperClientException\n\n\n\nConversation:\nuser: Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?\nassistant: You are right I missed it","output":"You are right I missed it","metadata":{"issue_id":"KAFKA-18531","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:25:13.000+0000","updated":"2025-01-15T03:50:22.000+0000","resolved":"2025-01-15T03:50:22.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913154","author":{"displayName":"TengYao Chi"},"body":"Is this ticket a duplicate of https://issues.apache.org/jira/browse/KAFKA-18427 ?","body_raw":"Is this ticket a duplicate of [18427|https://issues.apache.org/jira/browse/KAFKA-18427] ?","created":"2025-01-15T03:41:04.890+0000","updated":"2025-01-15T03:41:04.890+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913157","author":{"displayName":"黃竣陽"},"body":"You are right I missed it","body_raw":"You are right I missed it","created":"2025-01-15T03:49:44.201+0000","updated":"2025-01-15T03:49:44.201+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper","output":"Remove ZooKeeperInternals","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper","output":"Resolved","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper","output":"Blocker","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper","output":"We should remove it, due to we deprecated zookeeper","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper","output":"Fixed","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZooKeeperInternals\n\nWe should remove it, due to we deprecated zookeeper\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","output":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","metadata":{"issue_id":"KAFKA-18530","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T03:14:16.000+0000","updated":"2025-02-06T11:00:44.000+0000","resolved":"2025-02-06T11:00:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924487","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd 4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","body_raw":"trunk: https://github.com/apache/kafka/commit/a3d9d881e1c0c7fe47962cb42b7a19111b614fdd\r\n \r\n4.0: https://github.com/apache/kafka/commit/cf8d3ac49e5c87be6c91d3f14cb4aa3f7776fba1","created":"2025-02-06T11:00:44.038+0000","updated":"2025-02-06T11:00:44.038+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"ConsumerRebootstrapTest should run for async consumer","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Blocker","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Fixed","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"ConsumerRebootstrapTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.\n\nConversation:\nuser: Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","output":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"ConsumerRebootstrapTest should run for async consumer","metadata":{"issue_id":"KAFKA-18529","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:37:40.000+0000","updated":"2025-01-30T16:02:22.000+0000","resolved":"2025-01-24T19:35:47.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17922416","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","body_raw":"Merged to trunk, left for 4.1 (because it requires KAFKA-17986 that is not in 4.0)","created":"2025-01-30T16:02:22.121+0000","updated":"2025-01-30T16:02:22.121+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"MultipleListenersWithSameSecurityProtocolBaseTest and GssapiAuthenticationTest should run for async consumer\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18528","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:36:14.000+0000","updated":"2025-01-28T15:21:45.000+0000","resolved":"2025-01-27T20:51:55.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17921772","author":{"displayName":"Lianet Magrans"},"body":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","body_raw":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","created":"2025-01-28T15:21:45.825+0000","updated":"2025-01-28T15:21:45.825+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"MultipleListenersWithSameSecurityProtocolBaseTest and GssapiAuthenticationTest should run for async consumer\n\n","output":"Blocker","metadata":{"issue_id":"KAFKA-18528","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:36:14.000+0000","updated":"2025-01-28T15:21:45.000+0000","resolved":"2025-01-27T20:51:55.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17921772","author":{"displayName":"Lianet Magrans"},"body":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","body_raw":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","created":"2025-01-28T15:21:45.825+0000","updated":"2025-01-28T15:21:45.825+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: MultipleListenersWithSameSecurityProtocolBaseTest and GssapiAuthenticationTest should run for async consumer\n\n","output":"","metadata":{"issue_id":"KAFKA-18528","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:36:14.000+0000","updated":"2025-01-28T15:21:45.000+0000","resolved":"2025-01-27T20:51:55.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17921772","author":{"displayName":"Lianet Magrans"},"body":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","body_raw":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","created":"2025-01-28T15:21:45.825+0000","updated":"2025-01-28T15:21:45.825+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"MultipleListenersWithSameSecurityProtocolBaseTest and GssapiAuthenticationTest should run for async consumer\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18528","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:36:14.000+0000","updated":"2025-01-28T15:21:45.000+0000","resolved":"2025-01-27T20:51:55.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17921772","author":{"displayName":"Lianet Magrans"},"body":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","body_raw":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","created":"2025-01-28T15:21:45.825+0000","updated":"2025-01-28T15:21:45.825+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"MultipleListenersWithSameSecurityProtocolBaseTest and GssapiAuthenticationTest should run for async consumer\n\n\n\nConversation:\nuser: Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","output":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","metadata":{"issue_id":"KAFKA-18528","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:36:14.000+0000","updated":"2025-01-28T15:21:45.000+0000","resolved":"2025-01-27T20:51:55.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17921772","author":{"displayName":"Lianet Magrans"},"body":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","body_raw":"Cherry-picked to 4.0 [https://github.com/apache/kafka/commit/aeb9acc7ef6053b0f940b210eaf3ffaa078ba8d4] (validated all enabled tests pass in 4.0)","created":"2025-01-28T15:21:45.825+0000","updated":"2025-01-28T15:21:45.825+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Blocker","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Duplicate","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.\n\nConversation:\nuser: Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after au\nassistant: Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better\nassistant: Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the","output":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"SaslClientsWithInvalidCredentialsTest.java and SaslClientsWithInvalidCredentialsTest.scala should run for async consumer","metadata":{"issue_id":"KAFKA-18527","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:32:55.000+0000","updated":"2025-02-21T15:35:40.000+0000","resolved":"2025-01-28T16:49:32.000+0000","resolution":"Duplicate","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914130","author":{"displayName":"TengYao Chi"},"body":"Hi lianetm , Some tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. While authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs. I found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue https://issues.apache.org/jira/browse/KAFKA-18517. Is this behavior expected? If I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","body_raw":"Hi [~lianetm] ,\r\n\r\nSome tests in SaslClientsWithInvalidCredentialsTest.scala are failing after enabling KRaft. \r\nWhile authentication failures are expected in these tests, consumer#poll doesn't return until the test timeout occurs.\r\n\r\nI found that the consumer keeps attempting to find coordinator even after authentication failure, which is similar to issue [18157|https://issues.apache.org/jira/browse/KAFKA-18517]. Is this behavior expected?\r\n\r\nIf I understand correctly, an authentication failure should be considered a fatal error, but it seems the new consumer doesn't have a related mechanism to handle this scenario.","created":"2025-01-17T14:34:02.737+0000","updated":"2025-01-17T14:35:27.612+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17915839","author":{"displayName":"Lianet Magrans"},"body":"Hey frankvicky , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.","body_raw":"Hey [~frankvicky] , this looks interesting, could be indeed a similar gap. I expect the auth failure should come out either from the FindCoord request or from Metadata requests in this case (I guess those are the 2 requests being generated here). Let me take a closer look at the failing tests to better understand and I will update back here asap.  ","created":"2025-01-21T21:37:10.418+0000","updated":"2025-01-21T21:37:10.418+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916074","author":{"displayName":"Lianet Magrans"},"body":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619 Simple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","body_raw":"Hey, looking into it I found a bug https://issues.apache.org/jira/browse/KAFKA-18619\r\n\r\nSimple fix but we should validate that the tests pass repeatedly (just in case there is something else behind it). Maybe we can address KAFKA-18619 in a separate PR with the fix and enabling the tests (and leave the PR you have here only for the .java file). Please feel free to take 18619 if you want, if not I can take it. Hope it helps!","created":"2025-01-22T14:56:49.952+0000","updated":"2025-01-22T14:56:49.952+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"CustomQuotaCallbackTest should run for async consumer","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Blocker","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Not A Problem","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"CustomQuotaCallbackTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.\n\nConversation:\nuser: m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.\nassistant: I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?\nassistant: Make sense to me.","output":"Make sense to me.","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"CustomQuotaCallbackTest should run for async consumer","metadata":{"issue_id":"KAFKA-18526","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:30:03.000+0000","updated":"2025-02-21T15:35:48.000+0000","resolved":"2025-01-21T14:01:57.000+0000","resolution":"Not A Problem","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":3,"comments":[{"id":"17914980","author":{"displayName":"David Jacot"},"body":"m1a2st Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","body_raw":"[~m1a2st] Is there real value in doing this? In other words, does this test really test something specific to the consumer implementation? It seems to me that it is only about testing the server side of things but I may be wrong.","created":"2025-01-21T13:45:47.777+0000","updated":"2025-01-21T13:45:47.777+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915315","author":{"displayName":"Lianet Magrans"},"body":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense m1a2st ?","body_raw":"I agree that this one does not seem to rely on any consumer-side behaviour (unlike other tests under the same umbrella that verify consumer behaviour on shutdown, for instance). I would resolve with \"Not a problem\", makes sense [~m1a2st] ?","created":"2025-01-21T13:59:33.604+0000","updated":"2025-01-21T13:59:33.604+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17915643","author":{"displayName":"黃竣陽"},"body":"Make sense to me.","body_raw":"Make sense to me.","created":"2025-01-21T14:01:14.694+0000","updated":"2025-01-21T14:01:14.694+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Update the Test should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Update the Test should run for async consumer","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update the Test should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update the Test should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Blocker","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update the Test should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update the Test should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Fixed","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both consumers.","output":"Update the Test should run for async consumer","metadata":{"issue_id":"KAFKA-18525","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-15T00:23:51.000+0000","updated":"2025-01-28T16:51:12.000+0000","resolved":"2025-01-28T16:51:12.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511)","output":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/j","output":"Resolved","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/j","output":"Major","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/j","output":"org.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyE","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/j","output":"Duplicate","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache\n\norg.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.ju\n\nConversation:\nuser: I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089\nassistant: Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","output":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"org.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511)","output":"Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache","metadata":{"issue_id":"KAFKA-18524","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-14T20:51:20.000+0000","updated":"2025-07-09T12:46:58.000+0000","resolved":"2025-02-18T16:29:21.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17913696","author":{"displayName":"TaiJuWu"},"body":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","body_raw":"I believe this is similar issue with https://issues.apache.org/jira/browse/KAFKA-18089","created":"2025-01-16T12:07:07.205+0000","updated":"2025-01-16T12:07:07.205+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"18004141","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:46:58.574+0000","updated":"2025-07-09T12:46:58.574+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Improve default.replication.factor docs\n\nWe had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"controller nodes\" in KRaft. This is not clear from the docs. From the user: {quote}Although the config setting {{default.replication.factor}} is listed under \"Broker configuration\" in the Kafka docs, digging in the Kafka source code revealed that the {{-1}} value is actually mapped on the controllers, not the \"normal\" brokers. Adding {{default.replication.factor}} to the controllers as well, resolved the problem. {quote} Would be great to improve the docs accordingly. Not sure if this might apply to other configs, too.","output":"Improve default.replication.factor docs","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Improve default.replication.factor docs\n\nWe had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"controller nodes\" in KRaft. This is not clear from the docs. From the user: {quote}Although the config setting {{default.replication.factor}} is listed under \"Broker configuration\" in the Kafka docs, diggin","output":"Open","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Improve default.replication.factor docs\n\nWe had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"controller nodes\" in KRaft. This is not clear from the docs. From the user: {quote}Although the config setting {{default.replication.factor}} is listed under \"Broker configuration\" in the Kafka docs, diggin","output":"Minor","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Improve default.replication.factor docs\n\nWe had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"controller nodes\" in KRaft. This is not clear from the docs. From the user: {quote}Although the config setting {{default.replication.factor}} is listed under \"Broker configuration\" in the Kafka docs, diggin","output":"We had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"contro","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Improve default.replication.factor docs\n\nWe had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating i\n\nConversation:\nuser: Hi mjsax I would take a look 😺","output":"Hi mjsax I would take a look 😺","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We had a user report, that the `default.replication.factor` was not picked up correctly by a KafkaStreams app, which by default uses `-1` as client side config for `replication.factor` when creating internal topics. Turns out, that there seems to be a difference between \"regular brokers\" and \"controller nodes\" in KRaft. This is not clear from the docs. From the user: {quote}Although the config setting {{default.replication.factor}} is listed under \"Broker configuration\" in the Kafka docs, digging in the Kafka source code revealed that the {{-1}} value is actually mapped on the controllers, not the \"normal\" brokers. Adding {{default.replication.factor}} to the controllers as well, resolved the problem. {quote} Would be great to improve the docs accordingly. Not sure if this might apply to o","output":"Improve default.replication.factor docs","metadata":{"issue_id":"KAFKA-18523","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-14T20:39:18.000+0000","updated":"2025-01-14T23:54:13.000+0000","resolved":null,"labels":[],"components":["docs"],"comment_count":1,"comments":[{"id":"17913108","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take a look 😺","body_raw":"Hi [~mjsax] \r\n\r\nI would take a look 😺","created":"2025-01-14T23:54:13.246+0000","updated":"2025-01-14T23:54:13.246+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"Limit fetch records batches as per acquired records","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"Resolved","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"Major","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"Fixed","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Limit fetch records batches as per acquired records\n\n[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321\n\nConversation:\nuser: junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `rec\nassistant: apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.\nassistant: Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other chec\nassistant: apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords\nassistant: Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","output":"The first PR is merged and for MemoryRecords we have separate task.","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/pull/17322#discussion_r1803823292] https://github.com/apache/kafka/pull/18459/files#r1913905321","output":"Limit fetch records batches as per acquired records","metadata":{"issue_id":"KAFKA-18522","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-14T16:37:14.000+0000","updated":"2025-02-28T13:36:50.000+0000","resolved":"2025-02-28T13:36:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17916761","author":{"displayName":"Apoorv Mittal"},"body":"junrao I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","body_raw":"[~junrao] I have created this taks based on the discussion we had on the PRs where there might be lesser number of acquired records but the `records` response can have all fetched records data. But I need your help to undersatnd a doubt: on taking another look it seems the ShareFetchResponse holds `records` (FileRecords) which are wired and client transform them for usage. Is there a way I can subset the FileRecords as per acquired records without going away with zero copy buffer in server, prior sending to client?","created":"2025-01-24T15:36:28.597+0000","updated":"2025-01-24T15:36:28.597+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17916806","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice.","body_raw":"[~apoorvmittal10] : Yes, FileRecords just points to a byte range in a file channel. Once we figure out the actual size of data that need to be returned, we can take a slice of FileRecords with a new end. See FileRecords.slice. ","created":"2025-01-24T17:42:57.891+0000","updated":"2025-01-24T17:42:57.891+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922341","author":{"displayName":"Apoorv Mittal"},"body":"Hi junrao, thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where ` currentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing? !image-2025-01-30-11-53-46-496.png!","body_raw":"Hi [~junrao], thanks for the help. I looked into the code and started working on it. One query I have regarding below check, where `\r\ncurrentSizeInBytes` is lesser than `start` hence for any non-negative start position this check will error out. Also position cannot be non-negative as per the other check in the method. What am I missing?\r\n \r\n\r\n!image-2025-01-30-11-53-46-496.png!","created":"2025-01-30T11:55:18.639+0000","updated":"2025-01-30T11:55:18.639+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17922512","author":{"displayName":"Jun Rao"},"body":"apoorvmittal10 : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced FileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","body_raw":"[~apoorvmittal10] : Thanks for finding this. I think this is a bug introduced in [https://github.com/apache/kafka/pull/5332/files.] So far, the bug is not exposed yet since we only create a sliced \r\nFileRecords from an unsliced FileRecords, whose start is always 0. This will break if a sliced FileRecords can be created from another sliced FileRecords.","created":"2025-01-30T19:44:31.904+0000","updated":"2025-01-30T19:44:31.904+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17922550","author":{"displayName":"Apoorv Mittal"},"body":"Thanks for confirming junrao, I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","body_raw":"Thanks for confirming [~junrao], I have created following Jira for same https://issues.apache.org/jira/browse/KAFKA-18683 and working on it.","created":"2025-01-31T00:48:59.193+0000","updated":"2025-01-31T00:48:59.193+0000","updateAuthor":{"displayName":"Apoorv Mittal"}},{"id":"17931473","author":{"displayName":"Apoorv Mittal"},"body":"The first PR is merged and for MemoryRecords we have separate task.","body_raw":"The first PR is merged and for MemoryRecords we have separate task.","created":"2025-02-28T13:36:50.167+0000","updated":"2025-02-28T13:36:50.167+0000","updateAuthor":{"displayName":"Apoorv Mittal"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"Cleanup NodeApiVersions zkMigrationEnabled field","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"Resolved","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"Major","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"We remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"Fixed","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cleanup NodeApiVersions zkMigrationEnabled field\n\nWe remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","output":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We remove the `reasonAllControllersZkMigrationNotReady` in https://issues.apache.org/jira/browse/KAFKA-18136, thus we didn't use this field in code base, we should remove it","output":"Cleanup NodeApiVersions zkMigrationEnabled field","metadata":{"issue_id":"KAFKA-18521","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T16:23:38.000+0000","updated":"2025-01-16T12:08:55.000+0000","resolved":"2025-01-16T12:08:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913697","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee] 4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","body_raw":"trunk: [https://github.com/apache/kafka/commit/3c1f965c60789dcc8ee14ebabcbb4e16ebffc5ee]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9ffa0befc7f8a5df5b257697d867508ece94a0e3","created":"2025-01-16T12:08:48.754+0000","updated":"2025-01-16T12:08:48.754+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove Json.scala , cleanup AclEntry.scala\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"Remove Json.scala , cleanup AclEntry.scala","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove Json.scala , cleanup AclEntry.scala\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"Resolved","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove Json.scala , cleanup AclEntry.scala\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"Minor","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove Json.scala , cleanup AclEntry.scala\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove Json.scala , cleanup AclEntry.scala\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"Fixed","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Json.scala] This class only use in ZkData and DelegationTokenManagerZk, I could remove this tool after we remove these class","output":"Remove Json.scala , cleanup AclEntry.scala","metadata":{"issue_id":"KAFKA-18519","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T15:35:33.000+0000","updated":"2025-01-22T15:24:52.000+0000","resolved":"2025-01-22T15:24:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"ConsumerBounceTest should run for async consumer","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Blocker","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"Fixed","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"ConsumerBounceTest should run for async consumer\n\nAll tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.\n\nConversation:\nuser: Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?\nassistant: There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest\nassistant: Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apach\nassistant: I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?\nassistant: Sounds great, thanks! Keep me in the loop to help if you need.","output":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"All tests in the file are parametrized to run for Classic only, so they won't run with the new async consumer. We should enabled them so that they run both both consumers.","output":"ConsumerBounceTest should run for async consumer","metadata":{"issue_id":"KAFKA-18517","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-14T13:55:01.000+0000","updated":"2025-01-22T17:16:11.000+0000","resolved":"2025-01-22T17:04:33.000+0000","resolution":"Fixed","labels":["kip-848-client-support"],"components":["clients","consumer"],"comment_count":6,"comments":[{"id":"17912913","author":{"displayName":"黃竣陽"},"body":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","body_raw":"Should we identify all tests that are currently running only the classic consumer now and update them to run both consumer?","created":"2025-01-14T14:00:49.702+0000","updated":"2025-01-14T14:00:49.702+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912915","author":{"displayName":"黃竣陽"},"body":"There are the test classes, which have some test cases only test the classic protocol - SaslClientsWithInvalidCredentialsTest - ConsumerBounceTest - PlaintextAdminIntegrationTest - GssapiAuthenticationTest - MultipleListenersWithSameSecurityProtocolBaseTest - SaslClientsWithInvalidCredentialsTest","body_raw":"There are the test classes, which have some test cases only test the classic protocol\r\n- SaslClientsWithInvalidCredentialsTest\r\n- ConsumerBounceTest\r\n- PlaintextAdminIntegrationTest\r\n- GssapiAuthenticationTest\r\n- MultipleListenersWithSameSecurityProtocolBaseTest\r\n- SaslClientsWithInvalidCredentialsTest","created":"2025-01-14T14:04:04.351+0000","updated":"2025-01-14T14:04:58.857+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912982","author":{"displayName":"Lianet Magrans"},"body":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] ) Other than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","body_raw":"Hey, thanks for taking a look. We definitely have to make sure that whatever is running for the CLASSIC only, is indeed related to something that is not supported for the new consumer/protocol. (ie. tests related to client-side assignors are supposed to run only for classic [https://github.com/apache/kafka/blob/81938eb0864436bce63aadfd0fb3fdfe0bc71779/core/src/test/scala/integration/kafka/api/PlaintextConsumerAssignorsTest.scala#L36-L39] )\r\n\r\nOther than that, whatever we find that should run for both and it's not, we should attempt to enable it (I just opened a PR for the ConsumerBounceTest btw). Do you want to work on this? I think we should start by triaging those you already identify (attempt to run them for both doing any test config change required like in the ConsumerBounceTest. If bigger issues arise we file a Jira to investigate further). Let me know if you want to take it and I can help (or I'll drive the triage if you don't have bandwidth). Thanks!","created":"2025-01-14T17:07:24.667+0000","updated":"2025-01-14T17:07:24.667+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912989","author":{"displayName":"黃竣陽"},"body":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","body_raw":"I think I can create a new Jira ticket as an umbrella ticket. I will create subtasks for each class I've listed, allowing us to verify each test separately. It make sense?","created":"2025-01-14T17:26:03.746+0000","updated":"2025-01-14T17:26:03.746+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17913060","author":{"displayName":"Lianet Magrans"},"body":"Sounds great, thanks! Keep me in the loop to help if you need.","body_raw":"Sounds great, thanks! Keep me in the loop to help if you need. ","created":"2025-01-14T20:06:33.163+0000","updated":"2025-01-14T20:06:33.163+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17916120","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/fd8556b4b1262c72c466b02535260fb8bb74e556] ","created":"2025-01-22T17:16:11.242+0000","updated":"2025-01-22T17:16:11.242+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","output":"Remove RackAwareMode","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","output":"Resolved","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","output":"Major","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","output":"It only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper","output":"Fixed","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove RackAwareMode\n\nIt only use in `AdminZkClient`, we should remove it when we deprecated zookeeper\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","output":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","metadata":{"issue_id":"KAFKA-18516","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:36:10.000+0000","updated":"2025-01-19T10:52:29.000+0000","resolved":"2025-01-19T10:52:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914431","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889] 4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","body_raw":"trunk: [https://github.com/apache/kafka/commit/eb1c8419aa8b90a187821f318ba21d048661d889]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cfa481d6f72dd994d8045c3c5810d4264106e366","created":"2025-01-19T10:52:29.210+0000","updated":"2025-01-19T10:52:29.210+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove DelegationTokenManagerZk\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18515","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:30:43.000+0000","updated":"2025-01-18T17:48:19.000+0000","resolved":"2025-01-18T17:48:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914356","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","body_raw":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","created":"2025-01-18T17:48:19.408+0000","updated":"2025-01-18T17:48:19.408+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove DelegationTokenManagerZk\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18515","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:30:43.000+0000","updated":"2025-01-18T17:48:19.000+0000","resolved":"2025-01-18T17:48:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914356","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","body_raw":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","created":"2025-01-18T17:48:19.408+0000","updated":"2025-01-18T17:48:19.408+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove DelegationTokenManagerZk\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18515","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:30:43.000+0000","updated":"2025-01-18T17:48:19.000+0000","resolved":"2025-01-18T17:48:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914356","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","body_raw":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","created":"2025-01-18T17:48:19.408+0000","updated":"2025-01-18T17:48:19.408+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove DelegationTokenManagerZk\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18515","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:30:43.000+0000","updated":"2025-01-18T17:48:19.000+0000","resolved":"2025-01-18T17:48:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914356","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","body_raw":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","created":"2025-01-18T17:48:19.408+0000","updated":"2025-01-18T17:48:19.408+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove DelegationTokenManagerZk\n\nas title\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","output":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","metadata":{"issue_id":"KAFKA-18515","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-14T13:30:43.000+0000","updated":"2025-01-18T17:48:19.000+0000","resolved":"2025-01-18T17:48:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914356","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9 4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","body_raw":"trunk: https://github.com/apache/kafka/commit/a6faec179a3685d66698a2dc3c6a1823bd0c87f9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/bce4b6a1775166d5a9364453a2931f2dc13e39c6","created":"2025-01-18T17:48:19.408+0000","updated":"2025-01-18T17:48:19.408+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ReplicaStateMachine\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"Remove ReplicaStateMachine","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ReplicaStateMachine\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ReplicaStateMachine\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"Major","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ReplicaStateMachine\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ReplicaStateMachine\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala","output":"Remove ReplicaStateMachine","metadata":{"issue_id":"KAFKA-18512","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T07:00:23.000+0000","updated":"2025-01-17T03:09:53.000+0000","resolved":"2025-01-17T03:09:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Remove ControllerChannelManager","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Major","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Remove ControllerChannelManager","metadata":{"issue_id":"KAFKA-18511","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:58:33.000+0000","updated":"2025-01-17T03:25:03.000+0000","resolved":"2025-01-17T03:25:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerChannelContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"Remove ControllerChannelContext","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerChannelContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerChannelContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"Major","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerChannelContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerChannelContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelContext.scala","output":"Remove ControllerChannelContext","metadata":{"issue_id":"KAFKA-18510","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:54:52.000+0000","updated":"2025-01-17T03:24:41.000+0000","resolved":"2025-01-17T03:24:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"Move StateChangeLogger to server-common module","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"Resolved","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"Major","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"Fixed","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Move StateChangeLogger to server-common module\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We \n\nConversation:\nuser: What's the goal of this ticket? Could we please update the description.\nassistant: Hi dajac Thanks for the reminder. I have just updated the description.\nassistant: Changing the fix version to 4.1 as this is not required for 4.0.\nassistant: Moving to the next release as we're now in code freeze for 4.1.0.\nassistant: After discussion with frankvicky, I will handle this issue.","output":"After discussion with frankvicky, I will handle this issue.","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/StateChangeLogger.scala] Since we are removing zk, many lots methods of StateChangeLogger have no longer been used. We should clean them.","output":"Move StateChangeLogger to server-common module","metadata":{"issue_id":"KAFKA-18509","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:51:54.000+0000","updated":"2025-10-09T00:22:46.000+0000","resolved":"2025-10-06T14:57:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17914982","author":{"displayName":"David Jacot"},"body":"What's the goal of this ticket? Could we please update the description.","body_raw":"What's the goal of this ticket? Could we please update the description.","created":"2025-01-21T13:48:40.613+0000","updated":"2025-01-21T13:48:40.613+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17915724","author":{"displayName":"TengYao Chi"},"body":"Hi dajac Thanks for the reminder. I have just updated the description.","body_raw":"Hi [~dajac] \r\n\r\nThanks for the reminder.\r\nI have just updated the description.","created":"2025-01-21T15:01:28.534+0000","updated":"2025-01-21T15:01:28.534+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17922367","author":{"displayName":"David Jacot"},"body":"Changing the fix version to 4.1 as this is not required for 4.0.","body_raw":"Changing the fix version to 4.1 as this is not required for 4.0.","created":"2025-01-30T12:53:55.726+0000","updated":"2025-01-30T12:53:55.726+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17986067","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:37:54.883+0000","updated":"2025-06-25T09:37:54.883+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"18024687","author":{"displayName":"黃竣陽"},"body":"After discussion with frankvicky, I will handle this issue.","body_raw":"After discussion with [~frankvicky], I will handle this issue.","created":"2025-10-04T15:09:16.208+0000","updated":"2025-10-04T15:09:16.208+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"Remove ControllerContext","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"Major","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerContext\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala","output":"Remove ControllerContext","metadata":{"issue_id":"KAFKA-18508","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:48:41.000+0000","updated":"2025-02-06T16:22:35.000+0000","resolved":"2025-01-17T03:26:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Remove ControllerChannelManager","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Major","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerChannelManager\n\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Duplicate","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerChannelManager.scala","output":"Remove ControllerChannelManager","metadata":{"issue_id":"KAFKA-18507","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:47:34.000+0000","updated":"2025-02-21T15:36:03.000+0000","resolved":"2025-01-14T07:20:28.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerState\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"Remove ControllerState","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerState\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerState\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"Major","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerState\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerState\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerState.scala","output":"Remove ControllerState","metadata":{"issue_id":"KAFKA-18506","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:45:03.000+0000","updated":"2025-01-17T03:10:06.000+0000","resolved":"2025-01-17T03:10:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ControllerEventManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"Remove ControllerEventManager","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ControllerEventManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ControllerEventManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"Major","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ControllerEventManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ControllerEventManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/ControllerEventManager.scala","output":"Remove ControllerEventManager","metadata":{"issue_id":"KAFKA-18505","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:44:18.000+0000","updated":"2025-01-17T03:09:25.000+0000","resolved":"2025-01-17T03:09:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove PartitionStateMachine\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"Remove PartitionStateMachine","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove PartitionStateMachine\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove PartitionStateMachine\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"Major","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove PartitionStateMachine\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove PartitionStateMachine\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/PartitionStateMachine.scala","output":"Remove PartitionStateMachine","metadata":{"issue_id":"KAFKA-18504","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:43:31.000+0000","updated":"2025-01-17T03:07:39.000+0000","resolved":"2025-01-17T03:07:39.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove TopicDeletionManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"Remove TopicDeletionManager","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove TopicDeletionManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove TopicDeletionManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"Major","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove TopicDeletionManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove TopicDeletionManager\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/TopicDeletionManager.scala","output":"Remove TopicDeletionManager","metadata":{"issue_id":"KAFKA-18503","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:42:07.000+0000","updated":"2025-01-17T03:06:37.000+0000","resolved":"2025-01-17T03:06:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"Remove kafka.controller.Election","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"Resolved","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"Major","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"Fixed","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove kafka.controller.Election\n\nhttps://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","output":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/controller/Election.scala","output":"Remove kafka.controller.Election","metadata":{"issue_id":"KAFKA-18502","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-14T06:41:11.000+0000","updated":"2025-01-14T21:45:29.000+0000","resolved":"2025-01-14T21:45:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913079","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514] 4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","body_raw":"trunk: [https://github.com/apache/kafka/commit/118e1835cc58fe4518df5e49e6eac5ecb97e0514]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9203c6a5cf6e0a0b8c0677a36efac8610f3b4090","created":"2025-01-14T21:45:29.376+0000","updated":"2025-01-14T21:45:29.376+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:197) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1709) at java.base@23.0.1/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:807) at java.base@23.0.1/java.util.stream.ReferencePipeline$7$1FlatMap.accept(ReferencePipeline.java:294) at java.base@23.0.1/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1709) at java.base@23.0.1/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:570) at java.base@23.0.1/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:560) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base@23.0.1/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:265) at java.base@23.0.1/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:636) at java.base@23.0.1/java.util.ArrayList.forEach(ArrayList.java:1597) at java.base@23.0.1/java.util.ArrayList.forEach(ArrayList.java:1597) {code}","output":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.Referen","output":"Resolved","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.Referen","output":"Major","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.Referen","output":"Thread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.Referen","output":"Duplicate","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests\n\nThread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controll\n\nConversation:\nuser: this is resolved by https://github.com/apache/kafka/pull/20053","output":"this is resolved by https://github.com/apache/kafka/pull/20053","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Thread leak detected in this PR: * https://github.com/apache/kafka/actions/runs/12753159081/job/35544671788#step:11:1451 {code:java} org.opentest4j.AssertionFailedError: Thread leak detected: controller-0-ThrottledChannelReaper-Produce ==> expected: but was: at app//org.apache.kafka.common.test.api.ClusterTestExtensions.afterEach(ClusterTestExtensions.java:158) at java.base@23.0.1/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:197) at java.base@23.0.1/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) at java.base@23.0.1/java.util.ArrayList$ArrayListSpliterator.f","output":"Fix flaky ClientQuotasRequestTest.testAlterClientQuotasInvalidRequests","metadata":{"issue_id":"KAFKA-18501","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-14T04:10:09.000+0000","updated":"2025-09-13T23:21:18.000+0000","resolved":"2025-09-13T23:21:18.000+0000","resolution":"Duplicate","labels":[],"components":["unit tests"],"comment_count":1,"comments":[{"id":"18020083","author":{"displayName":"Chia-Ping Tsai"},"body":"this is resolved by https://github.com/apache/kafka/pull/20053","body_raw":"this is resolved by https://github.com/apache/kafka/pull/20053","created":"2025-09-13T23:21:18.809+0000","updated":"2025-09-13T23:21:18.809+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Pull Requests not fully leveraging cache\n\nI noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish. [https://github.com/apache/kafka/pull/18449] Looking through the logs, there is a mixture of FROM-CACHE tasks, and tasks that fully ran. [https://github.com/apache/kafka/actions/runs/12677880015/job/35334496897?pr=18449]","output":"Pull Requests not fully leveraging cache","metadata":{"issue_id":"KAFKA-18500","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-14T00:23:28.000+0000","updated":"2025-03-04T00:02:13.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Pull Requests not fully leveraging cache\n\nI noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish. [https://github.com/apache/kafka/pull/18449] Looking through the logs, there is a mixture of FROM-CACHE tasks, and tasks that fully ran. [https://github.com/apache/kafka/actions/runs/12677880015/job/","output":"Open","metadata":{"issue_id":"KAFKA-18500","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-14T00:23:28.000+0000","updated":"2025-03-04T00:02:13.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Pull Requests not fully leveraging cache\n\nI noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish. [https://github.com/apache/kafka/pull/18449] Looking through the logs, there is a mixture of FROM-CACHE tasks, and tasks that fully ran. [https://github.com/apache/kafka/actions/runs/12677880015/job/","output":"Major","metadata":{"issue_id":"KAFKA-18500","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-14T00:23:28.000+0000","updated":"2025-03-04T00:02:13.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Pull Requests not fully leveraging cache\n\nI noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish. [https://github.com/apache/kafka/pull/18449] Looking through the logs, there is a mixture of FROM-CACHE tasks, and tasks that fully ran. [https://github.com/apache/kafka/actions/runs/12677880015/job/","output":"I noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish.","metadata":{"issue_id":"KAFKA-18500","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-14T00:23:28.000+0000","updated":"2025-03-04T00:02:13.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I noticed PRs were running for longer than expected, especially ones with no code changes. A no-op PR based on the \"trunk-cached\" ref should ideally run in only a few minutes as all the compilation and tests should be cached in the Gradle cache. I made such a PR, and it took quite a while to finish. [https://github.com/apache/kafka/pull/18449] Looking through the logs, there is a mixture of FROM-CACHE tasks, and tasks that fully ran. [https://github.com/apache/kafka/actions/runs/12677880015/job/35334496897?pr=18449]","output":"Pull Requests not fully leveraging cache","metadata":{"issue_id":"KAFKA-18500","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-14T00:23:28.000+0000","updated":"2025-03-04T00:02:13.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Clean up LogConfig\n\nthere are many useless handlers","output":"Resolved","metadata":{"issue_id":"KAFKA-18499","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-13T19:46:37.000+0000","updated":"2025-02-06T10:53:04.000+0000","resolved":"2025-02-06T10:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912651","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!","created":"2025-01-13T20:02:15.583+0000","updated":"2025-01-13T20:02:15.583+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17924481","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","body_raw":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57\r\n\r\n4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","created":"2025-02-06T10:53:04.770+0000","updated":"2025-02-06T10:53:04.770+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Clean up LogConfig\n\nthere are many useless handlers","output":"Major","metadata":{"issue_id":"KAFKA-18499","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-13T19:46:37.000+0000","updated":"2025-02-06T10:53:04.000+0000","resolved":"2025-02-06T10:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912651","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!","created":"2025-01-13T20:02:15.583+0000","updated":"2025-01-13T20:02:15.583+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17924481","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","body_raw":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57\r\n\r\n4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","created":"2025-02-06T10:53:04.770+0000","updated":"2025-02-06T10:53:04.770+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Clean up LogConfig\n\nthere are many useless handlers","output":"there are many useless handlers","metadata":{"issue_id":"KAFKA-18499","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-13T19:46:37.000+0000","updated":"2025-02-06T10:53:04.000+0000","resolved":"2025-02-06T10:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912651","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!","created":"2025-01-13T20:02:15.583+0000","updated":"2025-01-13T20:02:15.583+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17924481","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","body_raw":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57\r\n\r\n4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","created":"2025-02-06T10:53:04.770+0000","updated":"2025-02-06T10:53:04.770+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Clean up LogConfig\n\nthere are many useless handlers","output":"Fixed","metadata":{"issue_id":"KAFKA-18499","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-13T19:46:37.000+0000","updated":"2025-02-06T10:53:04.000+0000","resolved":"2025-02-06T10:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912651","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!","created":"2025-01-13T20:02:15.583+0000","updated":"2025-01-13T20:02:15.583+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17924481","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","body_raw":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57\r\n\r\n4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","created":"2025-02-06T10:53:04.770+0000","updated":"2025-02-06T10:53:04.770+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Clean up LogConfig\n\nthere are many useless handlers\n\nConversation:\nuser: Hi, I’d like to work on it. Thank you!\nassistant: trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","output":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","metadata":{"issue_id":"KAFKA-18499","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-13T19:46:37.000+0000","updated":"2025-02-06T10:53:04.000+0000","resolved":"2025-02-06T10:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912651","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!","created":"2025-01-13T20:02:15.583+0000","updated":"2025-01-13T20:02:15.583+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17924481","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57 4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","body_raw":"trunk: https://github.com/apache/kafka/commit/c23d4a0d73d8ad168a0bbfcc8b5141cde0a1cc57\r\n\r\n4.0: https://github.com/apache/kafka/commit/8b0ef93bb48570108ad98669bb9eb247e0679abc","created":"2025-02-06T10:53:04.770+0000","updated":"2025-02-06T10:53:04.770+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) [2024-12-19 02:45:29,656] DEBUG [RocksDB Metrics Recorder for KSTREAM-AGGREGATE-STATE-STORE-0000000014] Adding metrics recorder of task 1_0 to metrics recording trigger (org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder) [2024-12-19 02:45:29,656] DEBUG [RocksDB Metrics Recorder for KSTREAM-AGGREGATE-STATE-STORE-0000000014] Adding value providers for store KSTREAM-AGGREGATE-STATE-STORE-0000000014 of task 1_0 (org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder) [2024-12-19 02:45:29,656] DEBUG stream-client [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c] Registered state store KSTREAM-AGGREGATE-STATE-STORE-0000000014 to its state manager (org.apache.kafka.streams.processor.internals.ProcessorStateManager) [2024-12-19 02:45:29,675] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000018 in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) [2024-12-19 02:45:29,675] DEBUG [RocksDB Metrics Recorder for KSTREAM-AGGREGATE-STATE-STORE-0000000018] Adding metrics recorder of task 1_0 to metrics recording trigger (org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder) [2024-12-19 02:45:29,675] DEBUG [RocksDB Metrics Recorder for KSTREAM-AGGREGATE-STATE-STORE-0000000018] Adding value providers for store KSTREAM-AGGREGATE-STATE-STORE-0000000018 of task 1_0 (org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder) [2024-12-19 02:45:29,676] DEBUG stream-client [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c] Registered state store KSTREAM-AGGREGATE-STATE-STORE-0000000018 to its state manager (org.apache.kafka.streams.processor.internals.ProcessorStateManager) [2024-12-19 02:45:29,676] DEBUG stream-thread [main] standby-task [1_0] Registered state stores (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,676] INFO stream-client [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c] State store KSTREAM-AGGREGATE-STATE-STORE-0000000014 initialized from checkpoint with offset null at changelog EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000014-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager) [2024-12-19 02:45:29,676] INFO stream-client [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c] State store KSTREAM-AGGREGATE-STATE-STORE-0000000018 initialized from checkpoint with offset null at changelog EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000018-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager) [2024-12-19 02:45:29,676] DEBUG stream-thread [main] standby-task [1_0] Initialized state stores (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,676] INFO stream-thread [main] standby-task [1_0] Initialized (org.apache.kafka.streams.processor.internals.StandbyTask) [...] [2024-12-19 02:45:32,328] INFO stream-thread [main] standby-task [1_0] Suspended running (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:32,328] ERROR stream-thread [main] standby-task [1_0] Failed to acquire lock while closing the state store for STANDBY task 1_0 (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:32,328] INFO stream-thread [main] standby-task [1_0] Closed clean (org.apache.kafka.streams.processor.internals.StandbyTask) [...] [2024-12-19 02:45:32,374] INFO stream-thread [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c-StreamThread-1] Handle new assignment with: New active tasks: [1_0, 1_3, 0_2] New standby tasks: [0_4, 1_4, 0_3, 0_1, 1_2, 1_1, 0_0] Existing active tasks: [] Existing standby tasks: [0_4, 1_4, 0_3, 0_1, 1_2, 0_0, 1_1] (org.apache.kafka.streams.processor.internals.TaskManager) [2024-12-19 02:45:32,376] DEBUG stream-thread [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c-StreamThread-1] stream-task [1_0] Created state store manager for task 1_0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager) [...] [2024-12-19 02:45:32,562] INFO stream-thread [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c-StreamThread-1] Encountered lock exception. Reattempting locking the state in the next iteration. Error message was: stream-thread [EosTest-6cef0eeb-642f-446b-ada7-35c3d7f7121c-StreamThread-1] task [1_0] Failed to lock the state directory for task 1_0 (org.apache.kafka.streams.processor.internals.TaskManager) {code} The last log line keeps repeating... I am going to revert the PR for 4.0 release, and we should fix forward on `trunk`.","output":"Kafka Streams main thread does not release lock","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.st","output":"Resolved","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.st","output":"Critical","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.st","output":"We merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired st","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.st","output":"Fixed","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Kafka Streams main thread does not release lock\n\nWe merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant l\n\nConversation:\nuser: issue resolved and merged to trunk","output":"issue resolved and merged to trunk","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We merged [https://github.com/apache/kafka/pull/16922] which seems to introduce a regression bug, not releasing a lock properly. We saw a failing system test (timed out) with the following (relevant logs) {code:java} [2024-12-19 02:45:29,637] DEBUG stream-thread [main] standby-task [1_0] Acquired state directory lock (org.apache.kafka.streams.processor.internals.StandbyTask) [2024-12-19 02:45:29,655] INFO Opening store KSTREAM-AGGREGATE-STATE-STORE-0000000014 in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) [2024-12-19 02:45:29,656] DEBUG [RocksDB Metrics Recorder for KSTREAM-AGGREGATE-STATE-STORE-0000000014] Adding metrics recorder of task 1_0 to metrics recording trigger (org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder) [2024-12-1","output":"Kafka Streams main thread does not release lock","metadata":{"issue_id":"KAFKA-18498","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-13T18:50:42.000+0000","updated":"2025-01-29T19:13:54.000+0000","resolved":"2025-01-29T19:13:54.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17922189","author":{"displayName":"Bill Bejeck"},"body":"issue resolved and merged to trunk","body_raw":"issue resolved and merged to trunk","created":"2025-01-29T19:13:47.755+0000","updated":"2025-01-29T19:13:47.755+0000","updateAuthor":{"displayName":"Bill Bejeck"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"Remove config/kraft/server.properties","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"Resolved","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"Major","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"We should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"Fixed","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove config/kraft/server.properties\n\nWe should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties \n\nConversation:\nuser: Hi jsancio I think this ticket is related to 18229, I just linked it","output":"Hi jsancio I think this ticket is related to 18229, I just linked it","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We should remove the file config/kraft/server.properties by rename config/kraft/reconfig-server.properties to config/kraft/server.properties. All references to config/kraft/reconfig-server.properties should be replaced with config/kraft/server.properties.","output":"Remove config/kraft/server.properties","metadata":{"issue_id":"KAFKA-18497","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2025-01-13T18:08:23.000+0000","updated":"2025-01-24T04:47:58.000+0000","resolved":"2025-01-24T04:47:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912682","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I think this ticket is related to 18229, I just linked it","body_raw":"Hi [~jsancio] \r\n\r\nI think this ticket is related to 18229, I just linked it","created":"2025-01-14T00:28:56.172+0000","updated":"2025-01-14T00:28:56.172+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Using ACL and StandardAuthorizer with PLAINTEXT\n\nIt is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty List actions, e.g. when AuthHelper::filterByAuthorized invoked for DESCRIBE AclOperation with empth resouces list (it is possible in brocker registration process and in add ACL). As a result, context of security request in Authorizer is lost, and method StandardAuthorizer::authorize dont knows which action to authorize. My w/a creates fake principal with fake ResourcePattern with type LITERAL, because ANY is not allowed in constructor. I suggest to add a check if resource is empty and pass to Authorizer some valid placeholder.","output":"Using ACL and StandardAuthorizer with PLAINTEXT","metadata":{"issue_id":"KAFKA-18496","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Konstantin Morozov"},"created":"2025-01-13T17:45:05.000+0000","updated":"2025-01-13T17:47:37.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Using ACL and StandardAuthorizer with PLAINTEXT\n\nIt is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty List actions, e.g. when AuthHelper::filterByAuthorized invoked for DESCRIBE AclOperation with empth resouces list (it is possible in brocker registration process and in add ACL). As a result, context of","output":"Open","metadata":{"issue_id":"KAFKA-18496","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Konstantin Morozov"},"created":"2025-01-13T17:45:05.000+0000","updated":"2025-01-13T17:47:37.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Using ACL and StandardAuthorizer with PLAINTEXT\n\nIt is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty List actions, e.g. when AuthHelper::filterByAuthorized invoked for DESCRIBE AclOperation with empth resouces list (it is possible in brocker registration process and in add ACL). As a result, context of","output":"Minor","metadata":{"issue_id":"KAFKA-18496","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Konstantin Morozov"},"created":"2025-01-13T17:45:05.000+0000","updated":"2025-01-13T17:47:37.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Using ACL and StandardAuthorizer with PLAINTEXT\n\nIt is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty List actions, e.g. when AuthHelper::filterByAuthorized invoked for DESCRIBE AclOperation with empth resouces list (it is possible in brocker registration process and in add ACL). As a result, context of","output":"It is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty Li","metadata":{"issue_id":"KAFKA-18496","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Konstantin Morozov"},"created":"2025-01-13T17:45:05.000+0000","updated":"2025-01-13T17:47:37.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"It is not intended, but possible to use ACL and StandardAuthorizer with PLAINTEXT. As an example I've created an class, extending StandardAuthorizer that avoids limitations of PrincipalBuilder of PLAINTEXT. But there are several cases, when StandardAuthorizer::authorize methods invokes with empty List actions, e.g. when AuthHelper::filterByAuthorized invoked for DESCRIBE AclOperation with empth resouces list (it is possible in brocker registration process and in add ACL). As a result, context of security request in Authorizer is lost, and method StandardAuthorizer::authorize dont knows which action to authorize. My w/a creates fake principal with fake ResourcePattern with type LITERAL, because ANY is not allowed in constructor. I suggest to add a check if resource is empty and pass to Auth","output":"Using ACL and StandardAuthorizer with PLAINTEXT","metadata":{"issue_id":"KAFKA-18496","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Konstantin Morozov"},"created":"2025-01-13T17:45:05.000+0000","updated":"2025-01-13T17:47:37.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store\n\nh1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB instance. It was previously calculated using the RocksDB internal metric `NO_FILE_CLOSES`, which was removed in RocksDB version 9.7.3. Due to this change, the `numberOfOpenFiles` metric no longer provides accurate information. The current workaround is to have it always return -1, which is misleading. The `numberOfOpenFiles` metric is: Invalid It does not reflect the actual number of open files in RocksDB. This misleading and returning -1 can confuse users and monitoring systems and unnecessary it provides no useful information in its current state. h4. Proposed Solution This Jira issue is to implement the changes proposed in https://cwiki.apache.org/confluence/display/KAFKA/KIP-1125%3A+Remove+Invalid+%27numberOfOpenFiles%27+Metric+from+RocksDB+State+Store. The solution is to completely remove the `opennumberfiles` metric, including: * Removing the related code in `RocksDBMetricsRecorder` (`numberOfOpenFilesSensor`, recording logic). * Removing the `numberOfOpenFilesSensor` method in `RocksDBMetrics` * Updating the Kafka Streams documentation to reflect the metric's removal.","output":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store\n\nh1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB instance. It was previously calculated using the RocksDB internal metric `NO_FILE_CLOSES`, which was removed in RocksDB version 9.7.3. Due to this change, the `numberOfOpenFiles` metric no longer provides ","output":"Open","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store\n\nh1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB instance. It was previously calculated using the RocksDB internal metric `NO_FILE_CLOSES`, which was removed in RocksDB version 9.7.3. Due to this change, the `numberOfOpenFiles` metric no longer provides ","output":"Blocker","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store\n\nh1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB instance. It was previously calculated using the RocksDB internal metric `NO_FILE_CLOSES`, which was removed in RocksDB version 9.7.3. Due to this change, the `numberOfOpenFiles` metric no longer provides ","output":"h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB insta","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store\n\nh1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be r\n\nConversation:\nuser: swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1\nassistant: It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.\nassistant: cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?\nassistant: We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the met\nassistant: I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example","output":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.* The `numberOfOpenFiles` metric in the Kafka Streams RocksDB state store implementation is currently invalid and should be removed. This metric was intended to report the approximate number of open files in the RocksDB instance. It was previously calculated using the RocksDB internal metric `NO_FILE_CLOSES`, which was removed in RocksDB version 9.7.3. Due to this change, the `numberOfOpenFiles` metric no longer provides accurate information. The current workaround is to have it always return -1, which is misleading. The `numberOfOpenFiles` metric is: Invalid It does not reflect the actual number of open files in RocksDB. This misleading and returning -1 can confuse users and monitoring systems and unnecessary it pr","output":"Remove Invalid 'numberOfOpenFiles' Metric from RocksDB State Store","metadata":{"issue_id":"KAFKA-18495","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":null,"reporter":{"displayName":"Swikar Patel"},"created":"2025-01-13T17:40:29.000+0000","updated":"2025-06-15T05:07:30.000+0000","resolved":null,"labels":["kip"],"components":["streams"],"comment_count":6,"comments":[{"id":"17912642","author":{"displayName":"Matthias J. Sax"},"body":"swikarpat Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release). cadonna – Thoughts?","body_raw":"[~swikarpat] Thanks for filing this ticket. I am not sure if we actually need a KIP for this... It's a corner case... Given that you already wrote it up, we might just move to a VOTE directly, skipping the DISCUSSION. – However, we need to keep the metric until 5.0 is released. We cannot do it in 4.0.1 as you put on the KIP wiki page (if we would do it in 4.0.1, we could have done it right away in 4.0.0 to begin with – the whole point of backward compatibility is to only remove stuff in major release).\r\n\r\n[~cadonna] – Thoughts?\r\n\r\n ","created":"2025-01-13T19:16:09.710+0000","updated":"2025-01-13T19:16:09.710+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912644","author":{"displayName":"Bruno Cadonna"},"body":"It is better to have a KIP for the removal, IMO. I agree that we can immediately move to VOTE. I also agree that we cannot remove it until 5.0.","body_raw":"It is better to have a KIP for the removal, IMO. \r\n\r\nI agree that we can immediately move to VOTE.\r\nI also agree that we cannot remove it until 5.0.","created":"2025-01-13T19:20:16.711+0000","updated":"2025-01-13T19:20:16.711+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17912648","author":{"displayName":"Swikar Patel"},"body":"cadonna Thanks for commenting. mjsax Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","body_raw":"[~cadonna] Thanks for commenting.\r\n\r\n[~mjsax] Thanks. As this is for 5.0.0 then would we do VOTING at that time or do now?","created":"2025-01-13T19:43:39.204+0000","updated":"2025-01-13T19:43:39.204+0000","updateAuthor":{"displayName":"Swikar Patel"}},{"id":"17912649","author":{"displayName":"Bruno Cadonna"},"body":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface. Imagine a user that exports that metric in their observability system. The export could break if we just removed the metric.","body_raw":"We should VOTE now. 5.0.0 is just the next release where we are allowed to break the public interface since it is the next major release. A metric is part of the public interface.\r\n\r\nImagine a user that exports that metric in their observability system. The export could break if we just removed the metric. ","created":"2025-01-13T19:46:43.799+0000","updated":"2025-01-13T19:47:54.406+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17968855","author":{"displayName":"Mickael Maison"},"body":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense. While we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it. We can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0. While it's not great, we've used this mechanism a number of times in the past, for example https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","body_raw":"I stumbled on this by accident and I don't know the background or details, so ignore me if I say nonsense.\r\n\r\nWhile we can't remove the metrics until the next major version, if this is really problematic, there's an approach we can use to workaround it.\r\n\r\nWe can introduce a new configuration, for example \"remove.this.metric\" which when set to true removes the metric. It would default to false, to keep the current behavior, and directly be marked as deprecated and to be removed in 5.0.0.\r\n\r\nWhile it's not great, we've used this mechanism a number of times in the past, for example [KIP-830|https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter]. This lets users opt-in new behavior without having to wait, considering 5.0.0 is potentially still pretty far off.","created":"2025-06-13T07:48:37.951+0000","updated":"2025-06-13T07:48:37.951+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17976505","author":{"displayName":"Matthias J. Sax"},"body":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","body_raw":"Interesting idea. But not sure if worth the trouble? Is there a significant advantage to not register the metric vs keep it until 5.0 and let it just report `-1` (unknown) surrogate value?","created":"2025-06-15T05:07:30.376+0000","updated":"2025-06-15T05:07:30.376+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at org.gradle.api.internal.file.collections.LazilyInitializedFileCollection.visitChildren(LazilyInitializedFileCollection.java:52) at org.gradle.api.internal.file.CompositeFileCollection.visitContents(CompositeFileCollection.java:113) at org.gradle.api.internal.file.AbstractFileCollection.getFiles(AbstractFileCollection.java:123) at org.jetbrains.plugins.gradle.tooling.builder.ScalaModelBuilderImpl.createModel(ScalaModelBuilderImpl.java:64) at org.jetbrains.plugins.gradle.tooling.builder.ScalaModelBuilderImpl.buildAll(ScalaModelBuilderImpl.java:46) at com.intellij.gradle.toolingExtension.impl.modelBuilder.ExtraModelBuilder.buildServiceModel(ExtraModelBuilder.java:90) ```","output":"Fix configure :streams:integration-tests project error","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at or","output":"Resolved","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at or","output":"Major","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at or","output":"It alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? ","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at or","output":"Fixed","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix configure :streams:integration-tests project error\n\nIt alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala li\n\nConversation:\nuser: What do you mean by \"when I configure gradle\"? What command are you running?\nassistant: I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!\nassistant: Thanks!","output":"Thanks!","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"It alaways has an error when I configure the gradle, we should fix it. ``` Unable to build Scala project configuration org.gradle.api.GradleException: Cannot infer Scala class path because no Scala library Jar was found. Does project ':streams:integration-tests' declare dependency to scala-library? Searched classpath: file collection. at org.gradle.api.tasks.ScalaRuntime$1.inferScalaClasspath(ScalaRuntime.java:103) at org.gradle.api.tasks.ScalaRuntime$1.createDelegate(ScalaRuntime.java:92) at org.gradle.api.internal.file.collections.LazilyInitializedFileCollection.visitChildren(LazilyInitializedFileCollection.java:52) at org.gradle.api.internal.file.CompositeFileCollection.visitContents(CompositeFileCollection.java:113) at org.gradle.api.internal.file.AbstractFileCollection.getFiles(Abstra","output":"Fix configure :streams:integration-tests project error","metadata":{"issue_id":"KAFKA-18493","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:42:44.000+0000","updated":"2025-01-14T21:04:45.000+0000","resolved":"2025-01-14T21:04:45.000+0000","resolution":"Fixed","labels":[],"components":["build","streams"],"comment_count":3,"comments":[{"id":"17912640","author":{"displayName":"Matthias J. Sax"},"body":"What do you mean by \"when I configure gradle\"? What command are you running?","body_raw":"What do you mean by \"when I configure gradle\"? What command are you running?","created":"2025-01-13T19:07:06.114+0000","updated":"2025-01-13T19:07:06.114+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912685","author":{"displayName":"黃竣陽"},"body":"I use the intellij feature !image-2025-01-14-08-47-46-010.png|width=519,height=174! !image-2025-01-14-08-46-29-732.png|width=520,height=186!","body_raw":"I use the intellij feature\r\n\r\n!image-2025-01-14-08-47-46-010.png|width=519,height=174!\r\n\r\n!image-2025-01-14-08-46-29-732.png|width=520,height=186!","created":"2025-01-14T00:46:33.112+0000","updated":"2025-01-14T00:48:22.525+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912715","author":{"displayName":"Matthias J. Sax"},"body":"Thanks!","body_raw":"Thanks!","created":"2025-01-14T02:56:04.967+0000","updated":"2025-01-14T02:56:04.967+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis","output":"Cleanup RequestHandlerHelper","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis","output":"Resolved","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis","output":"Minor","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis","output":"We should clean up this helper class after we cleanup zookeeper from KafkaApis","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis","output":"Fixed","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cleanup RequestHandlerHelper\n\nWe should clean up this helper class after we cleanup zookeeper from KafkaApis\n\nConversation:\nuser: Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","output":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","metadata":{"issue_id":"KAFKA-18492","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-13T16:22:40.000+0000","updated":"2025-01-20T14:50:47.000+0000","resolved":"2025-01-20T14:50:20.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914706","author":{"displayName":"Mickael Maison"},"body":"Merged to trunk: https://github.com/apache/kafka/pull/18608 and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec] Marking as resolved","body_raw":"Merged to trunk: [https://github.com/apache/kafka/commit/ce5d7be1dc1582700f0b0d183268d2d3d395829a|https://github.com/apache/kafka/pull/18608] and 4.0: [https://github.com/apache/kafka/commit/3dfd9d9cb0d8f16a9aad599f035d56c3823bd2ec]\r\nMarking as resolved","created":"2025-01-20T14:50:11.370+0000","updated":"2025-01-20T14:50:11.370+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"Resolved","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"Major","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"Fixed","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager\n\n* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","output":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"* [https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L324] * [https://github.com/apache/kafka/blob/772aa241b2f1c3e8c8b7ad73ddc910c8272f2d1d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2661-L2672] * [https://github.com/apache/kafka/blob/d4aee71e36204146ebc935851afb93574dbc131a/core/src/main/scala/kafka/server/ReplicaManager.scala#L2039-L2054]","output":"Remove zkClient & maybeUpdateMetadataCache from ReplicaManager","metadata":{"issue_id":"KAFKA-18491","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:17:21.000+0000","updated":"2025-01-13T19:31:02.000+0000","resolved":"2025-01-13T19:31:02.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17912646","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882 4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","body_raw":"trunk: https://github.com/apache/kafka/commit/da0c3beffa96944a899ea949a7ab5705f67be882\r\n\r\n4.0: https://github.com/apache/kafka/commit/39069bbad227803fcd56761d6bdeb5d2471b19ee","created":"2025-01-13T19:31:02.750+0000","updated":"2025-01-13T19:31:02.750+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"Remove ReplicaManager#electLeaders","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"Resolved","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"Major","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"Duplicate","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ReplicaManager#electLeaders\n\n[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController\n\nConversation:\nuser: Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","output":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/blob/b4be1785998d382843002ffd8685d67fe8d9c97d/core/src/main/scala/kafka/server/ReplicaManager.scala#L2775-L2814] ReplicaManager#electLeaders use zk kafkaController","output":"Remove ReplicaManager#electLeaders","metadata":{"issue_id":"KAFKA-18490","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chu Cheng Li"},"created":"2025-01-13T13:15:24.000+0000","updated":"2025-01-17T03:22:14.000+0000","resolved":"2025-01-17T03:22:14.000+0000","resolution":"Duplicate","labels":[],"components":["core"],"comment_count":1,"comments":[{"id":"17913935","author":{"displayName":"Chu Cheng Li"},"body":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481","body_raw":"Closes as this has been covered by https://github.com/apache/kafka/commit/0e502e0b477a5af5e7c545ddab03de41ad2be481\n","created":"2025-01-17T03:21:33.320+0000","updated":"2025-01-17T03:21:33.320+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated\n\nA minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);}} {{ }} {{wraps herder's listener in workerMetricsGroup's listener. WorkerMetricsGroup is the entity which records task success and failure in the metric .}} {{In the PR, the wrapped listener was missed and instead the herder listener is passed to the task builder (as part of constructor).}} {{ }}","output":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated\n\nA minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);}} {{ }} {{wraps herder's listener in workerMetricsGroup's listener. WorkerMetricsGroup is the entity which records task success and failure in the metric .}} {{In the PR, the wrapped listener was mis","output":"Open","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated\n\nA minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);}} {{ }} {{wraps herder's listener in workerMetricsGroup's listener. WorkerMetricsGroup is the entity which records task success and failure in the metric .}} {{In the PR, the wrapped listener was mis","output":"Minor","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Connector tasks metrics task-startup-attempts, task-startup-success are not being updated\n\nA minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);}} {{ }} {{wraps herder's listener in workerMetricsGroup's listener. WorkerMetricsGroup is the entity which records task success and failure in the metric .}} {{In the PR, the wrapped listener was mis","output":"A minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated\n\nA minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{f\n\nConversation:\nuser: Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin\nassistant: If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","output":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"A minor bug was introduced as part of https://github.com/apache/kafka/commit/9e8ef8bb317599c184ce8201d494edf109d9c528#diff-90b4072cc070e1d6417b3bb59a4a35668c4d97ce1dd6a2587db9f2c9a9ebf93e The line {{final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);}} {{ }} {{wraps herder's listener in workerMetricsGroup's listener. WorkerMetricsGroup is the entity which records task success and failure in the metric .}} {{In the PR, the wrapped listener was missed and instead the herder listener is passed to the task builder (as part of constructor).}} {{ }}","output":"Connector tasks metrics task-startup-attempts, task-startup-success are not being updated","metadata":{"issue_id":"KAFKA-18489","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"Ashwin Pankaj"},"reporter":{"displayName":"Ashwin Pankaj"},"created":"2025-01-13T11:47:24.000+0000","updated":"2025-02-13T03:09:43.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":2,"comments":[{"id":"17913535","author":{"displayName":"Ashwin Pankaj"},"body":"Hello ChrisEgerton mimaison , I just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix Thanks, Ashwin","body_raw":"Hello [~ChrisEgerton] [~mimaison] ,\r\n\r\n \r\n\r\nI just wanted to check if the listener mentioned in the description (and shown in attached screenshot) was unintentionally changed and if it is OK to go ahead with a fix\r\n\r\n \r\n\r\nThanks,\r\n\r\nAshwin","created":"2025-01-16T03:46:00.818+0000","updated":"2025-01-16T03:46:00.818+0000","updateAuthor":{"displayName":"Ashwin Pankaj"}},{"id":"17914711","author":{"displayName":"Mickael Maison"},"body":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","body_raw":"If the metrics are not working anymore then this is a bug. It seems you've looked into the issue, can you open a PR?","created":"2025-01-20T15:02:38.896+0000","updated":"2025-01-20T15:02:38.896+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Additional protocol tests for share consumption\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18488","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-13T10:54:04.000+0000","updated":"2025-04-23T10:09:04.000+0000","resolved":"2025-04-23T10:09:04.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17921687","author":{"displayName":"Andrew Schofield"},"body":"Reopened while tracking down flakiness.","body_raw":"Reopened while tracking down flakiness.","created":"2025-01-28T10:54:04.879+0000","updated":"2025-01-28T10:54:04.879+0000","updateAuthor":{"displayName":"Andrew Schofield"}},{"id":"17946679","author":{"displayName":"Andrew Schofield"},"body":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","created":"2025-04-23T10:09:04.247+0000","updated":"2025-04-23T10:09:04.247+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Additional protocol tests for share consumption\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18488","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-13T10:54:04.000+0000","updated":"2025-04-23T10:09:04.000+0000","resolved":"2025-04-23T10:09:04.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17921687","author":{"displayName":"Andrew Schofield"},"body":"Reopened while tracking down flakiness.","body_raw":"Reopened while tracking down flakiness.","created":"2025-01-28T10:54:04.879+0000","updated":"2025-01-28T10:54:04.879+0000","updateAuthor":{"displayName":"Andrew Schofield"}},{"id":"17946679","author":{"displayName":"Andrew Schofield"},"body":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","created":"2025-04-23T10:09:04.247+0000","updated":"2025-04-23T10:09:04.247+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Additional protocol tests for share consumption\n\n","output":"","metadata":{"issue_id":"KAFKA-18488","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-13T10:54:04.000+0000","updated":"2025-04-23T10:09:04.000+0000","resolved":"2025-04-23T10:09:04.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17921687","author":{"displayName":"Andrew Schofield"},"body":"Reopened while tracking down flakiness.","body_raw":"Reopened while tracking down flakiness.","created":"2025-01-28T10:54:04.879+0000","updated":"2025-01-28T10:54:04.879+0000","updateAuthor":{"displayName":"Andrew Schofield"}},{"id":"17946679","author":{"displayName":"Andrew Schofield"},"body":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","created":"2025-04-23T10:09:04.247+0000","updated":"2025-04-23T10:09:04.247+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Additional protocol tests for share consumption\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18488","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-13T10:54:04.000+0000","updated":"2025-04-23T10:09:04.000+0000","resolved":"2025-04-23T10:09:04.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17921687","author":{"displayName":"Andrew Schofield"},"body":"Reopened while tracking down flakiness.","body_raw":"Reopened while tracking down flakiness.","created":"2025-01-28T10:54:04.879+0000","updated":"2025-01-28T10:54:04.879+0000","updateAuthor":{"displayName":"Andrew Schofield"}},{"id":"17946679","author":{"displayName":"Andrew Schofield"},"body":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","created":"2025-04-23T10:09:04.247+0000","updated":"2025-04-23T10:09:04.247+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Additional protocol tests for share consumption\n\n\n\nConversation:\nuser: Reopened while tracking down flakiness.\nassistant: https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","output":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","metadata":{"issue_id":"KAFKA-18488","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-13T10:54:04.000+0000","updated":"2025-04-23T10:09:04.000+0000","resolved":"2025-04-23T10:09:04.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17921687","author":{"displayName":"Andrew Schofield"},"body":"Reopened while tracking down flakiness.","body_raw":"Reopened while tracking down flakiness.","created":"2025-01-28T10:54:04.879+0000","updated":"2025-01-28T10:54:04.879+0000","updateAuthor":{"displayName":"Andrew Schofield"}},{"id":"17946679","author":{"displayName":"Andrew Schofield"},"body":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","body_raw":"https://issues.apache.org/jira/browse/KAFKA-18794 tracks the resolution of the flaky test.","created":"2025-04-23T10:09:04.247+0000","updated":"2025-04-23T10:09:04.247+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ReplicaManager#stopReplicas\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18487","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:34.000+0000","updated":"2025-01-21T10:48:09.000+0000","resolved":"2025-01-21T10:48:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17914477","author":{"displayName":"Ismael Juma"},"body":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","body_raw":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","created":"2025-01-19T17:31:10.142+0000","updated":"2025-01-19T17:31:10.142+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914739","author":{"displayName":"David Jacot"},"body":"ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","body_raw":"[~ijuma] Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","created":"2025-01-20T17:19:31.516+0000","updated":"2025-01-20T17:19:31.516+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914742","author":{"displayName":"Ismael Juma"},"body":"We can and will remove the auto generated class, there's already a PR for that.","body_raw":"We can and will remove the auto generated class, there's already a PR for that.","created":"2025-01-20T17:23:28.915+0000","updated":"2025-01-20T17:23:28.915+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914743","author":{"displayName":"Ismael Juma"},"body":"PR: https://github.com/apache/kafka/pull/18497","body_raw":"PR: https://github.com/apache/kafka/pull/18497","created":"2025-01-20T17:24:21.162+0000","updated":"2025-01-20T17:24:21.162+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914745","author":{"displayName":"David Jacot"},"body":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","body_raw":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","created":"2025-01-20T17:31:33.879+0000","updated":"2025-01-20T17:31:33.879+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914747","author":{"displayName":"Ismael Juma"},"body":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","body_raw":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","created":"2025-01-20T17:34:18.139+0000","updated":"2025-01-20T17:34:36.678+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ReplicaManager#stopReplicas\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18487","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:34.000+0000","updated":"2025-01-21T10:48:09.000+0000","resolved":"2025-01-21T10:48:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17914477","author":{"displayName":"Ismael Juma"},"body":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","body_raw":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","created":"2025-01-19T17:31:10.142+0000","updated":"2025-01-19T17:31:10.142+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914739","author":{"displayName":"David Jacot"},"body":"ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","body_raw":"[~ijuma] Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","created":"2025-01-20T17:19:31.516+0000","updated":"2025-01-20T17:19:31.516+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914742","author":{"displayName":"Ismael Juma"},"body":"We can and will remove the auto generated class, there's already a PR for that.","body_raw":"We can and will remove the auto generated class, there's already a PR for that.","created":"2025-01-20T17:23:28.915+0000","updated":"2025-01-20T17:23:28.915+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914743","author":{"displayName":"Ismael Juma"},"body":"PR: https://github.com/apache/kafka/pull/18497","body_raw":"PR: https://github.com/apache/kafka/pull/18497","created":"2025-01-20T17:24:21.162+0000","updated":"2025-01-20T17:24:21.162+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914745","author":{"displayName":"David Jacot"},"body":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","body_raw":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","created":"2025-01-20T17:31:33.879+0000","updated":"2025-01-20T17:31:33.879+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914747","author":{"displayName":"Ismael Juma"},"body":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","body_raw":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","created":"2025-01-20T17:34:18.139+0000","updated":"2025-01-20T17:34:36.678+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ReplicaManager#stopReplicas\n\n","output":"","metadata":{"issue_id":"KAFKA-18487","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:34.000+0000","updated":"2025-01-21T10:48:09.000+0000","resolved":"2025-01-21T10:48:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17914477","author":{"displayName":"Ismael Juma"},"body":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","body_raw":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","created":"2025-01-19T17:31:10.142+0000","updated":"2025-01-19T17:31:10.142+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914739","author":{"displayName":"David Jacot"},"body":"ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","body_raw":"[~ijuma] Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","created":"2025-01-20T17:19:31.516+0000","updated":"2025-01-20T17:19:31.516+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914742","author":{"displayName":"Ismael Juma"},"body":"We can and will remove the auto generated class, there's already a PR for that.","body_raw":"We can and will remove the auto generated class, there's already a PR for that.","created":"2025-01-20T17:23:28.915+0000","updated":"2025-01-20T17:23:28.915+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914743","author":{"displayName":"Ismael Juma"},"body":"PR: https://github.com/apache/kafka/pull/18497","body_raw":"PR: https://github.com/apache/kafka/pull/18497","created":"2025-01-20T17:24:21.162+0000","updated":"2025-01-20T17:24:21.162+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914745","author":{"displayName":"David Jacot"},"body":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","body_raw":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","created":"2025-01-20T17:31:33.879+0000","updated":"2025-01-20T17:31:33.879+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914747","author":{"displayName":"Ismael Juma"},"body":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","body_raw":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","created":"2025-01-20T17:34:18.139+0000","updated":"2025-01-20T17:34:36.678+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ReplicaManager#stopReplicas\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18487","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:34.000+0000","updated":"2025-01-21T10:48:09.000+0000","resolved":"2025-01-21T10:48:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17914477","author":{"displayName":"Ismael Juma"},"body":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","body_raw":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","created":"2025-01-19T17:31:10.142+0000","updated":"2025-01-19T17:31:10.142+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914739","author":{"displayName":"David Jacot"},"body":"ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","body_raw":"[~ijuma] Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","created":"2025-01-20T17:19:31.516+0000","updated":"2025-01-20T17:19:31.516+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914742","author":{"displayName":"Ismael Juma"},"body":"We can and will remove the auto generated class, there's already a PR for that.","body_raw":"We can and will remove the auto generated class, there's already a PR for that.","created":"2025-01-20T17:23:28.915+0000","updated":"2025-01-20T17:23:28.915+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914743","author":{"displayName":"Ismael Juma"},"body":"PR: https://github.com/apache/kafka/pull/18497","body_raw":"PR: https://github.com/apache/kafka/pull/18497","created":"2025-01-20T17:24:21.162+0000","updated":"2025-01-20T17:24:21.162+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914745","author":{"displayName":"David Jacot"},"body":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","body_raw":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","created":"2025-01-20T17:31:33.879+0000","updated":"2025-01-20T17:31:33.879+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914747","author":{"displayName":"Ismael Juma"},"body":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","body_raw":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","created":"2025-01-20T17:34:18.139+0000","updated":"2025-01-20T17:34:36.678+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ReplicaManager#stopReplicas\n\n\n\nConversation:\nuser: StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.\nassistant: ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.\nassistant: We can and will remove the auto generated class, there's already a PR for that.\nassistant: PR: https://github.com/apache/kafka/pull/18497\nassistant: Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","output":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","metadata":{"issue_id":"KAFKA-18487","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:34.000+0000","updated":"2025-01-21T10:48:09.000+0000","resolved":"2025-01-21T10:48:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17914477","author":{"displayName":"Ismael Juma"},"body":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","body_raw":"StopReplicaPartitionState may no longer be needed after this is removed. Let's make sure we don't remove tests that are still needed in kraft mode even though they were using zk mechanisms as an implementation detail.","created":"2025-01-19T17:31:10.142+0000","updated":"2025-01-19T17:31:10.142+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914739","author":{"displayName":"David Jacot"},"body":"ijuma Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","body_raw":"[~ijuma] Did you mean StopReplicaRequest? StopReplicaPartitionState is auto-generated so we cannot remove it.","created":"2025-01-20T17:19:31.516+0000","updated":"2025-01-20T17:19:31.516+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914742","author":{"displayName":"Ismael Juma"},"body":"We can and will remove the auto generated class, there's already a PR for that.","body_raw":"We can and will remove the auto generated class, there's already a PR for that.","created":"2025-01-20T17:23:28.915+0000","updated":"2025-01-20T17:23:28.915+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914743","author":{"displayName":"Ismael Juma"},"body":"PR: https://github.com/apache/kafka/pull/18497","body_raw":"PR: https://github.com/apache/kafka/pull/18497","created":"2025-01-20T17:24:21.162+0000","updated":"2025-01-20T17:24:21.162+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914745","author":{"displayName":"David Jacot"},"body":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","body_raw":"Sorry, I meant that I cannot remove it without removing the schema. You did it the right way in your PR.","created":"2025-01-20T17:31:33.879+0000","updated":"2025-01-20T17:31:33.879+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914747","author":{"displayName":"Ismael Juma"},"body":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","body_raw":"Makes sense - I wasn't sure if this would land before or after that other PR and hence my comment. If this lands before that PR, then there is nothing to do regarding that class, I agree.","created":"2025-01-20T17:34:18.139+0000","updated":"2025-01-20T17:34:36.678+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ReplicaManager#becomeLeaderOrFollower\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18486","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:03.000+0000","updated":"2025-06-29T17:21:57.000+0000","resolved":"2025-06-29T17:21:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17912547","author":{"displayName":"Chu Cheng Li"},"body":"Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","body_raw":"Hi [~dajac],\r\n\r\nWhile working on KAFKA-18491\r\n\r\nI noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}.\r\n\r\nIf you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","created":"2025-01-13T14:13:48.931+0000","updated":"2025-01-13T14:13:48.931+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17914476","author":{"displayName":"Ismael Juma"},"body":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","body_raw":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","created":"2025-01-19T17:26:45.392+0000","updated":"2025-01-19T17:28:45.849+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17950705","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac any updates?","body_raw":"[~dajac] any updates?","created":"2025-05-10T17:30:03.290+0000","updated":"2025-05-10T17:30:03.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17951449","author":{"displayName":"David Jacot"},"body":"chia7712 No progress on this one. Feel free to take it if you have bandwidth.","body_raw":"[~chia7712] No progress on this one. Feel free to take it if you have bandwidth.","created":"2025-05-14T18:02:29.370+0000","updated":"2025-05-14T18:02:29.370+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17951534","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac thanks for response. taijuwu and I will take over it :)","body_raw":"[~dajac] thanks for response. [~taijuwu] and I will take over it :)","created":"2025-05-15T00:56:16.603+0000","updated":"2025-05-15T00:56:16.603+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986782","author":{"displayName":"Chia-Ping Tsai"},"body":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","body_raw":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","created":"2025-06-29T17:21:57.051+0000","updated":"2025-06-29T17:21:57.051+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ReplicaManager#becomeLeaderOrFollower\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18486","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:03.000+0000","updated":"2025-06-29T17:21:57.000+0000","resolved":"2025-06-29T17:21:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17912547","author":{"displayName":"Chu Cheng Li"},"body":"Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","body_raw":"Hi [~dajac],\r\n\r\nWhile working on KAFKA-18491\r\n\r\nI noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}.\r\n\r\nIf you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","created":"2025-01-13T14:13:48.931+0000","updated":"2025-01-13T14:13:48.931+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17914476","author":{"displayName":"Ismael Juma"},"body":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","body_raw":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","created":"2025-01-19T17:26:45.392+0000","updated":"2025-01-19T17:28:45.849+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17950705","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac any updates?","body_raw":"[~dajac] any updates?","created":"2025-05-10T17:30:03.290+0000","updated":"2025-05-10T17:30:03.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17951449","author":{"displayName":"David Jacot"},"body":"chia7712 No progress on this one. Feel free to take it if you have bandwidth.","body_raw":"[~chia7712] No progress on this one. Feel free to take it if you have bandwidth.","created":"2025-05-14T18:02:29.370+0000","updated":"2025-05-14T18:02:29.370+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17951534","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac thanks for response. taijuwu and I will take over it :)","body_raw":"[~dajac] thanks for response. [~taijuwu] and I will take over it :)","created":"2025-05-15T00:56:16.603+0000","updated":"2025-05-15T00:56:16.603+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986782","author":{"displayName":"Chia-Ping Tsai"},"body":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","body_raw":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","created":"2025-06-29T17:21:57.051+0000","updated":"2025-06-29T17:21:57.051+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ReplicaManager#becomeLeaderOrFollower\n\n","output":"","metadata":{"issue_id":"KAFKA-18486","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:03.000+0000","updated":"2025-06-29T17:21:57.000+0000","resolved":"2025-06-29T17:21:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17912547","author":{"displayName":"Chu Cheng Li"},"body":"Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","body_raw":"Hi [~dajac],\r\n\r\nWhile working on KAFKA-18491\r\n\r\nI noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}.\r\n\r\nIf you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","created":"2025-01-13T14:13:48.931+0000","updated":"2025-01-13T14:13:48.931+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17914476","author":{"displayName":"Ismael Juma"},"body":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","body_raw":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","created":"2025-01-19T17:26:45.392+0000","updated":"2025-01-19T17:28:45.849+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17950705","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac any updates?","body_raw":"[~dajac] any updates?","created":"2025-05-10T17:30:03.290+0000","updated":"2025-05-10T17:30:03.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17951449","author":{"displayName":"David Jacot"},"body":"chia7712 No progress on this one. Feel free to take it if you have bandwidth.","body_raw":"[~chia7712] No progress on this one. Feel free to take it if you have bandwidth.","created":"2025-05-14T18:02:29.370+0000","updated":"2025-05-14T18:02:29.370+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17951534","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac thanks for response. taijuwu and I will take over it :)","body_raw":"[~dajac] thanks for response. [~taijuwu] and I will take over it :)","created":"2025-05-15T00:56:16.603+0000","updated":"2025-05-15T00:56:16.603+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986782","author":{"displayName":"Chia-Ping Tsai"},"body":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","body_raw":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","created":"2025-06-29T17:21:57.051+0000","updated":"2025-06-29T17:21:57.051+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ReplicaManager#becomeLeaderOrFollower\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18486","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:03.000+0000","updated":"2025-06-29T17:21:57.000+0000","resolved":"2025-06-29T17:21:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17912547","author":{"displayName":"Chu Cheng Li"},"body":"Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","body_raw":"Hi [~dajac],\r\n\r\nWhile working on KAFKA-18491\r\n\r\nI noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}.\r\n\r\nIf you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","created":"2025-01-13T14:13:48.931+0000","updated":"2025-01-13T14:13:48.931+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17914476","author":{"displayName":"Ismael Juma"},"body":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","body_raw":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","created":"2025-01-19T17:26:45.392+0000","updated":"2025-01-19T17:28:45.849+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17950705","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac any updates?","body_raw":"[~dajac] any updates?","created":"2025-05-10T17:30:03.290+0000","updated":"2025-05-10T17:30:03.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17951449","author":{"displayName":"David Jacot"},"body":"chia7712 No progress on this one. Feel free to take it if you have bandwidth.","body_raw":"[~chia7712] No progress on this one. Feel free to take it if you have bandwidth.","created":"2025-05-14T18:02:29.370+0000","updated":"2025-05-14T18:02:29.370+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17951534","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac thanks for response. taijuwu and I will take over it :)","body_raw":"[~dajac] thanks for response. [~taijuwu] and I will take over it :)","created":"2025-05-15T00:56:16.603+0000","updated":"2025-05-15T00:56:16.603+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986782","author":{"displayName":"Chia-Ping Tsai"},"body":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","body_raw":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","created":"2025-06-29T17:21:57.051+0000","updated":"2025-06-29T17:21:57.051+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ReplicaManager#becomeLeaderOrFollower\n\n\n\nConversation:\nuser: Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your \nassistant: We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.\nassistant: dajac any updates?\nassistant: chia7712 No progress on this one. Feel free to take it if you have bandwidth.\nassistant: dajac thanks for response. taijuwu and I will take over it :)","output":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","metadata":{"issue_id":"KAFKA-18486","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-13T10:39:03.000+0000","updated":"2025-06-29T17:21:57.000+0000","resolved":"2025-06-29T17:21:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17912547","author":{"displayName":"Chu Cheng Li"},"body":"Hi dajac, While working on KAFKA-18491 I noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}. If you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","body_raw":"Hi [~dajac],\r\n\r\nWhile working on KAFKA-18491\r\n\r\nI noticed that removing the {{controllerEpoch}} has some dependencies with {{becomeLeaderOrFollower}} and {{{}stopReplicas{}}}.\r\n\r\nIf you're currently occupied and don't have the bandwidth to handle this ticket, I'd be happy to take it on. Please let me know your thoughts. Thanks!","created":"2025-01-13T14:13:48.931+0000","updated":"2025-01-13T14:13:48.931+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17914476","author":{"displayName":"Ismael Juma"},"body":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","body_raw":"We should also remove LeaderAndIsrRequest as part of this (or file a separate ticket for that part if it turns out to be complicated) - note that we have to make sure we don't remove tests that were using zk mechanisms but are still relevant in kraft mode.","created":"2025-01-19T17:26:45.392+0000","updated":"2025-01-19T17:28:45.849+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17950705","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac any updates?","body_raw":"[~dajac] any updates?","created":"2025-05-10T17:30:03.290+0000","updated":"2025-05-10T17:30:03.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17951449","author":{"displayName":"David Jacot"},"body":"chia7712 No progress on this one. Feel free to take it if you have bandwidth.","body_raw":"[~chia7712] No progress on this one. Feel free to take it if you have bandwidth.","created":"2025-05-14T18:02:29.370+0000","updated":"2025-05-14T18:02:29.370+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17951534","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac thanks for response. taijuwu and I will take over it :)","body_raw":"[~dajac] thanks for response. [~taijuwu] and I will take over it :)","created":"2025-05-15T00:56:16.603+0000","updated":"2025-05-15T00:56:16.603+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986782","author":{"displayName":"Chia-Ping Tsai"},"body":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","body_raw":"There are some follow-up cleanups, but the target function has been removed. Hence, closing this PR as resolved","created":"2025-06-29T17:21:57.051+0000","updated":"2025-06-29T17:21:57.051+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"More robust exception handling for new group coordinator unload\n\nBugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"More robust exception handling for new group coordinator unload","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"More robust exception handling for new group coordinator unload\n\nBugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"Resolved","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"More robust exception handling for new group coordinator unload\n\nBugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"Blocker","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: More robust exception handling for new group coordinator unload\n\nBugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"Bugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving t","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"More robust exception handling for new group coordinator unload\n\nBugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"Fixed","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Bugs like KAFKA-18174 can throw exceptions during coordinator unload. When this happens, the coordinator context is not removed and a subsequent coordinator load will fail, leaving the coordinator in an unavailable state. As an improvement, it would be nice to guard against bugs like these leaving the coordinator in a stuck state.","output":"More robust exception handling for new group coordinator unload","metadata":{"issue_id":"KAFKA-18484","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Sean Quah"},"reporter":{"displayName":"Sean Quah"},"created":"2025-01-13T07:50:23.000+0000","updated":"2025-01-24T05:14:48.000+0000","resolved":"2025-01-23T16:16:51.000+0000","resolution":"Fixed","labels":[],"components":["group-coordinator"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Protect `Loggers` and `Log4jController` against `LinkageError`s\n\nSince KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Protect `Loggers` and `Log4jController` against `LinkageError`s","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Protect `Loggers` and `Log4jController` against `LinkageError`s\n\nSince KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Resolved","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Protect `Loggers` and `Log4jController` against `LinkageError`s\n\nSince KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Minor","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Protect `Loggers` and `Log4jController` against `LinkageError`s\n\nSince KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Since KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Protect `Loggers` and `Log4jController` against `LinkageError`s\n\nSince KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Fixed","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Since KAFKA-18466 removed the dependency of {{kafka_ }} and {{connect-runtime}} from Log4j Core a linkage error will occur each time an application accesses the {{Log4jController}} and {{Loggers}} classes.","output":"Protect `Loggers` and `Log4jController` against `LinkageError`s","metadata":{"issue_id":"KAFKA-18483","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Piotr Karwasz"},"created":"2025-01-12T19:55:20.000+0000","updated":"2025-02-06T16:04:52.000+0000","resolved":"2025-02-06T16:04:52.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/streams/streams_relational_smoke_test.py\", line 95, in test_streams driver = StreamsRelationalSmokeTestService(self.test_context, self.kafka, \"driver\", \"ignored\", \"ignored\") File \"/opt/kafka-dev/tests/kafkatest/tests/streams/streams_relational_smoke_test.py\", line 38, in __init__ {code}","output":"Fix failing system test StreamsSmokeTest","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functoo","output":"Resolved","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functoo","output":"Blocker","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functoo","output":"Fail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/duckta","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functoo","output":"Fixed","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix failing system test StreamsSmokeTest\n\nFail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","output":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Fail report: {code:java} AttributeError(\"'ClusterNode' object has no attribute 'version'\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/streams/streams_relational_smoke_test.py\", line 95, in test_streams driver = StreamsRelationalSmokeTestService(self.test_context, self.kafka, \"driver\", \"ignored\", \"ignored\") File \"/opt/kafka-dev/tests/kafk","output":"Fix failing system test StreamsSmokeTest","metadata":{"issue_id":"KAFKA-18482","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Bill Bejeck"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T16:22:20.000+0000","updated":"2025-01-16T17:45:41.000+0000","resolved":"2025-01-15T20:49:58.000+0000","resolution":"Fixed","labels":[],"components":["streams","system tests"],"comment_count":1,"comments":[{"id":"17913446","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f] 4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/f4fca746cc281a5a204a128f57ad6b011efbc15f]\r\n\r\n4.0: https://github.com/apache/kafka/commit/54ff9711d8f466fa10f0b1d0f915dbed0ac86f5c","created":"2025-01-15T20:49:58.375+0000","updated":"2025-01-15T20:49:58.375+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZookeeperClient, KeeperException, Code\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18481","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T15:11:19.000+0000","updated":"2025-02-21T15:40:27.000+0000","resolved":"2025-01-12T15:17:33.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912280","author":{"displayName":"黃竣陽"},"body":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427","body_raw":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427 ","created":"2025-01-12T15:13:52.573+0000","updated":"2025-01-12T15:13:52.573+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912282","author":{"displayName":"TengYao Chi"},"body":"Thanks for pointing out","body_raw":"Thanks for pointing out ","created":"2025-01-12T15:17:52.870+0000","updated":"2025-01-12T15:17:52.870+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZookeeperClient, KeeperException, Code\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18481","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T15:11:19.000+0000","updated":"2025-02-21T15:40:27.000+0000","resolved":"2025-01-12T15:17:33.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912280","author":{"displayName":"黃竣陽"},"body":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427","body_raw":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427 ","created":"2025-01-12T15:13:52.573+0000","updated":"2025-01-12T15:13:52.573+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912282","author":{"displayName":"TengYao Chi"},"body":"Thanks for pointing out","body_raw":"Thanks for pointing out ","created":"2025-01-12T15:17:52.870+0000","updated":"2025-01-12T15:17:52.870+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZookeeperClient, KeeperException, Code\n\n","output":"","metadata":{"issue_id":"KAFKA-18481","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T15:11:19.000+0000","updated":"2025-02-21T15:40:27.000+0000","resolved":"2025-01-12T15:17:33.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912280","author":{"displayName":"黃竣陽"},"body":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427","body_raw":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427 ","created":"2025-01-12T15:13:52.573+0000","updated":"2025-01-12T15:13:52.573+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912282","author":{"displayName":"TengYao Chi"},"body":"Thanks for pointing out","body_raw":"Thanks for pointing out ","created":"2025-01-12T15:17:52.870+0000","updated":"2025-01-12T15:17:52.870+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZookeeperClient, KeeperException, Code\n\n","output":"Duplicate","metadata":{"issue_id":"KAFKA-18481","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T15:11:19.000+0000","updated":"2025-02-21T15:40:27.000+0000","resolved":"2025-01-12T15:17:33.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912280","author":{"displayName":"黃竣陽"},"body":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427","body_raw":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427 ","created":"2025-01-12T15:13:52.573+0000","updated":"2025-01-12T15:13:52.573+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912282","author":{"displayName":"TengYao Chi"},"body":"Thanks for pointing out","body_raw":"Thanks for pointing out ","created":"2025-01-12T15:17:52.870+0000","updated":"2025-01-12T15:17:52.870+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZookeeperClient, KeeperException, Code\n\n\n\nConversation:\nuser: It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427\nassistant: Thanks for pointing out","output":"Thanks for pointing out","metadata":{"issue_id":"KAFKA-18481","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T15:11:19.000+0000","updated":"2025-02-21T15:40:27.000+0000","resolved":"2025-01-12T15:17:33.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912280","author":{"displayName":"黃竣陽"},"body":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427","body_raw":"It's dulpicate with https://issues.apache.org/jira/browse/KAFKA-18427 ","created":"2025-01-12T15:13:52.573+0000","updated":"2025-01-12T15:13:52.573+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912282","author":{"displayName":"TengYao Chi"},"body":"Thanks for pointing out","body_raw":"Thanks for pointing out ","created":"2025-01-12T15:17:52.870+0000","updated":"2025-01-12T15:17:52.870+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.","output":"Fix fail e2e `test_offset_truncate`","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.","output":"Resolved","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.","output":"Blocker","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.","output":"I ran a client e2e on trunk and found a failed test case.","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.","output":"Fixed","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix fail e2e `test_offset_truncate`\n\nI ran a client e2e on trunk and found a failed test case.\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","output":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","metadata":{"issue_id":"KAFKA-18480","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-12T06:01:27.000+0000","updated":"2025-01-18T02:59:19.000+0000","resolved":"2025-01-18T02:59:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914277","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f] 4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","body_raw":"trunk: [https://github.com/apache/kafka/commit/78e35457ee966695bd392aa4e4c16cbfe919ad4f]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/9611d761de433489eb43664a1cf86274549feb5c","created":"2025-01-18T02:59:19.507+0000","updated":"2025-01-18T02:59:19.507+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"remove keepPartitionMetadataFile in UnifiedLog and LogManager\n\nThis is because of the base MV in 4.0 is 3.0.","output":"Resolved","metadata":{"issue_id":"KAFKA-18479","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Dmitry Werner"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T16:13:43.000+0000","updated":"2025-01-16T17:58:27.000+0000","resolved":"2025-01-15T22:01:12.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":3,"comments":[{"id":"17912204","author":{"displayName":"Dmitry Werner"},"body":"junrao Hello, can I take this?","body_raw":"[~junrao] Hello, can I take this?","created":"2025-01-11T16:39:52.041+0000","updated":"2025-01-11T16:39:52.041+0000","updateAuthor":{"displayName":"Dmitry Werner"}},{"id":"17912206","author":{"displayName":"Ismael Juma"},"body":"Yes.","body_raw":"Yes.","created":"2025-01-11T16:42:23.620+0000","updated":"2025-01-11T16:42:23.620+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17913462","author":{"displayName":"Jun Rao"},"body":"merged [https://github.com/apache/kafka/pull/18491] to trunk","body_raw":"merged [https://github.com/apache/kafka/pull/18491] to trunk","created":"2025-01-15T22:01:12.076+0000","updated":"2025-01-15T22:01:12.076+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"remove keepPartitionMetadataFile in UnifiedLog and LogManager\n\nThis is because of the base MV in 4.0 is 3.0.","output":"Major","metadata":{"issue_id":"KAFKA-18479","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Dmitry Werner"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T16:13:43.000+0000","updated":"2025-01-16T17:58:27.000+0000","resolved":"2025-01-15T22:01:12.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":3,"comments":[{"id":"17912204","author":{"displayName":"Dmitry Werner"},"body":"junrao Hello, can I take this?","body_raw":"[~junrao] Hello, can I take this?","created":"2025-01-11T16:39:52.041+0000","updated":"2025-01-11T16:39:52.041+0000","updateAuthor":{"displayName":"Dmitry Werner"}},{"id":"17912206","author":{"displayName":"Ismael Juma"},"body":"Yes.","body_raw":"Yes.","created":"2025-01-11T16:42:23.620+0000","updated":"2025-01-11T16:42:23.620+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17913462","author":{"displayName":"Jun Rao"},"body":"merged [https://github.com/apache/kafka/pull/18491] to trunk","body_raw":"merged [https://github.com/apache/kafka/pull/18491] to trunk","created":"2025-01-15T22:01:12.076+0000","updated":"2025-01-15T22:01:12.076+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: remove keepPartitionMetadataFile in UnifiedLog and LogManager\n\nThis is because of the base MV in 4.0 is 3.0.","output":"This is because of the base MV in 4.0 is 3.0.","metadata":{"issue_id":"KAFKA-18479","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Dmitry Werner"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T16:13:43.000+0000","updated":"2025-01-16T17:58:27.000+0000","resolved":"2025-01-15T22:01:12.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":3,"comments":[{"id":"17912204","author":{"displayName":"Dmitry Werner"},"body":"junrao Hello, can I take this?","body_raw":"[~junrao] Hello, can I take this?","created":"2025-01-11T16:39:52.041+0000","updated":"2025-01-11T16:39:52.041+0000","updateAuthor":{"displayName":"Dmitry Werner"}},{"id":"17912206","author":{"displayName":"Ismael Juma"},"body":"Yes.","body_raw":"Yes.","created":"2025-01-11T16:42:23.620+0000","updated":"2025-01-11T16:42:23.620+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17913462","author":{"displayName":"Jun Rao"},"body":"merged [https://github.com/apache/kafka/pull/18491] to trunk","body_raw":"merged [https://github.com/apache/kafka/pull/18491] to trunk","created":"2025-01-15T22:01:12.076+0000","updated":"2025-01-15T22:01:12.076+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"remove keepPartitionMetadataFile in UnifiedLog and LogManager\n\nThis is because of the base MV in 4.0 is 3.0.","output":"Fixed","metadata":{"issue_id":"KAFKA-18479","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Dmitry Werner"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T16:13:43.000+0000","updated":"2025-01-16T17:58:27.000+0000","resolved":"2025-01-15T22:01:12.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":3,"comments":[{"id":"17912204","author":{"displayName":"Dmitry Werner"},"body":"junrao Hello, can I take this?","body_raw":"[~junrao] Hello, can I take this?","created":"2025-01-11T16:39:52.041+0000","updated":"2025-01-11T16:39:52.041+0000","updateAuthor":{"displayName":"Dmitry Werner"}},{"id":"17912206","author":{"displayName":"Ismael Juma"},"body":"Yes.","body_raw":"Yes.","created":"2025-01-11T16:42:23.620+0000","updated":"2025-01-11T16:42:23.620+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17913462","author":{"displayName":"Jun Rao"},"body":"merged [https://github.com/apache/kafka/pull/18491] to trunk","body_raw":"merged [https://github.com/apache/kafka/pull/18491] to trunk","created":"2025-01-15T22:01:12.076+0000","updated":"2025-01-15T22:01:12.076+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"remove keepPartitionMetadataFile in UnifiedLog and LogManager\n\nThis is because of the base MV in 4.0 is 3.0.\n\nConversation:\nuser: junrao Hello, can I take this?\nassistant: Yes.\nassistant: merged [https://github.com/apache/kafka/pull/18491] to trunk","output":"merged [https://github.com/apache/kafka/pull/18491] to trunk","metadata":{"issue_id":"KAFKA-18479","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Dmitry Werner"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T16:13:43.000+0000","updated":"2025-01-16T17:58:27.000+0000","resolved":"2025-01-15T22:01:12.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":3,"comments":[{"id":"17912204","author":{"displayName":"Dmitry Werner"},"body":"junrao Hello, can I take this?","body_raw":"[~junrao] Hello, can I take this?","created":"2025-01-11T16:39:52.041+0000","updated":"2025-01-11T16:39:52.041+0000","updateAuthor":{"displayName":"Dmitry Werner"}},{"id":"17912206","author":{"displayName":"Ismael Juma"},"body":"Yes.","body_raw":"Yes.","created":"2025-01-11T16:42:23.620+0000","updated":"2025-01-11T16:42:23.620+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17913462","author":{"displayName":"Jun Rao"},"body":"merged [https://github.com/apache/kafka/pull/18491] to trunk","body_raw":"merged [https://github.com/apache/kafka/pull/18491] to trunk","created":"2025-01-15T22:01:12.076+0000","updated":"2025-01-15T22:01:12.076+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config\n\nThe newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config\n\nThe newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"Resolved","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config\n\nThe newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"Major","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: RocksDBTimeOrderedKeyValueBuffer does only use serdes from config\n\nThe newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"The newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` a","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config\n\nThe newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"Fixed","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The newly added stream-table join with grace-period, added a new RocksDBTimeOrderedKeyValueBuffer store. This new store's key and value serializers are not initialized correctly, but are always `null`, resulting in falling back to use the serdes from StreamsConfig. Thus, serdes passed via `Joined` are ignored.","output":"RocksDBTimeOrderedKeyValueBuffer does only use serdes from config","metadata":{"issue_id":"KAFKA-18478","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-11T02:54:18.000+0000","updated":"2025-01-15T22:10:33.000+0000","resolved":"2025-01-15T19:56:10.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread\n\nThis is because of the base MV in 4.0 is 3.0.","output":"Open","metadata":{"issue_id":"KAFKA-18477","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T00:26:28.000+0000","updated":"2025-10-24T03:20:56.000+0000","resolved":null,"labels":[],"components":["core"],"comment_count":17,"comments":[{"id":"17912156","author":{"displayName":"黃竣陽"},"body":"Hello junrao, If you wont work on this, may I take it?","body_raw":"Hello [~junrao], If you wont work on this, may I take it?","created":"2025-01-11T00:28:06.226+0000","updated":"2025-01-11T00:28:06.226+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912160","author":{"displayName":"Jun Rao"},"body":"m1a2st : Thanks for helping out. Feel free to take this.","body_raw":"[~m1a2st] : Thanks for helping out. Feel free to take this.","created":"2025-01-11T01:33:27.458+0000","updated":"2025-01-11T01:33:27.458+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17923701","author":{"displayName":"黃竣陽"},"body":"Hello junrao I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this? I think I missed something, Thanks for helping :)","body_raw":"Hello [~junrao] \r\nI have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this?\r\nI think I missed something, Thanks for helping :)","created":"2025-02-04T13:31:57.426+0000","updated":"2025-02-04T13:39:25.766+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17924745","author":{"displayName":"Jun Rao"},"body":"m1a2st : In AbstractFetcherThread, we have the following code. {code:java} private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) { currentState } else if (initialFetchState.initOffset < 0) { fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch) } else if (leader.isTruncationOnFetchSupported) { // With old message format, `latestEpoch` will be empty and we use Truncating state // to truncate to high watermark. val lastFetchedEpoch = latestEpoch(tp) val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state, lastFetchedEpoch) } else { PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state = Truncating, lastFetchedEpoch = None) } } {code} leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","body_raw":"[~m1a2st] :\r\n\r\nIn AbstractFetcherThread, we have the following code.\r\n{code:java}\r\nprivate def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\r\n  if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\r\n    currentState\r\n  } else if (initialFetchState.initOffset < 0) {\r\n    fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch)\r\n  } else if (leader.isTruncationOnFetchSupported) {\r\n    // With old message format, `latestEpoch` will be empty and we use Truncating state\r\n    // to truncate to high watermark.\r\n    val lastFetchedEpoch = latestEpoch(tp)\r\n    val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state, lastFetchedEpoch)\r\n  } else {\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state = Truncating, lastFetchedEpoch = None)\r\n  }\r\n} {code}\r\nleader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","created":"2025-02-07T01:07:35.539+0000","updated":"2025-02-07T01:07:55.842+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17925595","author":{"displayName":"黃竣陽"},"body":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} `leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is RemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)` [1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","body_raw":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n{quote}\r\n`leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is \r\nRemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)`\r\n[1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","created":"2025-02-10T14:41:40.640+0000","updated":"2025-02-10T14:41:40.640+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17928899","author":{"displayName":"Jun Rao"},"body":"m1a2st : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details. 1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that. 2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","body_raw":"[~m1a2st] : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details.\r\n\r\n1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that.\r\n\r\n2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","created":"2025-02-20T20:05:07.916+0000","updated":"2025-02-20T20:05:07.916+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17931623","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} junrao Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","body_raw":"{quote}\r\n\r\nsince we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n\r\n{quote}\r\n\r\n[~junrao] Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","created":"2025-03-01T06:39:32.050+0000","updated":"2025-03-01T06:39:32.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17931679","author":{"displayName":"Ismael Juma"},"body":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","body_raw":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","created":"2025-03-01T16:25:02.894+0000","updated":"2025-03-01T16:25:02.894+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17931915","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. {quote} I understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right? {quote} latestEpoch(tp) should always be available since we only have V2 message format in 4.0 {quote} Regardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0. {code:java} [2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread) [2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) {code}","body_raw":"{quote}\r\nIt's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. \r\n{quote}\r\n\r\nI understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right?\r\n\r\n{quote}\r\nlatestEpoch(tp) should always be available since we only have V2 message format in 4.0\r\n{quote}\r\n\r\nRegardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0.\r\n\r\n{code:java}\r\n[2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)\r\n[2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)\r\n{code}\r\n\r\n\r\n ","created":"2025-03-03T09:33:11.221+0000","updated":"2025-03-03T09:33:11.221+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932050","author":{"displayName":"Jun Rao"},"body":"chia7712 : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. When a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. So, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","body_raw":"[~chia7712] : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n\r\n \r\n\r\nWhen a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. \r\n\r\n \r\n\r\nSo, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","created":"2025-03-03T18:40:54.573+0000","updated":"2025-03-03T18:40:54.573+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932386","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your explanation. {quote} If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. {quote} Pardon me, do you mean the truncation logic using the high watermark can be removed?","body_raw":"[~junrao] thanks for your explanation.\r\n\r\n{quote}\r\nIf there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n{quote}\r\n\r\nPardon me, do you mean the truncation logic using the high watermark can be removed?","created":"2025-03-04T17:03:44.059+0000","updated":"2025-03-04T17:03:44.059+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932418","author":{"displayName":"Jun Rao"},"body":"{quote}do you mean the truncation logic using the high watermark can be removed? {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark(). {code:java} private def maybeTruncate(): Unit = { val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions() if (partitionsWithEpochs.nonEmpty) { truncateToEpochEndOffsets(partitionsWithEpochs) } if (partitionsWithoutEpochs.nonEmpty) { truncateToHighWatermark(partitionsWithoutEpochs) } } {code}","body_raw":"{quote}do you mean the truncation logic using the high watermark can be removed?\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark().\r\n{code:java}\r\nprivate def maybeTruncate(): Unit = {\r\n  val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions()\r\n  if (partitionsWithEpochs.nonEmpty) {\r\n    truncateToEpochEndOffsets(partitionsWithEpochs)\r\n  }\r\n  if (partitionsWithoutEpochs.nonEmpty) {\r\n    truncateToHighWatermark(partitionsWithoutEpochs)\r\n  }\r\n} {code}","created":"2025-03-04T19:54:10.473+0000","updated":"2025-03-04T19:54:10.473+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932421","author":{"displayName":"Jun Rao"},"body":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range.","body_raw":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range. ","created":"2025-03-04T20:12:18.720+0000","updated":"2025-03-04T20:12:18.720+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932499","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your patience!. {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs {quote} I was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. Also, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO? {code:java} protected def initialFetchOffset(log: UnifiedLog): Long = { if (log.latestEpoch.nonEmpty) log.logEndOffset else log.highWatermark } {code} https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","body_raw":"[~junrao] thanks for your patience!.\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs\r\n{quote}\r\n\r\nI was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. \r\n\r\nAlso, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO?\r\n\r\n{code:java}\r\n  protected def initialFetchOffset(log: UnifiedLog): Long = {\r\n    if (log.latestEpoch.nonEmpty)\r\n      log.logEndOffset\r\n    else\r\n      log.highWatermark\r\n  }\r\n{code}\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","created":"2025-03-05T04:11:14.564+0000","updated":"2025-03-05T04:11:14.564+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932712","author":{"displayName":"Jun Rao"},"body":"chia7712 : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","body_raw":"[~chia7712] : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","created":"2025-03-05T18:36:55.218+0000","updated":"2025-03-05T18:36:55.218+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932965","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao Thank you for all your responses. I finally understand this issue :)","body_raw":"[~junrao] Thank you for all your responses. I finally understand this issue :)","created":"2025-03-06T11:36:42.219+0000","updated":"2025-03-06T11:36:42.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986061","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:31:20.106+0000","updated":"2025-06-25T09:31:20.106+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread\n\nThis is because of the base MV in 4.0 is 3.0.","output":"Major","metadata":{"issue_id":"KAFKA-18477","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T00:26:28.000+0000","updated":"2025-10-24T03:20:56.000+0000","resolved":null,"labels":[],"components":["core"],"comment_count":17,"comments":[{"id":"17912156","author":{"displayName":"黃竣陽"},"body":"Hello junrao, If you wont work on this, may I take it?","body_raw":"Hello [~junrao], If you wont work on this, may I take it?","created":"2025-01-11T00:28:06.226+0000","updated":"2025-01-11T00:28:06.226+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912160","author":{"displayName":"Jun Rao"},"body":"m1a2st : Thanks for helping out. Feel free to take this.","body_raw":"[~m1a2st] : Thanks for helping out. Feel free to take this.","created":"2025-01-11T01:33:27.458+0000","updated":"2025-01-11T01:33:27.458+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17923701","author":{"displayName":"黃竣陽"},"body":"Hello junrao I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this? I think I missed something, Thanks for helping :)","body_raw":"Hello [~junrao] \r\nI have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this?\r\nI think I missed something, Thanks for helping :)","created":"2025-02-04T13:31:57.426+0000","updated":"2025-02-04T13:39:25.766+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17924745","author":{"displayName":"Jun Rao"},"body":"m1a2st : In AbstractFetcherThread, we have the following code. {code:java} private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) { currentState } else if (initialFetchState.initOffset < 0) { fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch) } else if (leader.isTruncationOnFetchSupported) { // With old message format, `latestEpoch` will be empty and we use Truncating state // to truncate to high watermark. val lastFetchedEpoch = latestEpoch(tp) val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state, lastFetchedEpoch) } else { PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state = Truncating, lastFetchedEpoch = None) } } {code} leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","body_raw":"[~m1a2st] :\r\n\r\nIn AbstractFetcherThread, we have the following code.\r\n{code:java}\r\nprivate def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\r\n  if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\r\n    currentState\r\n  } else if (initialFetchState.initOffset < 0) {\r\n    fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch)\r\n  } else if (leader.isTruncationOnFetchSupported) {\r\n    // With old message format, `latestEpoch` will be empty and we use Truncating state\r\n    // to truncate to high watermark.\r\n    val lastFetchedEpoch = latestEpoch(tp)\r\n    val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state, lastFetchedEpoch)\r\n  } else {\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state = Truncating, lastFetchedEpoch = None)\r\n  }\r\n} {code}\r\nleader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","created":"2025-02-07T01:07:35.539+0000","updated":"2025-02-07T01:07:55.842+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17925595","author":{"displayName":"黃竣陽"},"body":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} `leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is RemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)` [1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","body_raw":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n{quote}\r\n`leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is \r\nRemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)`\r\n[1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","created":"2025-02-10T14:41:40.640+0000","updated":"2025-02-10T14:41:40.640+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17928899","author":{"displayName":"Jun Rao"},"body":"m1a2st : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details. 1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that. 2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","body_raw":"[~m1a2st] : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details.\r\n\r\n1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that.\r\n\r\n2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","created":"2025-02-20T20:05:07.916+0000","updated":"2025-02-20T20:05:07.916+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17931623","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} junrao Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","body_raw":"{quote}\r\n\r\nsince we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n\r\n{quote}\r\n\r\n[~junrao] Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","created":"2025-03-01T06:39:32.050+0000","updated":"2025-03-01T06:39:32.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17931679","author":{"displayName":"Ismael Juma"},"body":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","body_raw":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","created":"2025-03-01T16:25:02.894+0000","updated":"2025-03-01T16:25:02.894+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17931915","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. {quote} I understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right? {quote} latestEpoch(tp) should always be available since we only have V2 message format in 4.0 {quote} Regardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0. {code:java} [2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread) [2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) {code}","body_raw":"{quote}\r\nIt's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. \r\n{quote}\r\n\r\nI understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right?\r\n\r\n{quote}\r\nlatestEpoch(tp) should always be available since we only have V2 message format in 4.0\r\n{quote}\r\n\r\nRegardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0.\r\n\r\n{code:java}\r\n[2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)\r\n[2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)\r\n{code}\r\n\r\n\r\n ","created":"2025-03-03T09:33:11.221+0000","updated":"2025-03-03T09:33:11.221+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932050","author":{"displayName":"Jun Rao"},"body":"chia7712 : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. When a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. So, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","body_raw":"[~chia7712] : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n\r\n \r\n\r\nWhen a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. \r\n\r\n \r\n\r\nSo, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","created":"2025-03-03T18:40:54.573+0000","updated":"2025-03-03T18:40:54.573+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932386","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your explanation. {quote} If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. {quote} Pardon me, do you mean the truncation logic using the high watermark can be removed?","body_raw":"[~junrao] thanks for your explanation.\r\n\r\n{quote}\r\nIf there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n{quote}\r\n\r\nPardon me, do you mean the truncation logic using the high watermark can be removed?","created":"2025-03-04T17:03:44.059+0000","updated":"2025-03-04T17:03:44.059+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932418","author":{"displayName":"Jun Rao"},"body":"{quote}do you mean the truncation logic using the high watermark can be removed? {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark(). {code:java} private def maybeTruncate(): Unit = { val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions() if (partitionsWithEpochs.nonEmpty) { truncateToEpochEndOffsets(partitionsWithEpochs) } if (partitionsWithoutEpochs.nonEmpty) { truncateToHighWatermark(partitionsWithoutEpochs) } } {code}","body_raw":"{quote}do you mean the truncation logic using the high watermark can be removed?\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark().\r\n{code:java}\r\nprivate def maybeTruncate(): Unit = {\r\n  val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions()\r\n  if (partitionsWithEpochs.nonEmpty) {\r\n    truncateToEpochEndOffsets(partitionsWithEpochs)\r\n  }\r\n  if (partitionsWithoutEpochs.nonEmpty) {\r\n    truncateToHighWatermark(partitionsWithoutEpochs)\r\n  }\r\n} {code}","created":"2025-03-04T19:54:10.473+0000","updated":"2025-03-04T19:54:10.473+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932421","author":{"displayName":"Jun Rao"},"body":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range.","body_raw":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range. ","created":"2025-03-04T20:12:18.720+0000","updated":"2025-03-04T20:12:18.720+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932499","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your patience!. {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs {quote} I was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. Also, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO? {code:java} protected def initialFetchOffset(log: UnifiedLog): Long = { if (log.latestEpoch.nonEmpty) log.logEndOffset else log.highWatermark } {code} https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","body_raw":"[~junrao] thanks for your patience!.\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs\r\n{quote}\r\n\r\nI was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. \r\n\r\nAlso, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO?\r\n\r\n{code:java}\r\n  protected def initialFetchOffset(log: UnifiedLog): Long = {\r\n    if (log.latestEpoch.nonEmpty)\r\n      log.logEndOffset\r\n    else\r\n      log.highWatermark\r\n  }\r\n{code}\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","created":"2025-03-05T04:11:14.564+0000","updated":"2025-03-05T04:11:14.564+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932712","author":{"displayName":"Jun Rao"},"body":"chia7712 : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","body_raw":"[~chia7712] : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","created":"2025-03-05T18:36:55.218+0000","updated":"2025-03-05T18:36:55.218+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932965","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao Thank you for all your responses. I finally understand this issue :)","body_raw":"[~junrao] Thank you for all your responses. I finally understand this issue :)","created":"2025-03-06T11:36:42.219+0000","updated":"2025-03-06T11:36:42.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986061","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:31:20.106+0000","updated":"2025-06-25T09:31:20.106+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread\n\nThis is because of the base MV in 4.0 is 3.0.","output":"This is because of the base MV in 4.0 is 3.0.","metadata":{"issue_id":"KAFKA-18477","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T00:26:28.000+0000","updated":"2025-10-24T03:20:56.000+0000","resolved":null,"labels":[],"components":["core"],"comment_count":17,"comments":[{"id":"17912156","author":{"displayName":"黃竣陽"},"body":"Hello junrao, If you wont work on this, may I take it?","body_raw":"Hello [~junrao], If you wont work on this, may I take it?","created":"2025-01-11T00:28:06.226+0000","updated":"2025-01-11T00:28:06.226+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912160","author":{"displayName":"Jun Rao"},"body":"m1a2st : Thanks for helping out. Feel free to take this.","body_raw":"[~m1a2st] : Thanks for helping out. Feel free to take this.","created":"2025-01-11T01:33:27.458+0000","updated":"2025-01-11T01:33:27.458+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17923701","author":{"displayName":"黃竣陽"},"body":"Hello junrao I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this? I think I missed something, Thanks for helping :)","body_raw":"Hello [~junrao] \r\nI have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this?\r\nI think I missed something, Thanks for helping :)","created":"2025-02-04T13:31:57.426+0000","updated":"2025-02-04T13:39:25.766+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17924745","author":{"displayName":"Jun Rao"},"body":"m1a2st : In AbstractFetcherThread, we have the following code. {code:java} private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) { currentState } else if (initialFetchState.initOffset < 0) { fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch) } else if (leader.isTruncationOnFetchSupported) { // With old message format, `latestEpoch` will be empty and we use Truncating state // to truncate to high watermark. val lastFetchedEpoch = latestEpoch(tp) val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state, lastFetchedEpoch) } else { PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state = Truncating, lastFetchedEpoch = None) } } {code} leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","body_raw":"[~m1a2st] :\r\n\r\nIn AbstractFetcherThread, we have the following code.\r\n{code:java}\r\nprivate def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\r\n  if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\r\n    currentState\r\n  } else if (initialFetchState.initOffset < 0) {\r\n    fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch)\r\n  } else if (leader.isTruncationOnFetchSupported) {\r\n    // With old message format, `latestEpoch` will be empty and we use Truncating state\r\n    // to truncate to high watermark.\r\n    val lastFetchedEpoch = latestEpoch(tp)\r\n    val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state, lastFetchedEpoch)\r\n  } else {\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state = Truncating, lastFetchedEpoch = None)\r\n  }\r\n} {code}\r\nleader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","created":"2025-02-07T01:07:35.539+0000","updated":"2025-02-07T01:07:55.842+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17925595","author":{"displayName":"黃竣陽"},"body":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} `leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is RemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)` [1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","body_raw":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n{quote}\r\n`leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is \r\nRemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)`\r\n[1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","created":"2025-02-10T14:41:40.640+0000","updated":"2025-02-10T14:41:40.640+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17928899","author":{"displayName":"Jun Rao"},"body":"m1a2st : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details. 1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that. 2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","body_raw":"[~m1a2st] : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details.\r\n\r\n1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that.\r\n\r\n2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","created":"2025-02-20T20:05:07.916+0000","updated":"2025-02-20T20:05:07.916+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17931623","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} junrao Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","body_raw":"{quote}\r\n\r\nsince we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n\r\n{quote}\r\n\r\n[~junrao] Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","created":"2025-03-01T06:39:32.050+0000","updated":"2025-03-01T06:39:32.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17931679","author":{"displayName":"Ismael Juma"},"body":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","body_raw":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","created":"2025-03-01T16:25:02.894+0000","updated":"2025-03-01T16:25:02.894+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17931915","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. {quote} I understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right? {quote} latestEpoch(tp) should always be available since we only have V2 message format in 4.0 {quote} Regardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0. {code:java} [2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread) [2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) {code}","body_raw":"{quote}\r\nIt's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. \r\n{quote}\r\n\r\nI understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right?\r\n\r\n{quote}\r\nlatestEpoch(tp) should always be available since we only have V2 message format in 4.0\r\n{quote}\r\n\r\nRegardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0.\r\n\r\n{code:java}\r\n[2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)\r\n[2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)\r\n{code}\r\n\r\n\r\n ","created":"2025-03-03T09:33:11.221+0000","updated":"2025-03-03T09:33:11.221+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932050","author":{"displayName":"Jun Rao"},"body":"chia7712 : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. When a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. So, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","body_raw":"[~chia7712] : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n\r\n \r\n\r\nWhen a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. \r\n\r\n \r\n\r\nSo, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","created":"2025-03-03T18:40:54.573+0000","updated":"2025-03-03T18:40:54.573+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932386","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your explanation. {quote} If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. {quote} Pardon me, do you mean the truncation logic using the high watermark can be removed?","body_raw":"[~junrao] thanks for your explanation.\r\n\r\n{quote}\r\nIf there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n{quote}\r\n\r\nPardon me, do you mean the truncation logic using the high watermark can be removed?","created":"2025-03-04T17:03:44.059+0000","updated":"2025-03-04T17:03:44.059+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932418","author":{"displayName":"Jun Rao"},"body":"{quote}do you mean the truncation logic using the high watermark can be removed? {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark(). {code:java} private def maybeTruncate(): Unit = { val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions() if (partitionsWithEpochs.nonEmpty) { truncateToEpochEndOffsets(partitionsWithEpochs) } if (partitionsWithoutEpochs.nonEmpty) { truncateToHighWatermark(partitionsWithoutEpochs) } } {code}","body_raw":"{quote}do you mean the truncation logic using the high watermark can be removed?\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark().\r\n{code:java}\r\nprivate def maybeTruncate(): Unit = {\r\n  val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions()\r\n  if (partitionsWithEpochs.nonEmpty) {\r\n    truncateToEpochEndOffsets(partitionsWithEpochs)\r\n  }\r\n  if (partitionsWithoutEpochs.nonEmpty) {\r\n    truncateToHighWatermark(partitionsWithoutEpochs)\r\n  }\r\n} {code}","created":"2025-03-04T19:54:10.473+0000","updated":"2025-03-04T19:54:10.473+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932421","author":{"displayName":"Jun Rao"},"body":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range.","body_raw":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range. ","created":"2025-03-04T20:12:18.720+0000","updated":"2025-03-04T20:12:18.720+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932499","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your patience!. {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs {quote} I was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. Also, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO? {code:java} protected def initialFetchOffset(log: UnifiedLog): Long = { if (log.latestEpoch.nonEmpty) log.logEndOffset else log.highWatermark } {code} https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","body_raw":"[~junrao] thanks for your patience!.\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs\r\n{quote}\r\n\r\nI was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. \r\n\r\nAlso, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO?\r\n\r\n{code:java}\r\n  protected def initialFetchOffset(log: UnifiedLog): Long = {\r\n    if (log.latestEpoch.nonEmpty)\r\n      log.logEndOffset\r\n    else\r\n      log.highWatermark\r\n  }\r\n{code}\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","created":"2025-03-05T04:11:14.564+0000","updated":"2025-03-05T04:11:14.564+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932712","author":{"displayName":"Jun Rao"},"body":"chia7712 : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","body_raw":"[~chia7712] : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","created":"2025-03-05T18:36:55.218+0000","updated":"2025-03-05T18:36:55.218+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932965","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao Thank you for all your responses. I finally understand this issue :)","body_raw":"[~junrao] Thank you for all your responses. I finally understand this issue :)","created":"2025-03-06T11:36:42.219+0000","updated":"2025-03-06T11:36:42.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986061","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:31:20.106+0000","updated":"2025-06-25T09:31:20.106+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread\n\nThis is because of the base MV in 4.0 is 3.0.\n\nConversation:\nuser: Hello junrao, If you wont work on this, may I take it?\nassistant: m1a2st : Thanks for helping out. Feel free to take this.\nassistant: Hello junrao I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEp\nassistant: m1a2st : In AbstractFetcherThread, we have the following code. {code:java} private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetch\nassistant: {quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating s","output":"Moving to the next release as we're now in code freeze for 4.1.0.","metadata":{"issue_id":"KAFKA-18477","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Jun Rao"},"created":"2025-01-11T00:26:28.000+0000","updated":"2025-10-24T03:20:56.000+0000","resolved":null,"labels":[],"components":["core"],"comment_count":17,"comments":[{"id":"17912156","author":{"displayName":"黃竣陽"},"body":"Hello junrao, If you wont work on this, may I take it?","body_raw":"Hello [~junrao], If you wont work on this, may I take it?","created":"2025-01-11T00:28:06.226+0000","updated":"2025-01-11T00:28:06.226+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17912160","author":{"displayName":"Jun Rao"},"body":"m1a2st : Thanks for helping out. Feel free to take this.","body_raw":"[~m1a2st] : Thanks for helping out. Feel free to take this.","created":"2025-01-11T01:33:27.458+0000","updated":"2025-01-11T01:33:27.458+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17923701","author":{"displayName":"黃竣陽"},"body":"Hello junrao I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this? I think I missed something, Thanks for helping :)","body_raw":"Hello [~junrao] \r\nI have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this?\r\nI think I missed something, Thanks for helping :)","created":"2025-02-04T13:31:57.426+0000","updated":"2025-02-04T13:39:25.766+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17924745","author":{"displayName":"Jun Rao"},"body":"m1a2st : In AbstractFetcherThread, we have the following code. {code:java} private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) { currentState } else if (initialFetchState.initOffset < 0) { fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch) } else if (leader.isTruncationOnFetchSupported) { // With old message format, `latestEpoch` will be empty and we use Truncating state // to truncate to high watermark. val lastFetchedEpoch = latestEpoch(tp) val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state, lastFetchedEpoch) } else { PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state = Truncating, lastFetchedEpoch = None) } } {code} leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","body_raw":"[~m1a2st] :\r\n\r\nIn AbstractFetcherThread, we have the following code.\r\n{code:java}\r\nprivate def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\r\n  if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\r\n    currentState\r\n  } else if (initialFetchState.initOffset < 0) {\r\n    fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch)\r\n  } else if (leader.isTruncationOnFetchSupported) {\r\n    // With old message format, `latestEpoch` will be empty and we use Truncating state\r\n    // to truncate to high watermark.\r\n    val lastFetchedEpoch = latestEpoch(tp)\r\n    val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state, lastFetchedEpoch)\r\n  } else {\r\n    PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\r\n      state = Truncating, lastFetchedEpoch = None)\r\n  }\r\n} {code}\r\nleader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.","created":"2025-02-07T01:07:35.539+0000","updated":"2025-02-07T01:07:55.842+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17925595","author":{"displayName":"黃竣陽"},"body":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} `leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is RemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)` [1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","body_raw":"{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n{quote}\r\n`leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is \r\nRemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)`\r\n[1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58","created":"2025-02-10T14:41:40.640+0000","updated":"2025-02-10T14:41:40.640+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17928899","author":{"displayName":"Jun Rao"},"body":"m1a2st : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details. 1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that. 2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","body_raw":"[~m1a2st] : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details.\r\n\r\n1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that.\r\n\r\n2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.","created":"2025-02-20T20:05:07.916+0000","updated":"2025-02-20T20:05:07.916+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17931623","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} junrao Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","body_raw":"{quote}\r\n\r\nsince we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.\r\n\r\n{quote}\r\n\r\n[~junrao] Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.","created":"2025-03-01T06:39:32.050+0000","updated":"2025-03-01T06:39:32.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17931679","author":{"displayName":"Ismael Juma"},"body":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","body_raw":"It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.","created":"2025-03-01T16:25:02.894+0000","updated":"2025-03-01T16:25:02.894+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17931915","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. {quote} I understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right? {quote} latestEpoch(tp) should always be available since we only have V2 message format in 4.0 {quote} Regardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0. {code:java} [2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread) [2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) {code}","body_raw":"{quote}\r\nIt's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. \r\n{quote}\r\n\r\nI understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right?\r\n\r\n{quote}\r\nlatestEpoch(tp) should always be available since we only have V2 message format in 4.0\r\n{quote}\r\n\r\nRegardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0.\r\n\r\n{code:java}\r\n[2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)\r\n[2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)\r\n{code}\r\n\r\n\r\n ","created":"2025-03-03T09:33:11.221+0000","updated":"2025-03-03T09:33:11.221+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932050","author":{"displayName":"Jun Rao"},"body":"chia7712 : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. When a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. So, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","body_raw":"[~chia7712] : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n\r\n \r\n\r\nWhen a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. \r\n\r\n \r\n\r\nSo, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.","created":"2025-03-03T18:40:54.573+0000","updated":"2025-03-03T18:40:54.573+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932386","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your explanation. {quote} If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. {quote} Pardon me, do you mean the truncation logic using the high watermark can be removed?","body_raw":"[~junrao] thanks for your explanation.\r\n\r\n{quote}\r\nIf there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior.\r\n{quote}\r\n\r\nPardon me, do you mean the truncation logic using the high watermark can be removed?","created":"2025-03-04T17:03:44.059+0000","updated":"2025-03-04T17:03:44.059+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932418","author":{"displayName":"Jun Rao"},"body":"{quote}do you mean the truncation logic using the high watermark can be removed? {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark(). {code:java} private def maybeTruncate(): Unit = { val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions() if (partitionsWithEpochs.nonEmpty) { truncateToEpochEndOffsets(partitionsWithEpochs) } if (partitionsWithoutEpochs.nonEmpty) { truncateToHighWatermark(partitionsWithoutEpochs) } } {code}","body_raw":"{quote}do you mean the truncation logic using the high watermark can be removed?\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark().\r\n{code:java}\r\nprivate def maybeTruncate(): Unit = {\r\n  val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions()\r\n  if (partitionsWithEpochs.nonEmpty) {\r\n    truncateToEpochEndOffsets(partitionsWithEpochs)\r\n  }\r\n  if (partitionsWithoutEpochs.nonEmpty) {\r\n    truncateToHighWatermark(partitionsWithoutEpochs)\r\n  }\r\n} {code}","created":"2025-03-04T19:54:10.473+0000","updated":"2025-03-04T19:54:10.473+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932421","author":{"displayName":"Jun Rao"},"body":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range.","body_raw":"So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range. ","created":"2025-03-04T20:12:18.720+0000","updated":"2025-03-04T20:12:18.720+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932499","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao thanks for your patience!. {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs {quote} I was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. Also, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO? {code:java} protected def initialFetchOffset(log: UnifiedLog): Long = { if (log.latestEpoch.nonEmpty) log.logEndOffset else log.highWatermark } {code} https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","body_raw":"[~junrao] thanks for your patience!.\r\n{quote}\r\nIf the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs\r\n{quote}\r\n\r\nI was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. \r\n\r\nAlso, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO?\r\n\r\n{code:java}\r\n  protected def initialFetchOffset(log: UnifiedLog): Long = {\r\n    if (log.latestEpoch.nonEmpty)\r\n      log.logEndOffset\r\n    else\r\n      log.highWatermark\r\n  }\r\n{code}\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430","created":"2025-03-05T04:11:14.564+0000","updated":"2025-03-05T04:11:14.564+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17932712","author":{"displayName":"Jun Rao"},"body":"chia7712 : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","body_raw":"[~chia7712] : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.","created":"2025-03-05T18:36:55.218+0000","updated":"2025-03-05T18:36:55.218+0000","updateAuthor":{"displayName":"Jun Rao"}},{"id":"17932965","author":{"displayName":"Chia-Ping Tsai"},"body":"junrao Thank you for all your responses. I finally understand this issue :)","body_raw":"[~junrao] Thank you for all your responses. I finally understand this issue :)","created":"2025-03-06T11:36:42.219+0000","updated":"2025-03-06T11:36:42.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17986061","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:31:20.106+0000","updated":"2025-06-25T09:31:20.106+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"KafkaStreams should swallow TransactionAbortedException\n\nA TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is passed into the ProductionExceptionHandler callback, and thus KS would FAIL by default, even after a recoverable error. Instead of passing the exception into the error handler, and let the user code take care of it, KS should just swallow the exception directly, and not call the handler at all. The current workaround to avoid failing for a recoverable error would be, to write a custom ProductionExceptionHandler which always returns CONTINUE for TransactionAbortedException.","output":"KafkaStreams should swallow TransactionAbortedException","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"KafkaStreams should swallow TransactionAbortedException\n\nA TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is ","output":"Resolved","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"KafkaStreams should swallow TransactionAbortedException\n\nA TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is ","output":"Minor","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: KafkaStreams should swallow TransactionAbortedException\n\nA TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is ","output":"A TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"KafkaStreams should swallow TransactionAbortedException\n\nA TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is ","output":"Fixed","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"A TransactionAbortedException could be handed back to the producer `Callback` if there is pending batches which are flushed after `abortTransaction()` was called. KafkaStreams only calls `abortTransaction()` after some previous error (could be fatal, for which we just cleanup and shutdown, but could also be a recoverable error for which KS would cleanup and resume), and thus a TransactionAbortedException is just a follow up error that should be swallowed. Right now, `TransactionAbortedError` is passed into the ProductionExceptionHandler callback, and thus KS would FAIL by default, even after a recoverable error. Instead of passing the exception into the error handler, and let the user code take care of it, KS should just swallow the exception directly, and not call the handler at all. The ","output":"KafkaStreams should swallow TransactionAbortedException","metadata":{"issue_id":"KAFKA-18476","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-10T22:41:25.000+0000","updated":"2025-01-15T02:14:42.000+0000","resolved":"2025-01-15T02:14:42.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread\n\nFlaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.api.PlaintextProducerSendTest&tests.test=testCloseWithZeroTimeoutFromCallerThread(String%2C%20String)%5B2%5D] Fails with : h3. testCloseWithZeroTimeoutFromCallerThread(String, String).quorum=kraft.groupProtocol=consumer org.opentest4j.AssertionFailedError: Fetch response should have no message returned. ==> expected: but was: at app//kafka.api.BaseProducerSendTest.$anonfun$testCloseWithZeroTimeoutFromCallerThread$1(BaseProducerSendTest.scala:522) at app//scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190) at app//kafka.api.BaseProducerSendTest.testCloseWithZeroTimeoutFromCallerThread(BaseProducerSendTest.scala:513)","output":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread\n\nFlaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.api.PlaintextProducerSendTest&tests.test=testCloseWithZeroTimeoutFromCaller","output":"Open","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread\n\nFlaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.api.PlaintextProducerSendTest&tests.test=testCloseWithZeroTimeoutFromCaller","output":"Major","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread\n\nFlaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.api.PlaintextProducerSendTest&tests.test=testCloseWithZeroTimeoutFromCaller","output":"Flaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relati","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread\n\nFlaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [h\n\nConversation:\nuser: Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are se\nassistant: Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42\nassistant: Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","output":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Flaky for new consumer only. Seems not too flaky, but failed recently in this build: [https://github.com/apache/kafka/actions/runs/12712131006/job/35437684253?pr=18050] And previously once on trunk [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.api.PlaintextProducerSendTest&tests.test=testCloseWithZeroTimeoutFromCallerThread(String%2C%20String)%5B2%5D] Fails with : h3. testCloseWithZeroTimeoutFromCallerThread(String, String).quorum=kraft.groupProtocol=consumer org.opentest4j.AssertionFailedError: Fetch response should have no message returned. ==> expected: but was: at app//kafka.api.BaseProducerSendTest.$anonfun","output":"Flaky PlaintextProducerSendTest testCloseWithZeroTimeoutFromCallerThread","metadata":{"issue_id":"KAFKA-18475","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":{"displayName":"Chang-Yu Huang"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-10T20:48:52.000+0000","updated":"2025-10-29T23:24:50.000+0000","resolved":null,"labels":["consumer-threading-refactor"],"components":["consumer"],"comment_count":3,"comments":[{"id":"18033677","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens. I will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","body_raw":"Hello [~lianetm]. I think the bug comes from a race condition introduced by RecordAccumulator#close and RecordAccumulator#batchReady. RecordAccumulator#close sets the closed flag to true, and RecordAccumulator#batchReady thinks the batch is sendable because of the flag. As a result those batches are sent in the same Sender#runOnce call. The assertThrows passes because the response has to be polled in the next Sender#runOnce call, but Sender#run starts to close after the run that the race condition happens.\r\n\r\nI will write a unit test to catch this race condition later on. Can I assign the ticket to myself if you are not working on it?","created":"2025-10-28T20:56:28.499+0000","updated":"2025-10-28T20:56:28.499+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033707","author":{"displayName":"Chang-Yu Huang"},"body":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","body_raw":"Here is the test: https://github.com/jack2012aa/kafka/commit/3452d003a249ea45d69ee95bb4849b01e9dc2c42","created":"2025-10-28T23:28:54.043+0000","updated":"2025-10-28T23:28:54.043+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}},{"id":"18033981","author":{"displayName":"Chang-Yu Huang"},"body":"Hello lianetm. I created a PR to fix the test. Please take a look if you have time. Thank you!","body_raw":"Hello [~lianetm]. I created a PR to fix the test. Please take a look if you have time. Thank you!","created":"2025-10-29T23:24:50.902+0000","updated":"2025-10-29T23:24:50.902+0000","updateAuthor":{"displayName":"Chang-Yu Huang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove zkBroker listener\n\nhttps://github.com/apache/kafka/pull/18465/files#r1910522896","output":"Remove zkBroker listener","metadata":{"issue_id":"KAFKA-18474","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:30:12.000+0000","updated":"2025-01-24T13:58:26.000+0000","resolved":"2025-01-24T13:58:13.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zkBroker listener\n\nhttps://github.com/apache/kafka/pull/18465/files#r1910522896","output":"Resolved","metadata":{"issue_id":"KAFKA-18474","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:30:12.000+0000","updated":"2025-01-24T13:58:26.000+0000","resolved":"2025-01-24T13:58:13.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zkBroker listener\n\nhttps://github.com/apache/kafka/pull/18465/files#r1910522896","output":"Blocker","metadata":{"issue_id":"KAFKA-18474","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:30:12.000+0000","updated":"2025-01-24T13:58:26.000+0000","resolved":"2025-01-24T13:58:13.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zkBroker listener\n\nhttps://github.com/apache/kafka/pull/18465/files#r1910522896","output":"https://github.com/apache/kafka/pull/18465/files#r1910522896","metadata":{"issue_id":"KAFKA-18474","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:30:12.000+0000","updated":"2025-01-24T13:58:26.000+0000","resolved":"2025-01-24T13:58:13.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zkBroker listener\n\nhttps://github.com/apache/kafka/pull/18465/files#r1910522896","output":"Fixed","metadata":{"issue_id":"KAFKA-18474","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:30:12.000+0000","updated":"2025-01-24T13:58:26.000+0000","resolved":"2025-01-24T13:58:13.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Remove DelayedDeleteTopics","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Resolved","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Major","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"We could remove it since Kafka no longer relies on Zookeeper.","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Won't Fix","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove DelayedDeleteTopics\n\nWe could remove it since Kafka no longer relies on Zookeeper.\n\nConversation:\nuser: merge into KAFKA-18407","output":"merge into KAFKA-18407","metadata":{"issue_id":"KAFKA-18473","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-10T15:26:20.000+0000","updated":"2025-01-16T10:18:51.000+0000","resolved":"2025-01-16T10:18:51.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913652","author":{"displayName":"黃竣陽"},"body":"merge into KAFKA-18407","body_raw":"merge into KAFKA-18407","created":"2025-01-16T10:18:51.756+0000","updated":"2025-01-16T10:18:51.756+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove MetadataSupport\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18472","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:10:27.000+0000","updated":"2025-01-16T08:09:33.000+0000","resolved":"2025-01-16T08:09:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912028","author":{"displayName":"黃竣陽"},"body":"Thanks for kangning.","body_raw":"Thanks for [~kangning].","created":"2025-01-10T16:18:10.433+0000","updated":"2025-01-10T16:18:10.433+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove MetadataSupport\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18472","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:10:27.000+0000","updated":"2025-01-16T08:09:33.000+0000","resolved":"2025-01-16T08:09:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912028","author":{"displayName":"黃竣陽"},"body":"Thanks for kangning.","body_raw":"Thanks for [~kangning].","created":"2025-01-10T16:18:10.433+0000","updated":"2025-01-10T16:18:10.433+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove MetadataSupport\n\n","output":"","metadata":{"issue_id":"KAFKA-18472","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:10:27.000+0000","updated":"2025-01-16T08:09:33.000+0000","resolved":"2025-01-16T08:09:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912028","author":{"displayName":"黃竣陽"},"body":"Thanks for kangning.","body_raw":"Thanks for [~kangning].","created":"2025-01-10T16:18:10.433+0000","updated":"2025-01-10T16:18:10.433+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove MetadataSupport\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18472","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:10:27.000+0000","updated":"2025-01-16T08:09:33.000+0000","resolved":"2025-01-16T08:09:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912028","author":{"displayName":"黃竣陽"},"body":"Thanks for kangning.","body_raw":"Thanks for [~kangning].","created":"2025-01-10T16:18:10.433+0000","updated":"2025-01-10T16:18:10.433+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove MetadataSupport\n\n\n\nConversation:\nuser: Thanks for kangning.","output":"Thanks for kangning.","metadata":{"issue_id":"KAFKA-18472","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-10T15:10:27.000+0000","updated":"2025-01-16T08:09:33.000+0000","resolved":"2025-01-16T08:09:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912028","author":{"displayName":"黃竣陽"},"body":"Thanks for kangning.","body_raw":"Thanks for [~kangning].","created":"2025-01-10T16:18:10.433+0000","updated":"2025-01-10T16:18:10.433+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. During this process, multiple threads may simultaneously invoke the method responsible for the copying. This can lead to a situation where one thread successfully completes the operation, while another abruptly has the buffer set to {{null}} during the process. Exception example: {code:java} Exception in thread \"pool-1-thread-3\" java.lang.NullPointerException: Cannot invoke \"java.nio.ByteBuffer.remaining()\" because \"this.keyBuffer\" is null at org.apache.kafka.common.header.internals.RecordHeader.key(RecordHeader.java:45) at br.com.autbank.workflow.TestMainExample.lambda$main$0(TestMainExample.java:36) at java.base/java.lang.Iterable.forEach(Iterable.java:75) at br.com.autbank.workflow.TestMainExample.lambda$main$1(TestMainExample.java:32) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) at java.base/java.lang.Thread.run(Thread.java:1583) {code} Code example for error: {code:java} public class TestMainExample { public static void main(String[] args) throws InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(5); for (int i = 0; i { try { headers.forEach((hdr) -> { if (hdr.value() == null) { throw new IllegalStateException(\"Bug find on value\"); } if (hdr.key() == null) { throw new IllegalStateException(\"Bug find on key\"); } }); } finally { count.countDown(); } }); } count.await(); } } } {code} I did a test synchronizing the method I use to access the headers and this resolved the problem in the context of my application, but I believe the ideal would be to either mark that the class is not thread safe or synchronize access to the bytebuffer. Thank you in advance to the team.","output":"Race conditions when accessing RecordHeader data","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. Du","output":"Resolved","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. Du","output":"Major","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. Du","output":"There is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/j","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. Du","output":"Duplicate","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Race conditions when accessing RecordHeader data\n\nThere is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/i\n\nConversation:\nuser: duplicate to KAFKA-18470","output":"duplicate to KAFKA-18470","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"There is a race condition in the {{RecordHeader}} class of Kafka when an instance is created using the https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38{{{}{}}}|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L38]. In this scenario, when attempting to access the {{key}} or {{{}value{}}}, a process copies the {{ByteBuffer}} into a byte array. During this process, multiple threads may simultaneously invoke the method responsible for the copying. This can lead to a situation where one thread successfully completes the operation, while another abruptly has the buffer set to {{null}} during the process. Exception example: {code:java} Exception","output":"Race conditions when accessing RecordHeader data","metadata":{"issue_id":"KAFKA-18471","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:40:57.000+0000","updated":"2025-02-27T09:32:15.000+0000","resolved":"2025-02-27T09:32:08.000+0000","resolution":"Duplicate","labels":[],"components":["clients"],"comment_count":1,"comments":[{"id":"17931069","author":{"displayName":"Chia-Ping Tsai"},"body":"duplicate to KAFKA-18470","body_raw":"duplicate to KAFKA-18470","created":"2025-02-27T09:32:08.721+0000","updated":"2025-02-27T09:32:08.721+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Race conditions when accessing RecordHeader data\n\n","output":"Open","metadata":{"issue_id":"KAFKA-18470","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:29:16.000+0000","updated":"2025-08-18T15:20:30.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"18014644","author":{"displayName":"Ming-Yen Chung"},"body":"The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","body_raw":"The detailed description is in [KAFKA-18471|https://issues.apache.org/jira/browse/KAFKA-18471]","created":"2025-08-18T15:19:47.004+0000","updated":"2025-08-18T15:20:30.163+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Race conditions when accessing RecordHeader data\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18470","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:29:16.000+0000","updated":"2025-08-18T15:20:30.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"18014644","author":{"displayName":"Ming-Yen Chung"},"body":"The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","body_raw":"The detailed description is in [KAFKA-18471|https://issues.apache.org/jira/browse/KAFKA-18471]","created":"2025-08-18T15:19:47.004+0000","updated":"2025-08-18T15:20:30.163+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Race conditions when accessing RecordHeader data\n\n","output":"","metadata":{"issue_id":"KAFKA-18470","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:29:16.000+0000","updated":"2025-08-18T15:20:30.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"18014644","author":{"displayName":"Ming-Yen Chung"},"body":"The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","body_raw":"The detailed description is in [KAFKA-18471|https://issues.apache.org/jira/browse/KAFKA-18471]","created":"2025-08-18T15:19:47.004+0000","updated":"2025-08-18T15:20:30.163+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Race conditions when accessing RecordHeader data\n\n\n\nConversation:\nuser: The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","output":"The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","metadata":{"issue_id":"KAFKA-18470","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Vinicius Vieira dos Santos"},"created":"2025-01-10T12:29:16.000+0000","updated":"2025-08-18T15:20:30.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"18014644","author":{"displayName":"Ming-Yen Chung"},"body":"The detailed description is in https://issues.apache.org/jira/browse/KAFKA-18471","body_raw":"The detailed description is in [KAFKA-18471|https://issues.apache.org/jira/browse/KAFKA-18471]","created":"2025-08-18T15:19:47.004+0000","updated":"2025-08-18T15:20:30.163+0000","updateAuthor":{"displayName":"Ming-Yen Chung"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes is called before remote metadata cache initialized. [1] [https://github.com/apache/kafka/blob/5684fc7a2ee1a4f29cb6d69d713233ed3c297882/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsRequestManager.java#L529-L534] [2] https://github.com/apache/kafka/blob/5684fc7a2ee1a4f29cb6d69d713233ed3c297882/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java#L180","output":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes","output":"Resolved","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes","output":"Blocker","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes","output":"In AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds).","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes","output":"Fixed","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update\n\nIn AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from \n\nConversation:\nuser: Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!\nassistant: Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","output":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"In AsyncConsumer, the ListOffsetRequest is only retried after the metadata update[1]. However, not every retriable errors are followed by a metadata update, such as the ReplicaNotAvailable error from remote storage. This errors leads to Consumer#offsetsForTimes failing after api timeout(60 seconds). This issue does not occur with ClassicConsumer, which always triggers a metadata update before retrying. [2] This issue is the root cause of the flaky test KAFKA-18036, where consumer#offsetsForTimes is called before remote metadata cache initialized. [1] [https://github.com/apache/kafka/blob/5684fc7a2ee1a4f29cb6d69d713233ed3c297882/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsRequestManager.java#L529-L534] [2] https://github.com/apache/kafka/blob/5684fc7a2ee1a4f29cb","output":"AsyncConsumer fails to retry ListOffsetRequest on ReplicaNotAvailable error without metadata update","metadata":{"issue_id":"KAFKA-18469","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Yu-Lin Chen"},"created":"2025-01-10T10:03:06.000+0000","updated":"2025-01-13T18:16:26.000+0000","resolved":"2025-01-13T18:04:14.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":2,"comments":[{"id":"17911999","author":{"displayName":"Lianet Magrans"},"body":"Thanks for filing and taking on this one Yu-Lin Chen ! I think this is a blocker for 4.0 (cc. dajac) so updated the Jira, and will follow-up on the open PR. Thanks!","body_raw":"Thanks for filing and taking on this one [~Yu-Lin Chen] ! I think this is a blocker for 4.0 (cc. [~dajac]) so updated the Jira, and will follow-up on the open PR. Thanks!","created":"2025-01-10T14:20:13.953+0000","updated":"2025-01-10T14:20:13.953+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17912635","author":{"displayName":"Lianet Magrans"},"body":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb]","body_raw":"Merged to trunk and cherry-picked to 4.0 [https://github.com/apache/kafka/commit/5f46ff855475c23f8dfba9ec77df4de9dd184ebb] ","created":"2025-01-13T18:16:26.818+0000","updated":"2025-01-13T18:16:26.818+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"Write share consumer performance tests","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"Resolved","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"Major","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"We need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"Won't Fix","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Write share consumer performance tests\n\nWe need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.\n\nConversation:\nuser: We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","output":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We need to have share consumer performance tests in AK to avoid any degradation in performance due to regressions.","output":"Write share consumer performance tests","metadata":{"issue_id":"KAFKA-18468","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-10T07:36:12.000+0000","updated":"2025-09-19T16:39:26.000+0000","resolved":"2025-09-18T10:26:57.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"18021451","author":{"displayName":"Andrew Schofield"},"body":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","body_raw":"We have the share consumer perf tool. There is currently no plan to create additional performance tests as part of Apache Kafka.","created":"2025-09-19T16:39:26.550+0000","updated":"2025-09-19T16:39:26.550+0000","updateAuthor":{"displayName":"Andrew Schofield"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add create topic doc\n\nAdd a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Add create topic doc","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add create topic doc\n\nAdd a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Resolved","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add create topic doc\n\nAdd a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Minor","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add create topic doc\n\nAdd a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Add a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add create topic doc\n\nAdd a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Fixed","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Add a comment to let the user know that the first partition in the list will be considered the isr leader when creating a topic","output":"Add create topic doc","metadata":{"issue_id":"KAFKA-18467","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"xuanzhang gong"},"created":"2025-01-10T02:29:16.000+0000","updated":"2025-01-12T12:32:19.000+0000","resolved":"2025-01-12T12:32:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","output":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","output":"Resolved","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","output":"Blocker","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","output":"see discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]","output":"Fixed","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove log4j-1.2-api from runtime scope while keeping it in distribution package\n\nsee discussion: [https://github.com/apache/kafka/pull/17373#issuecomment-2577813317]\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","output":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","metadata":{"issue_id":"KAFKA-18466","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-10T02:19:14.000+0000","updated":"2025-01-13T09:33:21.000+0000","resolved":"2025-01-12T12:23:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17912259","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1 4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","body_raw":"trunk: https://github.com/apache/kafka/commit/b0b54f6db1ab775291c3bb4fbcaccb62794f4dc1\r\n\r\n4.0: https://github.com/apache/kafka/commit/69559950194eb630cb60b4e5d4b451bfb14a0d47","created":"2025-01-12T12:23:42.780+0000","updated":"2025-01-12T12:23:42.780+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove MetadataVersions older than 3.0-IV1\n\nApache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Remove MetadataVersions older than 3.0-IV1","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove MetadataVersions older than 3.0-IV1\n\nApache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Resolved","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove MetadataVersions older than 3.0-IV1\n\nApache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Major","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove MetadataVersions older than 3.0-IV1\n\nApache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Apache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baselin","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove MetadataVersions older than 3.0-IV1\n\nApache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Fixed","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Apache Kafka 4.0 will only support KRaft and 3.0-IV1 is the minimum version supported by KRaft. So, we can assume that Apache Kafka 4.0 will only communicate with brokers that are 3.0-IV1 or newer. Note that KRaft was only marked as production-ready in 3.3, so we could go further and set the baseline to 3.3. I think we should have that discussion, but it made sense to start with the non controversial parts.","output":"Remove MetadataVersions older than 3.0-IV1","metadata":{"issue_id":"KAFKA-18465","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2025-01-10T01:03:56.000+0000","updated":"2025-01-11T18:11:22.000+0000","resolved":"2025-01-11T18:10:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Empty Abort Transaction can fence producer incorrectly with Transactions V2\n\nConsider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch and the PrepareCommit state currently only allows for retries where the request epoch is -1 the current epoch. Instead of hitting this, we should just return the concurrent transactions error when we have pending state and do the epoch check second. In this scenario, we will complete commit before proceeding to the empty abort.","output":"Empty Abort Transaction can fence producer incorrectly with Transactions V2","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Empty Abort Transaction can fence producer incorrectly with Transactions V2\n\nConsider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch","output":"Resolved","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Empty Abort Transaction can fence producer incorrectly with Transactions V2\n\nConsider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch","output":"Blocker","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Empty Abort Transaction can fence producer incorrectly with Transactions V2\n\nConsider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch","output":"Consider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer abort","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Empty Abort Transaction can fence producer incorrectly with Transactions V2\n\nConsider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch","output":"Fixed","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Consider the following scenario using transactions V2. Producer writes a transaction and commits. Producer starts the next transaction and tries to add a partition but gets an non-retriable error. Because the application doesn't know whether the partition was added/records written the producer aborts. In the case where the partition was not added, and the previous transaction was still in PrepareCommit state, we self-fence. This is because the epoch of the current state == incoming request epoch and the PrepareCommit state currently only allows for retries where the request epoch is -1 the current epoch. Instead of hitting this, we should just return the concurrent transactions error when we have pending state and do the epoch check second. In this scenario, we will complete commit before ","output":"Empty Abort Transaction can fence producer incorrectly with Transactions V2","metadata":{"issue_id":"KAFKA-18464","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Justine Olshan"},"reporter":{"displayName":"Justine Olshan"},"created":"2025-01-09T23:50:24.000+0000","updated":"2025-01-11T00:54:25.000+0000","resolved":"2025-01-11T00:54:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms\n\nWith https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed in the KIP but the main motivation is messages with invalid timestamp interfere with log rotation and log cleaning, because these functions depends on comparing the current timestamp of the server with the message timestamp to make the decision weather a log files should be rotated or evicted. However there is backward compatibility issue. For example if the cluster is upgraded to a 3.6.0 and above versions from a version that did not have these config options and contains message with invalid timestamp like nanosecond instead of a millisecond with future timestamp like `{{{}2446236110000{}}}` -> 22 years in the future, both the log cleaner and log rotation logic will still be impaired even if we have `log.message.timestamp.after.max.ms` configured with the proposed default value of {{{}`86400000{}}}` for version 4.0 and above. Reference https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030","output":"Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms","metadata":{"issue_id":"KAFKA-18463","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Mehari Beyene"},"created":"2025-01-09T19:42:20.000+0000","updated":"2025-01-09T19:42:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms\n\nWith https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed in the KIP but the main motivation is messages with invalid timestamp interfere with log rotation and log cleaning, because these functions depends on comparing the current timestamp of the server with","output":"Open","metadata":{"issue_id":"KAFKA-18463","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Mehari Beyene"},"created":"2025-01-09T19:42:20.000+0000","updated":"2025-01-09T19:42:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms\n\nWith https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed in the KIP but the main motivation is messages with invalid timestamp interfere with log rotation and log cleaning, because these functions depends on comparing the current timestamp of the server with","output":"Minor","metadata":{"issue_id":"KAFKA-18463","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Mehari Beyene"},"created":"2025-01-09T19:42:20.000+0000","updated":"2025-01-09T19:42:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms\n\nWith https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed in the KIP but the main motivation is messages with invalid timestamp interfere with log rotation and log cleaning, because these functions depends on comparing the current timestamp of the server with","output":"With https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed i","metadata":{"issue_id":"KAFKA-18463","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Mehari Beyene"},"created":"2025-01-09T19:42:20.000+0000","updated":"2025-01-09T19:42:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"With https://cwiki.apache.org/confluence/display/KAFKA/KIP-937 we have introduced two new configs, `log.message.timestamp.before.max.ms` and `log.message.timestamp.after.max.ms` that are used to validate the timestamp of messages sent by producer. The rational for adding these configs is discussed in the KIP but the main motivation is messages with invalid timestamp interfere with log rotation and log cleaning, because these functions depends on comparing the current timestamp of the server with the message timestamp to make the decision weather a log files should be rotated or evicted. However there is backward compatibility issue. For example if the cluster is upgraded to a 3.6.0 and above versions from a version that did not have these config options and contains message with invalid ti","output":"Backward compatibility issue with the configs log.message.timestamp.after.max.ms and log.message.timestamp.before.max.ms","metadata":{"issue_id":"KAFKA-18463","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Mehari Beyene"},"created":"2025-01-09T19:42:20.000+0000","updated":"2025-01-09T19:42:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"Upgrade RocksDB dependency from 9.7.3 to something newer","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"Resolved","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"Blocker","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"We did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a f","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"Fixed","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Upgrade RocksDB dependency from 9.7.3 to something newer\n\nWe did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer Roc\n\nConversation:\nuser: FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine\nassistant: Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/com\nassistant: The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","output":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We did upgrade RocksDB to 9.7.3 with 4.0 release, but there is already newer RocksDB releases (not available yet for us to use with 4.0 release). We should re-evaluate if we can upgrade to a newer RocksDB version with 4.1. When we close this ticket, we should also create a new BLOCKER ticket for a future release as a reminder, so we ensure we upgrade RocksDB regularly.","output":"Upgrade RocksDB dependency from 9.7.3 to something newer","metadata":{"issue_id":"KAFKA-18462","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-09T19:20:59.000+0000","updated":"2025-06-16T07:33:18.000+0000","resolved":"2025-06-16T07:31:57.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17934662","author":{"displayName":"Steven Schlansker"},"body":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","body_raw":"FWIW, we run with rocksdbjni 9.8.4 and it seems to do just fine","created":"2025-03-12T21:48:08.678+0000","updated":"2025-03-12T21:48:08.678+0000","updateAuthor":{"displayName":"Steven Schlansker"}},{"id":"17953981","author":{"displayName":"Kuan Po Tseng"},"body":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed: * {{random_access_max_buffer_size}} [https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7] * {{max_write_buffer_size_to_maintain}} [https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5] We'll need to remove those options in https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java I’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","body_raw":"Quick note: In the latest RocksDB release (v10.2.1), the following two options have been removed:\r\n * {{random_access_max_buffer_size}}\r\n[https://github.com/facebook/rocksdb/commit/d4bd67fb0972bcf654f9d63bdabc39001a8208b7]\r\n\r\n * {{max_write_buffer_size_to_maintain}}\r\n[https://github.com/facebook/rocksdb/commit/5e10baa412d06492c7ec4a4ded0949f6981c94e5]\r\n\r\nWe'll need to remove those options in [RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java]\r\nI’ve run the Streams integration tests and everything looks good so far. Currently running the end-to-end tests—fingers crossed everything goes smoothly.","created":"2025-05-26T04:54:45.023+0000","updated":"2025-05-26T04:54:45.023+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17955693","author":{"displayName":"Kuan Po Tseng"},"body":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","body_raw":"The streams e2e test seems ok, see [https://github.com/apache/kafka/pull/19880#issuecomment-2930973011]","created":"2025-06-02T14:25:32.813+0000","updated":"2025-06-02T14:25:32.813+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE\n\nI saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snapshot.setDelta(Snapshot.java:50) at org.apache.kafka.timeline.SnapshottableHashTable.updateTierData(SnapshottableHashTable.java:378) at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableAddOrReplace(SnapshottableHashTable.java:354) at org.apache.kafka.timeline.TimelineHashMap.put(TimelineHashMap.java:157) at org.apache.kafka.controller.FeatureControlManager.replay(FeatureControlManager.java:404) at org.apache.kafka.controller.QuorumControllerTestEnv. (QuorumControllerTestEnv.java:125) at org.apache.kafka.controller.QuorumControllerTestEnv$Builder.build(QuorumControllerTestEnv.java:88) at org.apache.kafka.controller.QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount(QuorumControllerMetricsIntegrationTest.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596){noformat}","output":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE\n\nI saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snapshot.setDelta(Snapshot.java:50) at org.apache.kafka.timeline.SnapshottableHashTable.updateTierData(SnapshottableHashTable.java:378) at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableAdd","output":"Patch Available","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE\n\nI saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snapshot.setDelta(Snapshot.java:50) at org.apache.kafka.timeline.SnapshottableHashTable.updateTierData(SnapshottableHashTable.java:378) at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableAdd","output":"Major","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE\n\nI saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snapshot.setDelta(Snapshot.java:50) at org.apache.kafka.timeline.SnapshottableHashTable.updateTierData(SnapshottableHashTable.java:378) at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableAdd","output":"I saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snap","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE\n\nI saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.ut\n\nConversation:\nuser: Can i take this? done some kafka contributions some time ago... thanks..\nassistant: Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.\nassistant: PR link:https://github.com/apache/kafka/pull/18684","output":"PR link:https://github.com/apache/kafka/pull/18684","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I saw this flaky failure on the 4.0 branch: {noformat} testFailingOverIncrementsNewActiveControllerCount(boolean).forceFailoverUsingLogLayer=true java.lang.NullPointerException: Cannot invoke \"java.util.IdentityHashMap.put(Object, Object)\" because \"this.map\" is null at org.apache.kafka.timeline.Snapshot.setDelta(Snapshot.java:50) at org.apache.kafka.timeline.SnapshottableHashTable.updateTierData(SnapshottableHashTable.java:378) at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableAddOrReplace(SnapshottableHashTable.java:354) at org.apache.kafka.timeline.TimelineHashMap.put(TimelineHashMap.java:157) at org.apache.kafka.controller.FeatureControlManager.replay(FeatureControlManager.java:404) at org.apache.kafka.controller.QuorumControllerTestEnv. (QuorumControllerTestEnv.java:125)","output":"QuorumControllerMetricsIntegrationTest.testFailingOverIncrementsNewActiveControllerCount flaky failing with NPE","metadata":{"issue_id":"KAFKA-18461","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Patch Available","priority":"Major","assignee":{"displayName":"Ryan Ye"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:35:37.000+0000","updated":"2025-03-07T14:48:12.000+0000","resolved":null,"labels":["flaky-test","newbie"],"components":[],"comment_count":3,"comments":[{"id":"17912225","author":{"displayName":"Nicolae Marasoiu"},"body":"Can i take this? done some kafka contributions some time ago... thanks..","body_raw":"Can i take this? done some kafka contributions some time ago... thanks..","created":"2025-01-11T22:20:30.190+0000","updated":"2025-01-11T22:20:30.190+0000","updateAuthor":{"displayName":"Nicolae Marasoiu"}},{"id":"17916572","author":{"displayName":"Ryan Ye"},"body":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","body_raw":"Addressed an issue where calling {{setDelta}} on a {{Snapshot}} could result in a {{NullPointerException}} if the {{map}} was set to {{null}} during the {{erase}} operation. This fix ensures that {{setDelta}} handles the scenario appropriately to prevent unexpected behavior.","created":"2025-01-24T03:00:47.261+0000","updated":"2025-01-24T03:00:47.261+0000","updateAuthor":{"displayName":"Ryan Ye"}},{"id":"17916616","author":{"displayName":"Ryan Ye"},"body":"PR link:https://github.com/apache/kafka/pull/18684","body_raw":"PR link:https://github.com/apache/kafka/pull/18684","created":"2025-01-24T07:03:55.472+0000","updated":"2025-01-24T07:03:55.472+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees\n\nI use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a directory java.nio.file.FileSystemException: /path/to/kafka/.git/config: Not a directory at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100) ~[?:?] at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?] at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) ~[?:?] at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) ~[?:?] at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149) ~[?:?] at java.base/java.nio.file.Files.readAttributes(Files.java:1764) ~[?:?] at org.eclipse.jgit.util.FileUtils.fileAttributes(FileUtils.java:804) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.util.FS.fileAttributes(FS.java:1273) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.internal.storage.file.FileSnapshot. (FileSnapshot.java:234) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.internal.storage.file.FileSnapshot.saveNoConfig(FileSnapshot.java:120) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.storage.file.FileBasedConfig.lambda$0(FileBasedConfig.java:119) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.util.FileUtils.readWithRetries(FileUtils.java:722) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.storage.file.FileBasedConfig.load(FileBasedConfig.java:115) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.loadConfig(BaseRepositoryBuilder.java:732) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.getConfig(BaseRepositoryBuilder.java:709) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.guessWorkTreeOrFail(BaseRepositoryBuilder.java:744) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.setupWorkTree(BaseRepositoryBuilder.java:675) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.setup(BaseRepositoryBuilder.java:602) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.lib.BaseRepositoryBuilder.build(BaseRepositoryBuilder.java:625) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.api.Git.open(Git.java:93) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.eclipse.jgit.api.Git.open(Git.java:73) ~[org.eclipse.jgit-6.4.0.202211300538-r.jar:6.4.0.202211300538-r] at org.apache.kafka.message.checker.CheckerUtils.getDataFromGit(CheckerUtils.java:131) ~[main/:?] at org.apache.kafka.message.checker.MetadataSchemaCheckerTool.run(MetadataSchemaCheckerTool.java:107) ~[main/:?] at org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest.testVerifyEvolutionGit(MetadataSchemaCheckerToolTest.java:32) ~[test/:?] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:728) ~[junit-platform-commons-1.10.2.jar:1.10.2] at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:218) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:214) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:139) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:69) ~[junit-jupiter-engine-5.10.2.jar:5.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.10.2.jar:1.10.2] at java.base/java.util.ArrayList.forEach(ArrayList.java:1541) ~[?:?] at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.10.2.jar:1.10.2] at java.base/java.util.ArrayList.forEach(ArrayList.java:1541) ~[?:?] at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) ~[junit-platform-engine-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198) ~[junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169) ~[junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93) ~[junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58) ~[junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141) [junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57) [junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103) [junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85) [junit-platform-launcher-1.10.2.jar:1.10.2] at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47) [junit-platform-launcher-1.10.2.jar:1.10.2] at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124) [gradle-testing-junit-platform-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99) [gradle-testing-junit-platform-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94) [gradle-testing-junit-platform-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63) [gradle-testing-base-infrastructure-8.10.2.jar:8.10.2] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) [gradle-messaging-8.10.2.jar:8.10.2] at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) [gradle-messaging-8.10.2.jar:8.10.2] at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) [gradle-messaging-8.10.2.jar:8.10.2] at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92) [gradle-messaging-8.10.2.jar:8.10.2] at com.sun.proxy.$Proxy4.stop(Unknown Source) [?:?] at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200) [gradle-testing-base-infrastructure-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132) [gradle-testing-base-infrastructure-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103) [gradle-testing-base-infrastructure-8.10.2.jar:8.10.2] at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63) [gradle-testing-base-infrastructure-8.10.2.jar:8.10.2] at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) [gradle-worker-main-8.10.2.jar:8.10.2] at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:121) [gradle-worker-main-8.10.2.jar:8.10.2] at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71) [gradle-worker-main-8.10.2.jar:8.10.2] at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) [gradle-worker.jar:?] at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) [gradle-worker.jar:?] {noformat} The reason for this appears to be that the version of JGit we're using does not support worktrees: [https://github.com/eclipse-jgit/jgit/issues/76]","output":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees\n\nI use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a directory java.nio.file.FileSystemException: /path/to/kafka/.git/config: Not a directory at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100) ~[?:?] at java.base/sun.nio.fs","output":"Open","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees\n\nI use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a directory java.nio.file.FileSystemException: /path/to/kafka/.git/config: Not a directory at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100) ~[?:?] at java.base/sun.nio.fs","output":"Minor","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees\n\nI use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a directory java.nio.file.FileSystemException: /path/to/kafka/.git/config: Not a directory at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100) ~[?:?] at java.base/sun.nio.fs","output":"I use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a dire","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees\n\nI use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker\n\nConversation:\nuser: Hi gharris1727 I would like to investigate this issue.","output":"Hi gharris1727 I would like to investigate this issue.","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I use Worktrees [https://git-scm.com/docs/git-worktree] with Kafka, and MetadataSchemaCheckerToolTest.testVerifyEvolutionGit is failing when run within a worktree. {noformat} 10:25:20.609 [Test worker] ERROR org.eclipse.jgit.internal.storage.file.FileSnapshot - /path/to/kafka/.git/config: Not a directory java.nio.file.FileSystemException: /path/to/kafka/.git/config: Not a directory at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100) ~[?:?] at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?] at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) ~[?:?] at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) ~[?:?] at java.base/sun.nio.fs.UnixFileSyst","output":"MetadataSchemaCheckerTool verify-evolution-git is broken in Git worktrees","metadata":{"issue_id":"KAFKA-18460","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Greg Harris"},"created":"2025-01-09T18:29:32.000+0000","updated":"2025-01-09T23:42:10.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911706","author":{"displayName":"TengYao Chi"},"body":"Hi gharris1727 I would like to investigate this issue.","body_raw":"Hi [~gharris1727] \r\nI would like to investigate this issue.","created":"2025-01-09T23:42:10.482+0000","updated":"2025-01-09T23:42:10.482+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Support multiple JDKs in ducktape\n\nEventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this is not the library the older Kafka release was actually released with, so this setup may introduce subtle bugs (or maybe not, if we didn't use anything that was removed in the newer ZK versions?) Eventually Oracle will probably make some breaking change in the JDK that will require us to use historic JDKs for historic Kafka releases. For now, we're hacking it to all use one new JDK.","output":"Support multiple JDKs in ducktape","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Support multiple JDKs in ducktape\n\nEventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this is not the library the older Kafka release was actually released with, so this setup may introduce subtle bugs (or maybe not, if we didn't use anything that was removed in the newer ZK versions?) Event","output":"Open","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Support multiple JDKs in ducktape\n\nEventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this is not the library the older Kafka release was actually released with, so this setup may introduce subtle bugs (or maybe not, if we didn't use anything that was removed in the newer ZK versions?) Event","output":"Major","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Support multiple JDKs in ducktape\n\nEventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this is not the library the older Kafka release was actually released with, so this setup may introduce subtle bugs (or maybe not, if we didn't use anything that was removed in the newer ZK versions?) Event","output":"Eventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this i","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Support multiple JDKs in ducktape\n\nEventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they ship\n\nConversation:\nuser: cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you ple\nassistant: Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), \nassistant: chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will ne\nassistant: cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable ","output":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable ","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Eventually we will need to support multiple JDKs in ducktape. One example of why we might want to do this is that we currently hack the older releases to use a different ZK library than what they shipped with. We do this because ZK 3.4 and earlier are not compatible with JDK 17 and newer. But this is not the library the older Kafka release was actually released with, so this setup may introduce subtle bugs (or maybe not, if we didn't use anything that was removed in the newer ZK versions?) Eventually Oracle will probably make some breaking change in the JDK that will require us to use historic JDKs for historic Kafka releases. For now, we're hacking it to all use one new JDK.","output":"Support multiple JDKs in ducktape","metadata":{"issue_id":"KAFKA-18459","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Colin McCabe"},"created":"2025-01-09T18:23:27.000+0000","updated":"2025-02-21T15:45:40.000+0000","resolved":null,"labels":[],"components":[],"comment_count":4,"comments":[{"id":"17911615","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for opening this jira. {quote} This version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. {quote} To enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","body_raw":"[~cmccabe] thanks for opening this jira. \r\n\r\n{quote}\r\nThis version of AK is ZK-only (no kraft support) and does not support newer versions of ZK. \r\n{quote}\r\n\r\nTo enable Broker 2.1 to run under JDK 17, we replaced ZooKeeper 3.4 with version 3.5 (https://issues.apache.org/jira/browse/ZOOKEEPER-3779). Could you please share the error you encountered? I'd be happy to take a look.","created":"2025-01-09T18:43:24.545+0000","updated":"2025-01-09T18:43:24.545+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911631","author":{"displayName":"Chia-Ping Tsai"},"body":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. zk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server","body_raw":"Please correct me if I misunderstand anything, but I run the `client_compatibility_features_test.py` for broker LATEST_2_1 and it works well. \r\n\r\nYou have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. \r\n\r\nzk 3.5.x client are fully compatible with 3.9.x servers (https://zookeeper.apache.org/releases.html), and in e2e ak 2.1 ~ 2.3 are using zk 3.5.7 client (we replace 3.4.x manually https://github.com/apache/kafka/blob/trunk/tests/docker/Dockerfile#L136). Hence, in e2e ak 2.1 ~ 2.3 should work with newer versions of ZK-server\r\n\r\n\r\n","created":"2025-01-09T19:30:46.347+0000","updated":"2025-01-09T19:30:46.347+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923539","author":{"displayName":"Colin McCabe"},"body":"chia7712 thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release. I do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine. bq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point. Looking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility. I will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","body_raw":"[~chia7712] thanks for commenting. I wasn't aware that we were replacing the ZK 3.4 jars with ZK 3.5 ones by using rm and cp. That's a bit risky, as I'm sure you know. However, if it is working for now, then maybe we don't have to change anything for the AK 4.0 release.\r\n\r\nI do think eventually we will need to run the older releases with their \"historic JDKs\" but if we can put off doing that for now, that's fine.\r\n\r\nbq. > You have mentioned that \"ZK 3.5.7 is not compatible with Kafka 2.1 Please read KIP-902\" (https://github.com/apache/kafka/pull/17576#issuecomment-2452655129), but I failed to get your point.\r\n\r\nLooking at the chart again, I think I mis-spoke. ZK 3.5 is compatible with Kafka 2.1 although it didn't ship with that release. It's ZK 3.8 that has the incompatibility.\r\n\r\nI will mark this as not a blocker if the tests are passing with this setup. Thanks for the corrections.","created":"2025-02-04T01:35:08.053+0000","updated":"2025-02-04T01:35:35.870+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17923545","author":{"displayName":"Chia-Ping Tsai"},"body":"cmccabe thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","body_raw":"[~cmccabe] thanks for your feedback. While supporting multiple JDKs would be the ideal approach as you suggested, I initially avoided it due to the complexity involved. However, I agree that the current approach of replacing zk jars is a hack. I will take over this Jira and investigate a more suitable way to deliver this change to trunk/4.0.","created":"2025-02-04T02:35:43.098+0000","updated":"2025-02-04T02:35:43.098+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Kafka Docker Image Documentation\n\nI wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of the docker container itself. I was able to resolve the issue by changing the KAFKA_LISTENERS property: Value that did NOT work for me: KAFKA_LISTENERS: PLAINTEXT://localhost:9092,CONTROLLER://localhost:9093 Value that worked for me: KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 Once I made that change, I was able to communicate with the broker from outside the container. I figured out the change I needed to make based on the sample “docker run” command that appears on the same page just above the docker-compose.yml approach.","output":"Kafka Docker Image Documentation","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Kafka Docker Image Documentation\n\nI wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of the docker container itself. I was able to resolve the issue by changing the KAFKA_LISTENERS property: Value that did NOT work for me: KAFKA_LISTENERS: PLAINTEXT://localhost:9092,CONTROLLER://localhos","output":"Open","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Kafka Docker Image Documentation\n\nI wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of the docker container itself. I was able to resolve the issue by changing the KAFKA_LISTENERS property: Value that did NOT work for me: KAFKA_LISTENERS: PLAINTEXT://localhost:9092,CONTROLLER://localhos","output":"Minor","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Kafka Docker Image Documentation\n\nI wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of the docker container itself. I was able to resolve the issue by changing the KAFKA_LISTENERS property: Value that did NOT work for me: KAFKA_LISTENERS: PLAINTEXT://localhost:9092,CONTROLLER://localhos","output":"I wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of ","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Kafka Docker Image Documentation\n\nI wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basical\n\nConversation:\nuser: Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","output":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I wanted to let you know that when I used the default docker-compose.yml configuration found on the Apache Kafka docker image site (https://hub.docker.com/r/apache/kafka), I ran into an issue. Basically, I was not able to call the broker using the CLI tools for creating topics, etc. from outside of the docker container itself. I was able to resolve the issue by changing the KAFKA_LISTENERS property: Value that did NOT work for me: KAFKA_LISTENERS: PLAINTEXT://localhost:9092,CONTROLLER://localhost:9093 Value that worked for me: KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 Once I made that change, I was able to communicate with the broker from outside the container. I figured out the change I needed to make based on the sample “docker run” command that appears on the same page just a","output":"Kafka Docker Image Documentation","metadata":{"issue_id":"KAFKA-18458","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Devin Michael"},"created":"2025-01-09T14:05:40.000+0000","updated":"2025-01-18T23:31:22.000+0000","resolved":null,"labels":[],"components":["documentation"],"comment_count":1,"comments":[{"id":"17914402","author":{"displayName":"Jeremy L"},"body":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","body_raw":"Anyone has bandwidth to work on PR for this? Please let me know whether it's OK to reassign to me, thanks","created":"2025-01-18T23:31:22.096+0000","updated":"2025-01-18T23:31:22.096+0000","updateAuthor":{"displayName":"Jeremy L"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout\n\norg.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextConsumerPollTest.testMaxPollIntervalMsShorterThanPollTimeout(PlaintextConsumerPollTest.scala:183) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:992) at java.base@17.0.13/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) at java.base@17.0.13/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:992) at java.base@17.0.13/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base@17.0.13/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) at java.base@17.0.13/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base@17.0.13/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base@17.0.13/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base@17.0.13/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) at java.base@17.0.13/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base@17.0.13/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) at java.base@17.0.13/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base@17.0.13/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base@17.0.13/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base@17.0.13/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) at java.base@17.0.13/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base@17.0.13/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) at java.base@17.0.13/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) at java.base@17.0.13/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base@17.0.13/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511)","output":"Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout","metadata":{"issue_id":"KAFKA-18456","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-09T02:22:54.000+0000","updated":"2025-01-09T02:22:54.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout\n\norg.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org","output":"Open","metadata":{"issue_id":"KAFKA-18456","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-09T02:22:54.000+0000","updated":"2025-01-09T02:22:54.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout\n\norg.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org","output":"Major","metadata":{"issue_id":"KAFKA-18456","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-09T02:22:54.000+0000","updated":"2025-01-09T02:22:54.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout\n\norg.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org","output":"org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failN","metadata":{"issue_id":"KAFKA-18456","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-09T02:22:54.000+0000","updated":"2025-01-09T02:22:54.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextConsumerPollTest.testMaxPollIntervalMsShorterThanPollTimeout(PlaintextConsumerPollTest.scala:183) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.stream.For","output":"Fix flaky PlaintextConsumerPollTest#testMaxPollIntervalMsShorterThanPollTimeout","metadata":{"issue_id":"KAFKA-18456","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-09T02:22:54.000+0000","updated":"2025-01-09T02:22:54.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add logs or metrics when connection count exceeds\n\nCurrently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https://github.com/apache/kafka/pull/12559. FYR","output":"Add logs or metrics when connection count exceeds","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add logs or metrics when connection count exceeds\n\nCurrently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https://github.com/apache/kafka/pull/12559. FYR","output":"Open","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add logs or metrics when connection count exceeds\n\nCurrently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https://github.com/apache/kafka/pull/12559. FYR","output":"Major","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add logs or metrics when connection count exceeds\n\nCurrently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https://github.com/apache/kafka/pull/12559. FYR","output":"Currently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https:/","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add logs or metrics when connection count exceeds\n\nCurrently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client c\n\nConversation:\nuser: Hi showuon May I take over this issue ?\nassistant: Assigned to you frankvicky ! Thanks.","output":"Assigned to you frankvicky ! Thanks.","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Currently, when client tried to connect to broker, and got throttled or exceeding connection count (waiting for available connection slot), there will be no logs or metrics in broker. All the client can see is connection timeout exception. We should improve it. There was a discussion in this https://github.com/apache/kafka/pull/12559. FYR","output":"Add logs or metrics when connection count exceeds","metadata":{"issue_id":"KAFKA-18455","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Luke Chen"},"created":"2025-01-09T02:21:26.000+0000","updated":"2025-01-28T07:34:03.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911297","author":{"displayName":"TengYao Chi"},"body":"Hi showuon May I take over this issue ?","body_raw":"Hi [~showuon] \r\nMay I take over this issue ?","created":"2025-01-09T02:24:11.672+0000","updated":"2025-01-09T02:24:11.672+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17911298","author":{"displayName":"Luke Chen"},"body":"Assigned to you frankvicky ! Thanks.","body_raw":"Assigned to you [~frankvicky] ! Thanks.","created":"2025-01-09T02:24:49.899+0000","updated":"2025-01-09T02:24:49.899+0000","updateAuthor":{"displayName":"Luke Chen"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Migrate Build Scan publication to develocity.apache.org\n\nThe Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will need to be updated. I will submit a PR at that time with the URL change.","output":"Migrate Build Scan publication to develocity.apache.org","metadata":{"issue_id":"KAFKA-18454","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Clay Johnson"},"created":"2025-01-08T17:22:06.000+0000","updated":"2025-01-25T01:37:26.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Migrate Build Scan publication to develocity.apache.org\n\nThe Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will need to be updated. I will submit a PR at that time with the URL change.","output":"Open","metadata":{"issue_id":"KAFKA-18454","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Clay Johnson"},"created":"2025-01-08T17:22:06.000+0000","updated":"2025-01-25T01:37:26.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Migrate Build Scan publication to develocity.apache.org\n\nThe Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will need to be updated. I will submit a PR at that time with the URL change.","output":"Minor","metadata":{"issue_id":"KAFKA-18454","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Clay Johnson"},"created":"2025-01-08T17:22:06.000+0000","updated":"2025-01-25T01:37:26.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Migrate Build Scan publication to develocity.apache.org\n\nThe Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will need to be updated. I will submit a PR at that time with the URL change.","output":"The Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will nee","metadata":{"issue_id":"KAFKA-18454","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Clay Johnson"},"created":"2025-01-08T17:22:06.000+0000","updated":"2025-01-25T01:37:26.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The Kafka project publishes Build Scans to the Develocity instance at ge.apache.org. This instance migrating to a new URL, develocity.apache.org, on Jan 14th, 2024. Build scans published to ge.apache.org after this date will not be migrated to the new Develocity instance. The configured URL will need to be updated. I will submit a PR at that time with the URL change.","output":"Migrate Build Scan publication to develocity.apache.org","metadata":{"issue_id":"KAFKA-18454","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Clay Johnson"},"created":"2025-01-08T17:22:06.000+0000","updated":"2025-01-25T01:37:26.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Integrate share fetch batch size in share fetch\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18452","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-08T15:52:14.000+0000","updated":"2025-01-15T19:08:54.000+0000","resolved":"2025-01-15T19:08:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913424","author":{"displayName":"Jun Rao"},"body":"merged the PR to trunk","body_raw":"merged the PR to trunk","created":"2025-01-15T19:08:54.121+0000","updated":"2025-01-15T19:08:54.121+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Integrate share fetch batch size in share fetch\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18452","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-08T15:52:14.000+0000","updated":"2025-01-15T19:08:54.000+0000","resolved":"2025-01-15T19:08:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913424","author":{"displayName":"Jun Rao"},"body":"merged the PR to trunk","body_raw":"merged the PR to trunk","created":"2025-01-15T19:08:54.121+0000","updated":"2025-01-15T19:08:54.121+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Integrate share fetch batch size in share fetch\n\n","output":"","metadata":{"issue_id":"KAFKA-18452","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-08T15:52:14.000+0000","updated":"2025-01-15T19:08:54.000+0000","resolved":"2025-01-15T19:08:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913424","author":{"displayName":"Jun Rao"},"body":"merged the PR to trunk","body_raw":"merged the PR to trunk","created":"2025-01-15T19:08:54.121+0000","updated":"2025-01-15T19:08:54.121+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Integrate share fetch batch size in share fetch\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18452","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-08T15:52:14.000+0000","updated":"2025-01-15T19:08:54.000+0000","resolved":"2025-01-15T19:08:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913424","author":{"displayName":"Jun Rao"},"body":"merged the PR to trunk","body_raw":"merged the PR to trunk","created":"2025-01-15T19:08:54.121+0000","updated":"2025-01-15T19:08:54.121+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Integrate share fetch batch size in share fetch\n\n\n\nConversation:\nuser: merged the PR to trunk","output":"merged the PR to trunk","metadata":{"issue_id":"KAFKA-18452","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2025-01-08T15:52:14.000+0000","updated":"2025-01-15T19:08:54.000+0000","resolved":"2025-01-15T19:08:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913424","author":{"displayName":"Jun Rao"},"body":"merged the PR to trunk","body_raw":"merged the PR to trunk","created":"2025-01-15T19:08:54.121+0000","updated":"2025-01-15T19:08:54.121+0000","updateAuthor":{"displayName":"Jun Rao"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady\n\norg.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.java:413) 2. -> at kafka.log.remote.RemoteLogManager.onLeadershipChange(RemoteLogManager.java:478) 3. -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) 4. -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) 5. [?]-> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815)","output":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady\n\norg.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.ja","output":"Resolved","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady\n\norg.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.ja","output":"Major","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady\n\norg.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.ja","output":"org.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$R","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady\n\norg.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.ja","output":"Fixed","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"org.mockito.exceptions.verification.NoInteractionsWanted: No interactions wanted here: -> at kafka.log.remote.RemoteLogManagerTest.testRLMOpsWhenMetadataIsNotReady(RemoteLogManagerTest.java:3706) But found this interaction on mock 'remoteLogMetadataManager': -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) *** For your reference, here is the list of all invocations ([?] - means unverified). 1. -> at kafka.log.remote.RemoteLogManager.configureRLMM(RemoteLogManager.java:413) 2. -> at kafka.log.remote.RemoteLogManager.onLeadershipChange(RemoteLogManager.java:478) 3. -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) 4. -> at kafka.log.remote.RemoteLogManager$RLMTask.run(RemoteLogManager.java:815) 5. [?]-> at kafka.log.remote.RemoteLogM","output":"Flaky RemoteLogManagerTest#testRLMOpsWhenMetadataIsNotReady","metadata":{"issue_id":"KAFKA-18451","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T12:50:59.000+0000","updated":"2025-01-15T08:19:09.000+0000","resolved":"2025-01-15T08:19:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"display which role can use the config\n\nAfter migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now exclusive to the controller, determining the exact scope for each configuration remains a challenge to users. While users can technically add all configurations to both controller and broker properties, our documentation should clearly indicate the intended scope of each configuration. This will not only enhance user understanding but also guide them in correctly configuring the broker and controller.","output":"display which role can use the config","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"display which role can use the config\n\nAfter migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now exclusive to the controller, determining the exact scope for each configuration remains a challenge to users. While users can technically add all configurations to both controller and broker properties, o","output":"Open","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"display which role can use the config\n\nAfter migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now exclusive to the controller, determining the exact scope for each configuration remains a challenge to users. While users can technically add all configurations to both controller and broker properties, o","output":"Major","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: display which role can use the config\n\nAfter migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now exclusive to the controller, determining the exact scope for each configuration remains a challenge to users. While users can technically add all configurations to both controller and broker properties, o","output":"After migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now excl","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"display which role can use the config\n\nAfter migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.exp\n\nConversation:\nuser: 1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP\nassistant: I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","output":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"After migrating zk to kraft, we identified several configurations that are no longer used by brokers. These include create.topic.policy.class.name, alter.config.policy.class.name, delegation.token.expiry.check.interval.ms, delegation.token.expiry.time.ms, etc. While these configurations are now exclusive to the controller, determining the exact scope for each configuration remains a challenge to users. While users can technically add all configurations to both controller and broker properties, our documentation should clearly indicate the intended scope of each configuration. This will not only enhance user understanding but also guide them in correctly configuring the broker and controller.","output":"display which role can use the config","metadata":{"issue_id":"KAFKA-18450","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T11:55:42.000+0000","updated":"2025-01-08T12:15:13.000+0000","resolved":null,"labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911036","author":{"displayName":"Chia-Ping Tsai"},"body":"1) We can enhance the documentation for these configurations 2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","body_raw":"1) We can enhance the documentation for these configurations\r\n2) Or, we can extend the ConfigKey class to include a \"role\" attribute (broker or controller). this needs KIP","created":"2025-01-08T12:01:26.454+0000","updated":"2025-01-08T12:01:26.454+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911044","author":{"displayName":"TengYao Chi"},"body":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component. Furthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","body_raw":"I think the second approach is better since it's more straightforward for users to find out which properties are for which component.\r\n\r\nFurthermore, we could also leverage throwing exceptions or other programmatical ways to inform users what's wrong.","created":"2025-01-08T12:15:13.327+0000","updated":"2025-01-08T12:15:13.327+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add share-group state configs to reconfigure-server.properties\n\n`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"Add share-group state configs to reconfigure-server.properties","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add share-group state configs to reconfigure-server.properties\n\n`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"Resolved","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add share-group state configs to reconfigure-server.properties\n\n`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"Major","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add share-group state configs to reconfigure-server.properties\n\n`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add share-group state configs to reconfigure-server.properties\n\n`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"Fixed","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"`reconfigure-server.properties` needs the share-group state configs which were added to `server.properties`.","output":"Add share-group state configs to reconfigure-server.properties","metadata":{"issue_id":"KAFKA-18449","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-08T10:23:57.000+0000","updated":"2025-01-09T13:48:08.000+0000","resolved":"2025-01-09T13:48:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove MetadataCacheControllerNodeProvider\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18446","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T10:00:33.000+0000","updated":"2025-01-11T18:16:53.000+0000","resolved":"2025-01-11T18:16:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911005","author":{"displayName":"xuanzhang gong"},"body":"Hi, I will handle this issue,plz assign to me","body_raw":"Hi, I will handle this issue,plz assign to me ","created":"2025-01-08T10:02:27.685+0000","updated":"2025-01-08T10:02:27.685+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17912214","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","body_raw":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08\r\n\r\n4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","created":"2025-01-11T18:16:53.063+0000","updated":"2025-01-11T18:16:53.063+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove MetadataCacheControllerNodeProvider\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18446","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T10:00:33.000+0000","updated":"2025-01-11T18:16:53.000+0000","resolved":"2025-01-11T18:16:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911005","author":{"displayName":"xuanzhang gong"},"body":"Hi, I will handle this issue,plz assign to me","body_raw":"Hi, I will handle this issue,plz assign to me ","created":"2025-01-08T10:02:27.685+0000","updated":"2025-01-08T10:02:27.685+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17912214","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","body_raw":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08\r\n\r\n4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","created":"2025-01-11T18:16:53.063+0000","updated":"2025-01-11T18:16:53.063+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove MetadataCacheControllerNodeProvider\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18446","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T10:00:33.000+0000","updated":"2025-01-11T18:16:53.000+0000","resolved":"2025-01-11T18:16:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911005","author":{"displayName":"xuanzhang gong"},"body":"Hi, I will handle this issue,plz assign to me","body_raw":"Hi, I will handle this issue,plz assign to me ","created":"2025-01-08T10:02:27.685+0000","updated":"2025-01-08T10:02:27.685+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17912214","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","body_raw":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08\r\n\r\n4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","created":"2025-01-11T18:16:53.063+0000","updated":"2025-01-11T18:16:53.063+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove MetadataCacheControllerNodeProvider\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18446","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T10:00:33.000+0000","updated":"2025-01-11T18:16:53.000+0000","resolved":"2025-01-11T18:16:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911005","author":{"displayName":"xuanzhang gong"},"body":"Hi, I will handle this issue,plz assign to me","body_raw":"Hi, I will handle this issue,plz assign to me ","created":"2025-01-08T10:02:27.685+0000","updated":"2025-01-08T10:02:27.685+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17912214","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","body_raw":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08\r\n\r\n4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","created":"2025-01-11T18:16:53.063+0000","updated":"2025-01-11T18:16:53.063+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove MetadataCacheControllerNodeProvider\n\nas title\n\nConversation:\nuser: Hi, I will handle this issue,plz assign to me\nassistant: trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","output":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","metadata":{"issue_id":"KAFKA-18446","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T10:00:33.000+0000","updated":"2025-01-11T18:16:53.000+0000","resolved":"2025-01-11T18:16:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911005","author":{"displayName":"xuanzhang gong"},"body":"Hi, I will handle this issue,plz assign to me","body_raw":"Hi, I will handle this issue,plz assign to me ","created":"2025-01-08T10:02:27.685+0000","updated":"2025-01-08T10:02:27.685+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17912214","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08 4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","body_raw":"trunk: https://github.com/apache/kafka/commit/1ee715473cbbed2d46bea2ef720630e6d7471d08\r\n\r\n4.0: https://github.com/apache/kafka/commit/946c14cf5a2c35e33f8f0ca99ff008640eb8a20c","created":"2025-01-11T18:16:53.063+0000","updated":"2025-01-11T18:16:53.063+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"Resolved","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"Major","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"the message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"Fixed","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend\n\nthe message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.\n\nConversation:\nuser: chia7712 If you have not started working on this, cloud you assign it to me ?\nassistant: trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","output":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"the message conversion has been removed by KAFKA-18269, so we don't need both LazyDownConversionRecords and LazyDownConversionRecordsSend now.","output":"Remove LazyDownConversionRecords and LazyDownConversionRecordsSend","metadata":{"issue_id":"KAFKA-18445","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-08T09:45:27.000+0000","updated":"2025-01-09T16:26:11.000+0000","resolved":"2025-01-09T16:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910999","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, cloud you assign it to me ?","body_raw":"[~chia7712]   If you have not started working on this, cloud you assign it to me ?","created":"2025-01-08T09:47:32.825+0000","updated":"2025-01-08T09:47:32.825+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17911578","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896 4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","body_raw":"trunk: https://github.com/apache/kafka/commit/fcd98da9aee8f59fe563712f84a80e6454ba0896\r\n4.0: https://github.com/apache/kafka/commit/c469e8b159dca96a8df531380ecf05f083ab34d4","created":"2025-01-09T16:26:11.414+0000","updated":"2025-01-09T16:26:11.414+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Run: https://github.com/apache/kafka/actions/runs/12415446571 test-catalog/clients/tests.yaml | 1 + test-catalog/generator/tests.yaml | 116 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ test-catalog/streams/tests.yaml | 1 + test-catalog/test-common/test-common-runtime/tests.yaml | 12 +++++++++++ 4 files changed, 130 insertions(+) commit 83cd514fb816c3aa5c35ee471696afafbb5e13e8 Author: github-actions[bot] Date: Thu Dec 19 17:32:38 2024 +0000 Update test catalog data for GHA workflow run 12415578652 Commit: https://github.com/apache/kafka/commit/35bb79953109f541e0733d5dac71bd55d8dc9210 GitHub Run: https://github.com/apache/kafka/actions/runs/12415578652 test-catalog/generator/tests.yaml | 116 test-catalog/streams/tests.yaml | 1 - test-catalog/test-common/test-common-runtime/tests.yaml | 12 ----------- 3 files changed, 129 deletions(-) {code} we see in workflow run 12415578652 that several tests are removed from the catalog, but re-added in the next workflow.","output":"Test catalog not always capturing everything","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Ru","output":"Resolved","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Ru","output":"Critical","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Ru","output":"Looking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actio","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Ru","output":"Fixed","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Test catalog not always capturing everything\n\nLooking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent exa\n\nConversation:\nuser: Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the w\nassistant: Looks like the 4.0 branch and trunk are colliding with each other.\nassistant: Fixed by disallowing 4.0 branch from updating the test-catalog branch","output":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Looking at the history of the test-catalog branch, it seems that something is causing partial results occasionally. This can cause tests to be incorrectly selected as quarantined. Here is a recent example {code:java} commit 2ab969a3d85d613a988cbfce1d56ea0920115639 (test-catalog) Author: github-actions[bot] Date: Thu Dec 19 18:53:08 2024 +0000 Update test catalog data for GHA workflow run 12415446571 Commit: https://github.com/apache/kafka/commit/b31aa651156fbc961fbb8460604393fef1c09185 GitHub Run: https://github.com/apache/kafka/actions/runs/12415446571 test-catalog/clients/tests.yaml | 1 + test-catalog/generator/tests.yaml | 116 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ test-catalog/streams/tests.yaml | 1 + test-catalog/test-com","output":"Test catalog not always capturing everything","metadata":{"issue_id":"KAFKA-18444","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"David Arthur"},"reporter":{"displayName":"David Arthur"},"created":"2025-01-08T02:02:53.000+0000","updated":"2025-01-14T00:19:50.000+0000","resolved":"2025-01-14T00:19:50.000+0000","resolution":"Fixed","labels":[],"components":["build"],"comment_count":3,"comments":[{"id":"17910884","author":{"displayName":"David Arthur"},"body":"Looking at a single removed test: {code:java} -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest: -- testSuccessfulParse -- testSuccessfulVerifyEvolution -- testVerifyEvolutionGit{code} org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution In the workflow logs, we indeed do not see that test being run","body_raw":"Looking at a single removed test:\r\n{code:java}\r\n -org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest:\r\n-- testSuccessfulParse\r\n-- testSuccessfulVerifyEvolution\r\n-- testVerifyEvolutionGit{code}\r\norg.apache.kafka.message.checker.MetadataSchemaCheckerToolTest#testSuccessfulVerifyEvolution\r\n\r\n \r\n\r\nIn the workflow logs, we indeed do not see that test being run","created":"2025-01-08T02:09:03.136+0000","updated":"2025-01-08T02:09:03.136+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17910888","author":{"displayName":"David Arthur"},"body":"Looks like the 4.0 branch and trunk are colliding with each other.","body_raw":"Looks like the 4.0 branch and trunk are colliding with each other.","created":"2025-01-08T02:13:34.406+0000","updated":"2025-01-08T02:13:34.406+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17912679","author":{"displayName":"David Arthur"},"body":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","body_raw":"Fixed by disallowing 4.0 branch from updating the test-catalog branch","created":"2025-01-14T00:19:50.969+0000","updated":"2025-01-14T00:19:50.969+0000","updateAuthor":{"displayName":"David Arthur"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkFourLetterWords\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18443","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T23:13:56.000+0000","updated":"2025-01-08T09:28:02.000+0000","resolved":"2025-01-08T09:28:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910991","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","body_raw":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca\r\n\r\n4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","created":"2025-01-08T09:28:02.379+0000","updated":"2025-01-08T09:28:02.379+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkFourLetterWords\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18443","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T23:13:56.000+0000","updated":"2025-01-08T09:28:02.000+0000","resolved":"2025-01-08T09:28:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910991","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","body_raw":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca\r\n\r\n4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","created":"2025-01-08T09:28:02.379+0000","updated":"2025-01-08T09:28:02.379+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkFourLetterWords\n\n","output":"","metadata":{"issue_id":"KAFKA-18443","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T23:13:56.000+0000","updated":"2025-01-08T09:28:02.000+0000","resolved":"2025-01-08T09:28:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910991","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","body_raw":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca\r\n\r\n4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","created":"2025-01-08T09:28:02.379+0000","updated":"2025-01-08T09:28:02.379+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkFourLetterWords\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18443","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T23:13:56.000+0000","updated":"2025-01-08T09:28:02.000+0000","resolved":"2025-01-08T09:28:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910991","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","body_raw":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca\r\n\r\n4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","created":"2025-01-08T09:28:02.379+0000","updated":"2025-01-08T09:28:02.379+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkFourLetterWords\n\n\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","output":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","metadata":{"issue_id":"KAFKA-18443","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T23:13:56.000+0000","updated":"2025-01-08T09:28:02.000+0000","resolved":"2025-01-08T09:28:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910991","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca 4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","body_raw":"trunk: https://github.com/apache/kafka/commit/8568157f7f022b5f2fceb9dc069b8d10b5bd05ca\r\n\r\n4.0: https://github.com/apache/kafka/commit/0cd95bc2d82106ea9fb36061d8419f7fc9051d69","created":"2025-01-08T09:28:02.379+0000","updated":"2025-01-08T09:28:02.379+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala#L186. In 3.3.2 kafka loads information about metadata directories via {{metadata.properties}} files and expects that the properties are duplicated for all log directories. We crash with a fatal error if they are non-duplicate which at that time would mean that another instance of kafka was using the same log directories. In https://github.com/apache/kafka/pull/14291 (added in Kafka version 3.6.x), the requirement that the {{metadata.properties}} files is duplicate in each directory was dropped. Each `metadata.properties` file will now contain a non-unique {{directory.id}} field for each dir. This has no effect on versions of kafka running {{kraft}} mode, >= 3.4.x (they care only about uniqueness of node.id and cluster.id) but does affect 3.3.2 and until 2.8.x (the change was added in https://github.com/apache/kafka/pull/9967) since it expects every {{metadata.properties}} file to be the same.","output":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheck","output":"Resolved","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheck","output":"Minor","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheck","output":"Downgrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied fr","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheck","output":"Won't Fix","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled\n\nDowngrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could\n\nConversation:\nuser: So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Clos","output":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Clos","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Downgrading from 3.6.x and above to 3.3.x will fail with the following error: {code:java} kafka.common.InconsistentBrokerMetadataException: BrokerMetadata is not consistent across log.dirs. This could happen if multiple brokers shared a log directory (log.dirs) or partial data was manually copied from another broker. ...{code} This is broken in 3.3.2 version of Kafka https://github.com/apache/kafka/blob/c9c03dd7ef9ff4edf2596e905cabececc72a9e9d/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala#L186. In 3.3.2 kafka loads information about metadata directories via {{metadata.properties}} files and expects that the properties are duplicated for all log directories. We crash with a fatal error if they are non-duplicate which at that time would mean that another instance of kafka w","output":"Downgrades to 3.3.2 will fail from versions 3.6.0 and above when JBOD is enabled","metadata":{"issue_id":"KAFKA-18442","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Jonah Hooper"},"created":"2025-01-07T21:50:02.000+0000","updated":"2025-01-29T21:39:32.000+0000","resolved":"2025-01-29T21:39:32.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922224","author":{"displayName":"Colin McCabe"},"body":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported. However, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), This PR has fixed the ducktape test failure. https://github.com/apache/kafka/pull/18386 Closing as WONTFIX","body_raw":"So firstly, this is a real bug, which we cannot fix since 3.3 is no longer supported.\r\n\r\nHowever, it's unlikely anyone will hit this since JBOD is not officially supported in 3.3 (when in KRaft mode, at least), \r\n\r\nThis PR has fixed the ducktape test failure.\r\n\r\nhttps://github.com/apache/kafka/pull/18386\r\n\r\nClosing as WONTFIX","created":"2025-01-29T21:39:32.216+0000","updated":"2025-01-29T21:39:32.216+0000","updateAuthor":{"displayName":"Colin McCabe"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at org.apache.kafka.clients.admin.KafkaAdminClientTest.lambda$callClientQuotasApisAndExpectAnAuthenticationError$44(KafkaAdminClientTest.java:1836) at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:53) at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:35) at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3115) at org.apache.kafka.clients.admin.KafkaAdminClientTest.callClientQuotasApisAndExpectAnAuthenticationError(KafkaAdminClientTest.java:1835) at org.apache.kafka.clients.admin.KafkaAdminClientTest.testAdminClientApisAuthenticationFailure(KafkaAdminClientTest.java:1791) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:728) at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) at org.junit.jupiter.engine.extension.SameThreadTimeoutInvocation.proceed(SameThreadTimeoutInvocation.java:45) at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156) at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147) at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86) at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103) at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93) at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92) at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:218) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:214) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:139) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:69) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at java.base/java.util.ArrayList.forEach(ArrayList.java:1597) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at java.base/java.util.ArrayList.forEach(ArrayList.java:1597) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85) at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47) at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124) at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99) at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92) at jdk.proxy1/jdk.proxy1.$Proxy4.stop(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200) at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132) at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103) at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63) at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:121) at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71) at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas ==> Unexpected type, expected: but was:","output":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFutu","output":"Resolved","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFutu","output":"Major","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFutu","output":"org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutio","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFutu","output":"Fixed","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure\n\norg.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node as\n\nConversation:\nuser: I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.\nassistant: Yeah, I think this is now completely broken. We should disable if we can't fix it asap.\nassistant: I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will \nassistant: PR: https://github.com/apache/kafka/pull/18735","output":"PR: https://github.com/apache/kafka/pull/18735","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"org.opentest4j.AssertionFailedError: Expected an authentication error, but got java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: alterClientQuotas at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at org.apache.kafka.clients.admin.KafkaAdminClientTest.lambda$callClientQuotasApisAndExpectAnAuthenticationError$44(KafkaAdminClientTest.java:1836) at org.junit.jupiter.api.AssertThrows.assertThrows(Asse","output":"Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure","metadata":{"issue_id":"KAFKA-18441","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:33:56.000+0000","updated":"2025-02-11T02:05:34.000+0000","resolved":"2025-02-11T02:05:34.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17921846","author":{"displayName":"Lianet Magrans"},"body":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","body_raw":"I'm seeing this test failing in several recent PRs (Failed, not Flaky). It's marked as Flaky but I guess it's not passing even with retries.","created":"2025-01-28T19:21:02.793+0000","updated":"2025-01-28T19:21:02.793+0000","updateAuthor":{"displayName":"Lianet Magrans"}},{"id":"17921982","author":{"displayName":"Ismael Juma"},"body":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","body_raw":"Yeah, I think this is now completely broken. We should disable if we can't fix it asap.","created":"2025-01-29T02:33:58.052+0000","updated":"2025-01-29T02:33:58.052+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17921984","author":{"displayName":"PoAn Yang"},"body":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","body_raw":"I think the root cause may be that default \"retry.backoff.ms\" is 100L. If the case can't finish all assertion in 100L, a new fetchMetadata request will be sent. Since we don't set next pendingAuthenticationErrors, so the mockClient will keep waiting for the metadata response. Finally, the case will fail. I will create a PR to set \"retry.backoff.ms\" in the case to avoid the error.","created":"2025-01-29T03:17:49.683+0000","updated":"2025-01-29T03:19:15.796+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17921990","author":{"displayName":"PoAn Yang"},"body":"PR: https://github.com/apache/kafka/pull/18735","body_raw":"PR: https://github.com/apache/kafka/pull/18735","created":"2025-01-29T05:38:56.504+0000","updated":"2025-01-29T05:38:56.504+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the request to controller until timeout.","output":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the req","output":"Resolved","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the req","output":"Blocker","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the req","output":"Admin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authori","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the req","output":"Fixed","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers\n\nAdmin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://git\n\nConversation:\nuser: we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.\nassistant: Hi chia7712, I will handle it. Thanks for creating the Jira.\nassistant: dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1\nassistant: chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.\nassistant: I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with cautio","output":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Admin use DescribeClusterRequest to build metadata when using bootstrap controllers, and controller APIs may return ClusterAuthorizationException when users have no \"ALTER\" permission (see https://github.com/apache/kafka/pull/14306#discussion_r1312367762). However, admin does not convert the authorized exception to fatal exception (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java#L276), so it keeps sending the request to controller until timeout.","output":"Admin does not convert the AuthorizationException to fatal error in using bootstrap controllers","metadata":{"issue_id":"KAFKA-18440","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T19:22:18.000+0000","updated":"2025-01-15T00:10:19.000+0000","resolved":"2025-01-15T00:08:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17910778","author":{"displayName":"Chia-Ping Tsai"},"body":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","body_raw":"we should convert the authorized exception to fatal error, so users can see the true exception rather than timeout exception.","created":"2025-01-07T19:23:13.861+0000","updated":"2025-01-07T19:23:13.861+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910842","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, I will handle it. Thanks for creating the Jira.","body_raw":"Hi [~chia7712], I will handle it. Thanks for creating the Jira.","created":"2025-01-07T23:55:06.130+0000","updated":"2025-01-07T23:55:06.130+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17912414","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","body_raw":"[~dajac] Should we ship this PR to 4.0.0? This is a (small) bug fix. The case of using controller as bootstrap without permission is rare so this bug is not critical. Hence, I think it is fine to delay it until 4.0.1","created":"2025-01-13T09:17:04.829+0000","updated":"2025-01-13T09:17:04.829+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17912527","author":{"displayName":"David Jacot"},"body":"chia7712 Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","body_raw":"[~chia7712] Thanks for raising it. I agree with you, it seems that it can wait until 4.0.1/4.1.0.","created":"2025-01-13T13:46:39.409+0000","updated":"2025-01-13T13:46:39.409+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912843","author":{"displayName":"Divij Vaidya"},"body":"I was weighing the decision of backporting this to 3.9/3.8. On one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception. We usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast. chia7712 dajac thoughts?","body_raw":"I was weighing the decision of backporting this to 3.9/3.8.\r\n\r\nOn one hand, this will change the user facing exception and hence, potentially break existing exception handling login in user's code . On the other hand, it does fix a genuine bug and throws the right exception.\r\n\r\nWe usually side with caution when we release patches and tend to only fix regressions (and security/durability problems). I am leaning towards *not* including this in 3.8/3.9. Without this fix, the user will still get the failure (after timeout has elapsed), it's just that they will not fail fast.\r\n\r\n[~chia7712] [~dajac] thoughts? ","created":"2025-01-14T11:40:03.169+0000","updated":"2025-01-14T11:45:10.530+0000","updateAuthor":{"displayName":"Divij Vaidya"}},{"id":"17912997","author":{"displayName":"Chia-Ping Tsai"},"body":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception. yangpoan if you are on the same page, please close this jira and update the fix version.","body_raw":"I see no compelling reason to backport this to active branches. As I mentioned, the use case is infrequent, and therefore it should be acceptable to continue throwing a timeout exception.\r\n\r\n[~yangpoan] if you are on the same page, please close this jira and update the fix version. ","created":"2025-01-14T17:45:29.290+0000","updated":"2025-01-14T17:45:29.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add invariant to Raft simulation test for prevote\n\n[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong test that was added in the PR, but does not apply well to existing tests which expect leadership to change (e.g. disruptive network partition added, simulated fetch lag) What we need it to do is check that a new leader is not elected if the current leader had connectivity with a majority of quorum.","output":"Add invariant to Raft simulation test for prevote","metadata":{"issue_id":"KAFKA-18439","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-07T18:16:42.000+0000","updated":"2025-01-17T14:55:28.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add invariant to Raft simulation test for prevote\n\n[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong test that was added in the PR, but does not apply well to existing tests which expect leadership to change (e.g. disruptive network partition added, simulated fetch lag) What we need it to do is check ","output":"Open","metadata":{"issue_id":"KAFKA-18439","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-07T18:16:42.000+0000","updated":"2025-01-17T14:55:28.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add invariant to Raft simulation test for prevote\n\n[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong test that was added in the PR, but does not apply well to existing tests which expect leadership to change (e.g. disruptive network partition added, simulated fetch lag) What we need it to do is check ","output":"Major","metadata":{"issue_id":"KAFKA-18439","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-07T18:16:42.000+0000","updated":"2025-01-17T14:55:28.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add invariant to Raft simulation test for prevote\n\n[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong test that was added in the PR, but does not apply well to existing tests which expect leadership to change (e.g. disruptive network partition added, simulated fetch lag) What we need it to do is check ","output":"[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong t","metadata":{"issue_id":"KAFKA-18439","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-07T18:16:42.000+0000","updated":"2025-01-17T14:55:28.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/pull/18240] introduces StableLeadershipWhenMajorityReachable, which is a crude first iteration of the invariant we need for checking correctness with PreVote. It checks that only one leader is elected during the span of a test. This works for the specific ping-pong test that was added in the PR, but does not apply well to existing tests which expect leadership to change (e.g. disruptive network partition added, simulated fetch lag) What we need it to do is check that a new leader is not elected if the current leader had connectivity with a majority of quorum.","output":"Add invariant to Raft simulation test for prevote","metadata":{"issue_id":"KAFKA-18439","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-07T18:16:42.000+0000","updated":"2025-01-17T14:55:28.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Correct version of ShareUpdateValue record from v1 to v0\n\nSHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"Correct version of ShareUpdateValue record from v1 to v0","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Correct version of ShareUpdateValue record from v1 to v0\n\nSHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"Resolved","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Correct version of ShareUpdateValue record from v1 to v0\n\nSHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"Major","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Correct version of ShareUpdateValue record from v1 to v0\n\nSHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"SHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Correct version of ShareUpdateValue record from v1 to v0\n\nSHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"Fixed","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"SHARE_UPDATE_RECORD_VALUE_VERSION should be 0. In AK 4.1, this is being fixed as part of a larger refactor to how all coordinator records are generated and managed, but in AK 4.0, a smaller fix is required.","output":"Correct version of ShareUpdateValue record from v1 to v0","metadata":{"issue_id":"KAFKA-18437","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T17:55:48.000+0000","updated":"2025-01-09T13:47:51.000+0000","resolved":"2025-01-09T13:47:51.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","output":"Remove zookeeper dependencies in build.gradle","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","output":"Resolved","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","output":"Blocker","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","output":"After we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project","output":"Fixed","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zookeeper dependencies in build.gradle\n\nAfter we clean up all zookeeper class, we could remove the zookeeper dependencies from Kafka project\n\nConversation:\nuser: Marking as 4.0 blocker. cc dajac\nassistant: trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","output":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","metadata":{"issue_id":"KAFKA-18435","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T17:17:43.000+0000","updated":"2025-01-31T14:32:06.000+0000","resolved":"2025-01-09T02:29:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17911243","author":{"displayName":"Colin McCabe"},"body":"Marking as 4.0 blocker. cc dajac","body_raw":"Marking as 4.0 blocker. cc [~dajac]","created":"2025-01-08T21:35:57.088+0000","updated":"2025-01-08T21:35:57.088+0000","updateAuthor":{"displayName":"Colin McCabe"}},{"id":"17911301","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae 4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","body_raw":"trunk: https://github.com/apache/kafka/commit/c28d9a348670f9efc599b481366da0af813aa8ae\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/f77616cba10ac3cd2242c0690052a4511433a717","created":"2025-01-09T02:29:54.821+0000","updated":"2025-01-09T02:29:54.821+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"enrich the authorization error message of connecting to controller\n\nDESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"enrich the authorization error message of connecting to controller","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"enrich the authorization error message of connecting to controller\n\nDESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"Resolved","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"enrich the authorization error message of connecting to controller\n\nDESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"Major","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: enrich the authorization error message of connecting to controller\n\nDESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"DESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"enrich the authorization error message of connecting to controller\n\nDESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"Fixed","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"DESCRIBE_CLUSTER request needs ALTER permission when connecting to controller. However, the error message does not include the reason. There is a good explanation in the origin PR (https://github.com/apache/kafka/pull/14306#discussion_r1312367762) and we should add it to the exception message","output":"enrich the authorization error message of connecting to controller","metadata":{"issue_id":"KAFKA-18434","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T15:22:52.000+0000","updated":"2025-01-08T17:56:58.000+0000","resolved":"2025-01-08T17:56:57.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement\n\nChange the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement\n\nChange the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Resolved","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement\n\nChange the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Major","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement\n\nChange the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Change the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the op","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement\n\nChange the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Fixed","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Change the RPC request to better align the consumer and broker processing for fetching and acknowledgement. Specifically: * Remove PartitionMaxBytes because for share fetching the broker has responsibility for which partitions are fetched as opposed to the consumer * Add BatchSize which gives the optimal number of records for batches of acquired records and acknowledgements. There will likely be a future round of optimisation to follow.","output":"Adjust ShareFetchRequest RPC to optimise fetching and acknowledgement","metadata":{"issue_id":"KAFKA-18433","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-07T13:32:21.000+0000","updated":"2025-04-01T09:36:15.000+0000","resolved":"2025-04-01T09:36:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove unused code from AutoTopicCreationManager\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18432","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T13:08:19.000+0000","updated":"2025-01-08T19:11:25.000+0000","resolved":"2025-01-08T19:11:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910979","author":{"displayName":"Chia-Ping Tsai"},"body":"AutoTopicCreationManager is still used by kraft server","body_raw":"AutoTopicCreationManager is still used by kraft server","created":"2025-01-08T08:52:57.752+0000","updated":"2025-01-08T08:52:57.752+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910980","author":{"displayName":"Chia-Ping Tsai"},"body":"however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordinator: TransactionCoordinator, shareCoordinator: Option[ShareCoordinator], ): AutoTopicCreationManager = { new DefaultAutoTopicCreationManager(config, channelManager, adminManager, controller, groupCoordinator, txnCoordinator, shareCoordinator) } } ```","body_raw":"however, we can remove some unused code. For example: \r\n```\r\nobject AutoTopicCreationManager {\r\n\r\n  def apply(\r\n   config: KafkaConfig,\r\n   channelManager: Option[NodeToControllerChannelManager],\r\n   adminManager: Option[ZkAdminManager],\r\n   controller: Option[KafkaController],\r\n   groupCoordinator: GroupCoordinator,\r\n   txnCoordinator: TransactionCoordinator,\r\n   shareCoordinator: Option[ShareCoordinator],\r\n ): AutoTopicCreationManager = {\r\n    new DefaultAutoTopicCreationManager(config, channelManager, adminManager,\r\n      controller, groupCoordinator, txnCoordinator, shareCoordinator)\r\n  }\r\n}\r\n```","created":"2025-01-08T08:56:12.079+0000","updated":"2025-01-08T08:56:12.079+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910981","author":{"displayName":"Chia-Ping Tsai"},"body":"`adminManager` and `controller` - those zk-related code can be removed too","body_raw":"`adminManager` and `controller` - those zk-related code can be removed too","created":"2025-01-08T08:57:32.218+0000","updated":"2025-01-08T08:57:32.218+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911211","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","body_raw":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba\r\n\r\n4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","created":"2025-01-08T19:11:25.561+0000","updated":"2025-01-08T19:11:25.561+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove unused code from AutoTopicCreationManager\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18432","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T13:08:19.000+0000","updated":"2025-01-08T19:11:25.000+0000","resolved":"2025-01-08T19:11:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910979","author":{"displayName":"Chia-Ping Tsai"},"body":"AutoTopicCreationManager is still used by kraft server","body_raw":"AutoTopicCreationManager is still used by kraft server","created":"2025-01-08T08:52:57.752+0000","updated":"2025-01-08T08:52:57.752+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910980","author":{"displayName":"Chia-Ping Tsai"},"body":"however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordinator: TransactionCoordinator, shareCoordinator: Option[ShareCoordinator], ): AutoTopicCreationManager = { new DefaultAutoTopicCreationManager(config, channelManager, adminManager, controller, groupCoordinator, txnCoordinator, shareCoordinator) } } ```","body_raw":"however, we can remove some unused code. For example: \r\n```\r\nobject AutoTopicCreationManager {\r\n\r\n  def apply(\r\n   config: KafkaConfig,\r\n   channelManager: Option[NodeToControllerChannelManager],\r\n   adminManager: Option[ZkAdminManager],\r\n   controller: Option[KafkaController],\r\n   groupCoordinator: GroupCoordinator,\r\n   txnCoordinator: TransactionCoordinator,\r\n   shareCoordinator: Option[ShareCoordinator],\r\n ): AutoTopicCreationManager = {\r\n    new DefaultAutoTopicCreationManager(config, channelManager, adminManager,\r\n      controller, groupCoordinator, txnCoordinator, shareCoordinator)\r\n  }\r\n}\r\n```","created":"2025-01-08T08:56:12.079+0000","updated":"2025-01-08T08:56:12.079+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910981","author":{"displayName":"Chia-Ping Tsai"},"body":"`adminManager` and `controller` - those zk-related code can be removed too","body_raw":"`adminManager` and `controller` - those zk-related code can be removed too","created":"2025-01-08T08:57:32.218+0000","updated":"2025-01-08T08:57:32.218+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911211","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","body_raw":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba\r\n\r\n4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","created":"2025-01-08T19:11:25.561+0000","updated":"2025-01-08T19:11:25.561+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove unused code from AutoTopicCreationManager\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18432","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T13:08:19.000+0000","updated":"2025-01-08T19:11:25.000+0000","resolved":"2025-01-08T19:11:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910979","author":{"displayName":"Chia-Ping Tsai"},"body":"AutoTopicCreationManager is still used by kraft server","body_raw":"AutoTopicCreationManager is still used by kraft server","created":"2025-01-08T08:52:57.752+0000","updated":"2025-01-08T08:52:57.752+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910980","author":{"displayName":"Chia-Ping Tsai"},"body":"however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordinator: TransactionCoordinator, shareCoordinator: Option[ShareCoordinator], ): AutoTopicCreationManager = { new DefaultAutoTopicCreationManager(config, channelManager, adminManager, controller, groupCoordinator, txnCoordinator, shareCoordinator) } } ```","body_raw":"however, we can remove some unused code. For example: \r\n```\r\nobject AutoTopicCreationManager {\r\n\r\n  def apply(\r\n   config: KafkaConfig,\r\n   channelManager: Option[NodeToControllerChannelManager],\r\n   adminManager: Option[ZkAdminManager],\r\n   controller: Option[KafkaController],\r\n   groupCoordinator: GroupCoordinator,\r\n   txnCoordinator: TransactionCoordinator,\r\n   shareCoordinator: Option[ShareCoordinator],\r\n ): AutoTopicCreationManager = {\r\n    new DefaultAutoTopicCreationManager(config, channelManager, adminManager,\r\n      controller, groupCoordinator, txnCoordinator, shareCoordinator)\r\n  }\r\n}\r\n```","created":"2025-01-08T08:56:12.079+0000","updated":"2025-01-08T08:56:12.079+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910981","author":{"displayName":"Chia-Ping Tsai"},"body":"`adminManager` and `controller` - those zk-related code can be removed too","body_raw":"`adminManager` and `controller` - those zk-related code can be removed too","created":"2025-01-08T08:57:32.218+0000","updated":"2025-01-08T08:57:32.218+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911211","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","body_raw":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba\r\n\r\n4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","created":"2025-01-08T19:11:25.561+0000","updated":"2025-01-08T19:11:25.561+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove unused code from AutoTopicCreationManager\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18432","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T13:08:19.000+0000","updated":"2025-01-08T19:11:25.000+0000","resolved":"2025-01-08T19:11:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910979","author":{"displayName":"Chia-Ping Tsai"},"body":"AutoTopicCreationManager is still used by kraft server","body_raw":"AutoTopicCreationManager is still used by kraft server","created":"2025-01-08T08:52:57.752+0000","updated":"2025-01-08T08:52:57.752+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910980","author":{"displayName":"Chia-Ping Tsai"},"body":"however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordinator: TransactionCoordinator, shareCoordinator: Option[ShareCoordinator], ): AutoTopicCreationManager = { new DefaultAutoTopicCreationManager(config, channelManager, adminManager, controller, groupCoordinator, txnCoordinator, shareCoordinator) } } ```","body_raw":"however, we can remove some unused code. For example: \r\n```\r\nobject AutoTopicCreationManager {\r\n\r\n  def apply(\r\n   config: KafkaConfig,\r\n   channelManager: Option[NodeToControllerChannelManager],\r\n   adminManager: Option[ZkAdminManager],\r\n   controller: Option[KafkaController],\r\n   groupCoordinator: GroupCoordinator,\r\n   txnCoordinator: TransactionCoordinator,\r\n   shareCoordinator: Option[ShareCoordinator],\r\n ): AutoTopicCreationManager = {\r\n    new DefaultAutoTopicCreationManager(config, channelManager, adminManager,\r\n      controller, groupCoordinator, txnCoordinator, shareCoordinator)\r\n  }\r\n}\r\n```","created":"2025-01-08T08:56:12.079+0000","updated":"2025-01-08T08:56:12.079+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910981","author":{"displayName":"Chia-Ping Tsai"},"body":"`adminManager` and `controller` - those zk-related code can be removed too","body_raw":"`adminManager` and `controller` - those zk-related code can be removed too","created":"2025-01-08T08:57:32.218+0000","updated":"2025-01-08T08:57:32.218+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911211","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","body_raw":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba\r\n\r\n4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","created":"2025-01-08T19:11:25.561+0000","updated":"2025-01-08T19:11:25.561+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove unused code from AutoTopicCreationManager\n\nas title\n\nConversation:\nuser: AutoTopicCreationManager is still used by kraft server\nassistant: however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordi\nassistant: `adminManager` and `controller` - those zk-related code can be removed too\nassistant: trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","output":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","metadata":{"issue_id":"KAFKA-18432","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T13:08:19.000+0000","updated":"2025-01-08T19:11:25.000+0000","resolved":"2025-01-08T19:11:25.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910979","author":{"displayName":"Chia-Ping Tsai"},"body":"AutoTopicCreationManager is still used by kraft server","body_raw":"AutoTopicCreationManager is still used by kraft server","created":"2025-01-08T08:52:57.752+0000","updated":"2025-01-08T08:52:57.752+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910980","author":{"displayName":"Chia-Ping Tsai"},"body":"however, we can remove some unused code. For example: ``` object AutoTopicCreationManager { def apply( config: KafkaConfig, channelManager: Option[NodeToControllerChannelManager], adminManager: Option[ZkAdminManager], controller: Option[KafkaController], groupCoordinator: GroupCoordinator, txnCoordinator: TransactionCoordinator, shareCoordinator: Option[ShareCoordinator], ): AutoTopicCreationManager = { new DefaultAutoTopicCreationManager(config, channelManager, adminManager, controller, groupCoordinator, txnCoordinator, shareCoordinator) } } ```","body_raw":"however, we can remove some unused code. For example: \r\n```\r\nobject AutoTopicCreationManager {\r\n\r\n  def apply(\r\n   config: KafkaConfig,\r\n   channelManager: Option[NodeToControllerChannelManager],\r\n   adminManager: Option[ZkAdminManager],\r\n   controller: Option[KafkaController],\r\n   groupCoordinator: GroupCoordinator,\r\n   txnCoordinator: TransactionCoordinator,\r\n   shareCoordinator: Option[ShareCoordinator],\r\n ): AutoTopicCreationManager = {\r\n    new DefaultAutoTopicCreationManager(config, channelManager, adminManager,\r\n      controller, groupCoordinator, txnCoordinator, shareCoordinator)\r\n  }\r\n}\r\n```","created":"2025-01-08T08:56:12.079+0000","updated":"2025-01-08T08:56:12.079+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910981","author":{"displayName":"Chia-Ping Tsai"},"body":"`adminManager` and `controller` - those zk-related code can be removed too","body_raw":"`adminManager` and `controller` - those zk-related code can be removed too","created":"2025-01-08T08:57:32.218+0000","updated":"2025-01-08T08:57:32.218+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17911211","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba 4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","body_raw":"trunk: https://github.com/apache/kafka/commit/5efaae65c6b33af034a368d71ebf68e10e4286ba\r\n\r\n4.0: https://github.com/apache/kafka/commit/754ede8b72b8c720c7bf306d9404f863049a9ac4","created":"2025-01-08T19:11:25.561+0000","updated":"2025-01-08T19:11:25.561+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"Remove KafkaController","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"Resolved","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"Major","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"Fixed","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove KafkaController\n\n[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]\n\nConversation:\nuser: reopend due to not backport to 4.0 yet.","output":"reopend due to not backport to 4.0 yet.","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala]","output":"Remove KafkaController","metadata":{"issue_id":"KAFKA-18431","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2025-01-07T12:57:44.000+0000","updated":"2025-01-21T12:23:55.000+0000","resolved":"2025-01-17T13:26:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913929","author":{"displayName":"TengYao Chi"},"body":"reopend due to not backport to 4.0 yet.","body_raw":"reopend due to not backport to 4.0 yet.","created":"2025-01-17T03:08:34.411+0000","updated":"2025-01-17T03:08:43.566+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Remove ZkNodeChangeNotificationListener","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Resolved","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Major","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"We could remove it since Kafka no longer relies on Zookeeper.","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Fixed","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkNodeChangeNotificationListener\n\nWe could remove it since Kafka no longer relies on Zookeeper.\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","output":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","metadata":{"issue_id":"KAFKA-18430","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:08:22.000+0000","updated":"2025-01-18T17:51:32.000+0000","resolved":"2025-01-18T17:51:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914357","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd] 4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","body_raw":"trunk: [https://github.com/apache/kafka/commit/a814e21da80cb166b772d29d9a0f3e63b8bc86fd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/030f5de55a137c8d8af78467dd2eb6c2fb0c0615","created":"2025-01-18T17:51:32.582+0000","updated":"2025-01-18T17:51:32.582+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Remove ZkFinalizedFeatureCache and StateChangeFailedException","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Resolved","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Major","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"We could remove it since Kafka no longer relies on Zookeeper.","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Fixed","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkFinalizedFeatureCache and StateChangeFailedException\n\nWe could remove it since Kafka no longer relies on Zookeeper.\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","output":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","metadata":{"issue_id":"KAFKA-18429","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:06:17.000+0000","updated":"2025-01-19T11:16:40.000+0000","resolved":"2025-01-19T11:16:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914433","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1] 4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","body_raw":"trunk: [https://github.com/apache/kafka/commit/516d5240b98916feb3e51c8a143dede05a4edad1]\r\n\r\n4.0: https://github.com/apache/kafka/commit/df676bcb427c7c5a0cdacde0b3a0c1d9fbd05d6c","created":"2025-01-19T11:16:40.702+0000","updated":"2025-01-19T11:16:40.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Measure share consumers performance\n\nCreate a script to evaluate share consumers performance","output":"Measure share consumers performance","metadata":{"issue_id":"KAFKA-18428","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-07T12:02:16.000+0000","updated":"2025-01-08T15:14:38.000+0000","resolved":"2025-01-08T15:14:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Measure share consumers performance\n\nCreate a script to evaluate share consumers performance","output":"Resolved","metadata":{"issue_id":"KAFKA-18428","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-07T12:02:16.000+0000","updated":"2025-01-08T15:14:38.000+0000","resolved":"2025-01-08T15:14:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Measure share consumers performance\n\nCreate a script to evaluate share consumers performance","output":"Major","metadata":{"issue_id":"KAFKA-18428","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-07T12:02:16.000+0000","updated":"2025-01-08T15:14:38.000+0000","resolved":"2025-01-08T15:14:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Measure share consumers performance\n\nCreate a script to evaluate share consumers performance","output":"Create a script to evaluate share consumers performance","metadata":{"issue_id":"KAFKA-18428","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-07T12:02:16.000+0000","updated":"2025-01-08T15:14:38.000+0000","resolved":"2025-01-08T15:14:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Measure share consumers performance\n\nCreate a script to evaluate share consumers performance","output":"Fixed","metadata":{"issue_id":"KAFKA-18428","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Abhinav Dixit"},"reporter":{"displayName":"Abhinav Dixit"},"created":"2025-01-07T12:02:16.000+0000","updated":"2025-01-08T15:14:38.000+0000","resolved":"2025-01-08T15:14:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZooKeeperClient\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Remove ZooKeeperClient","metadata":{"issue_id":"KAFKA-18427","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:01:07.000+0000","updated":"2025-01-19T16:24:30.000+0000","resolved":"2025-01-19T16:24:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZooKeeperClient\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Resolved","metadata":{"issue_id":"KAFKA-18427","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:01:07.000+0000","updated":"2025-01-19T16:24:30.000+0000","resolved":"2025-01-19T16:24:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZooKeeperClient\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Major","metadata":{"issue_id":"KAFKA-18427","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:01:07.000+0000","updated":"2025-01-19T16:24:30.000+0000","resolved":"2025-01-19T16:24:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZooKeeperClient\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"We could remove it since Kafka no longer relies on Zookeeper.","metadata":{"issue_id":"KAFKA-18427","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:01:07.000+0000","updated":"2025-01-19T16:24:30.000+0000","resolved":"2025-01-19T16:24:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZooKeeperClient\n\nWe could remove it since Kafka no longer relies on Zookeeper.","output":"Fixed","metadata":{"issue_id":"KAFKA-18427","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-07T12:01:07.000+0000","updated":"2025-01-19T16:24:30.000+0000","resolved":"2025-01-19T16:24:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"Remove FinalizedFeatureChangeListener","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"Resolved","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"Major","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"FinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"Fixed","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove FinalizedFeatureChangeListener\n\nFinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","output":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"FinalizedFeatureChangeListener is used to detect changes in ZK feature nodes through the zk client. We could remove it since AK no longer relies on ZK.","output":"Remove FinalizedFeatureChangeListener","metadata":{"issue_id":"KAFKA-18426","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:26:56.000+0000","updated":"2025-01-08T17:32:26.000+0000","resolved":"2025-01-08T17:32:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911178","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457 4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","body_raw":"trunk: https://github.com/apache/kafka/commit/af3f9e3a5adf3379b178b85878160752e8b72457\r\n\r\n4.0: https://github.com/apache/kafka/commit/3758e5b43f2a672bfda70e5359708719f1460095","created":"2025-01-08T17:32:26.547+0000","updated":"2025-01-08T17:32:26.547+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"Remove OffsetTrackingListener","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"Resolved","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"Major","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"Fixed","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove OffsetTrackingListener\n\n`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","output":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"`OffsetTrackingListener` is used to register ZK brokers with the KRaft controller during a KIP-866 migration. We could remove it since AK no longer relies on ZK.","output":"Remove OffsetTrackingListener","metadata":{"issue_id":"KAFKA-18425","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-07T11:17:13.000+0000","updated":"2025-01-08T16:22:50.000+0000","resolved":"2025-01-08T16:22:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17911158","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216 4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","body_raw":"trunk: https://github.com/apache/kafka/commit/aa22676c483e2e6164bd6cb67539d0a5eb30a216\r\n\r\n4.0: https://github.com/apache/kafka/commit/58a42bcf8607b285c614f3d6509503337df7ad23","created":"2025-01-08T16:22:50.600+0000","updated":"2025-01-08T16:22:50.600+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"Resolved","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"Major","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"That test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"Fixed","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups\n\nThat test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.\n\nConversation:\nuser: Hi dajac I would like to take over this issue,","output":"Hi dajac I would like to take over this issue,","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"That test tests 10000 things. It is hard to follow and debug. In my opinion, we should split it into smaller units.","output":"Consider splitting PlaintextAdminIntegrationTest#testConsumerGroups","metadata":{"issue_id":"KAFKA-18424","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-07T10:14:49.000+0000","updated":"2025-05-26T04:12:01.000+0000","resolved":"2025-05-26T04:12:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910573","author":{"displayName":"TengYao Chi"},"body":"Hi dajac I would like to take over this issue,","body_raw":"Hi [~dajac] \r\n\r\nI would like to take over this issue,","created":"2025-01-07T10:45:43.178+0000","updated":"2025-01-07T10:45:43.178+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545","output":"add Kafka client upgrade path","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545","output":"Resolved","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545","output":"Blocker","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545","output":"https://github.com/apache/kafka/pull/18193#issuecomment-2572283545","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545","output":"Fixed","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"add Kafka client upgrade path\n\nhttps://github.com/apache/kafka/pull/18193#issuecomment-2572283545\n\nConversation:\nuser: ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?\nassistant: dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...\nassistant: Thanks chia7712 !\nassistant: To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code cha\nassistant: Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","output":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","metadata":{"issue_id":"KAFKA-18422","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-07T06:24:33.000+0000","updated":"2025-03-12T13:58:32.000+0000","resolved":"2025-03-12T13:58:32.000+0000","resolution":"Fixed","labels":["need-kip"],"components":[],"comment_count":8,"comments":[{"id":"17914645","author":{"displayName":"David Jacot"},"body":"ijuma chia7712 How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","body_raw":"[~ijuma] [~chia7712] How do we go about this one for 4.0? We cannot really have it requiring a KIP and being a blocker for 4.0. Is it something that we could do in 4.1?","created":"2025-01-20T12:46:05.584+0000","updated":"2025-01-20T12:46:05.584+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914684","author":{"displayName":"Chia-Ping Tsai"},"body":"dajac yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","body_raw":"[~dajac] yes, we can move it to 4.1 as I don't think we can reach the consensus before 4.0.0 release ...","created":"2025-01-20T13:44:54.158+0000","updated":"2025-01-20T13:44:54.158+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914690","author":{"displayName":"David Jacot"},"body":"Thanks chia7712 !","body_raw":"Thanks [~chia7712] !","created":"2025-01-20T13:54:59.534+0000","updated":"2025-01-20T13:54:59.534+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914736","author":{"displayName":"Ismael Juma"},"body":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0. My take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","body_raw":"To be clear, we cannot change the client upgrade guarantees outside of a major release. So, we should discuss whether we do it now or in 5.0.\r\n\r\nMy take is that it's a no brainer to do what the KIP is proposing: it's good for everyone (less maintenance for us, reduced risk for users). Also, no code changes are required for this KIP. So, I think we should consider making an exception.","created":"2025-01-20T17:15:02.042+0000","updated":"2025-01-20T17:15:22.825+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17914749","author":{"displayName":"David Jacot"},"body":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","body_raw":"Making an exception for this one makes sense. Let me bring it back as a blocker for 4.0. I will also review the KIP this week.","created":"2025-01-20T17:40:21.332+0000","updated":"2025-01-20T17:40:21.332+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914775","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} Also, no code changes are required for this KIP. So, I think we should consider making an exception. {quote} we may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","body_raw":"{quote}\r\n\r\nAlso, no code changes are required for this KIP. So, I think we should consider making an exception.\r\n\r\n{quote}\r\n\r\nwe may need to change the code of e2e according to the supported upgrade path. That is not production code so it should be fine ... ?","created":"2025-01-20T19:05:52.219+0000","updated":"2025-01-20T19:05:52.219+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17914783","author":{"displayName":"Ismael Juma"},"body":"Yes, right - I meant no changes to the non test code. :)","body_raw":"Yes, right - I meant no changes to the non test code. :)","created":"2025-01-20T19:33:40.572+0000","updated":"2025-01-20T19:33:40.572+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17932534","author":{"displayName":"Chia-Ping Tsai"},"body":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. I think we can resolve this jira after the KIP vote gets pass","body_raw":"the versions excluded by the KIP-1124 in streams_upgrade_test.py are removed by KAFKA-18839. Also, the link of KIP is already added by https://github.com/apache/kafka/pull/19097. \r\n\r\nI think we can resolve this jira after the KIP vote gets pass","created":"2025-03-05T07:29:39.623+0000","updated":"2025-03-05T07:29:39.623+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed\n\nCf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=ListenerName(EXTERNAL),ControllerSecurityProtocol=PLAINTEXT,ControllerListenerName=ListenerName(CONTROLLER)}} {{}} {{}}","output":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed\n\nCf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=ListenerName(EXTERNAL),ControllerSecurityProtocol=PLAINTEXT,ControllerListenerName=ListenerName(CONTROLLER)}} {{}} {{}}","output":"Open","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed\n\nCf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=ListenerName(EXTERNAL),ControllerSecurityProtocol=PLAINTEXT,ControllerListenerName=ListenerName(CONTROLLER)}} {{}} {{}}","output":"Minor","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed\n\nCf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=ListenerName(EXTERNAL),ControllerSecurityProtocol=PLAINTEXT,ControllerListenerName=ListenerName(CONTROLLER)}} {{}} {{}}","output":"Cf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=Listen","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed\n\nCf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Ty\n\nConversation:\nuser: FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak dete","output":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak dete","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Cf [https://github.com/apache/kafka/actions/runs/12636867829/job/35222242253?pr=18402] {{FAILED ❌ ConsumerProtocolMigrationTest > testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy [1] Type=Raft-Isolated, MetadataVersion=4.0-IV3,BrokerSecurityProtocol=PLAINTEXT,BrokerListenerName=ListenerName(EXTERNAL),ControllerSecurityProtocol=PLAINTEXT,ControllerListenerName=ListenerName(CONTROLLER)}} {{}} {{}}","output":"Test ConsumerProtocolMigrationTest.testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy failed","metadata":{"issue_id":"KAFKA-18421","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"Matthias J. Sax"},"created":"2025-01-07T02:28:14.000+0000","updated":"2025-01-13T18:48:29.000+0000","resolved":null,"labels":["kip-848"],"components":["group-coordinator","unit tests"],"comment_count":1,"comments":[{"id":"17912611","author":{"displayName":"Lianet Magrans"},"body":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test. The test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","body_raw":"FYI, I updated labels/severity/version to fix the triage here, this is not a client-related test, not a blocker for 4.0. It's a broker integration test that does not use the consumer (sends the HB request directly through the network layer), and the failure on that build is due to a thread leak detected when closing the test.\r\n\r\nThe test has actually been pretty stable on trunk, no fails no flaky, [https://ge.apache.org/scans/tests?search.buildOutcome=failure&search.names=Git%20branch&search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FToronto&search.values=trunk&tests.container=kafka.server.ConsumerProtocolMigrationTest&tests.test=testDowngradeFromEmptyConsumerToClassicGroupWithDowngradePolicy()%5B1%5D] wonder if this was a hiccup with the infra...","created":"2025-01-13T16:54:16.470+0000","updated":"2025-01-13T16:54:16.470+0000","updateAuthor":{"displayName":"Lianet Magrans"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","output":"Find out the license which is in the license file but is not in distribution","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","output":"Resolved","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","output":"Major","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","output":"see discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076","output":"Duplicate","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Find out the license which is in the license file but is not in distribution\n\nsee discussion: https://github.com/apache/kafka/pull/18299#discussion_r1904604076\n\nConversation:\nuser: file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","output":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","metadata":{"issue_id":"KAFKA-18420","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"kangning.li"},"created":"2025-01-07T00:53:25.000+0000","updated":"2025-03-09T03:49:39.000+0000","resolved":"2025-03-09T03:49:39.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17933584","author":{"displayName":"kangning.li"},"body":"file: [https://github.com/apache/kafka/pull/18299] This PR has already accomplished this task. So I am going to close this issue.","body_raw":"file: [https://github.com/apache/kafka/pull/18299]\r\n \r\n\r\nThis PR has already accomplished this task. So I am going to close this issue.","created":"2025-03-09T03:47:59.931+0000","updated":"2025-03-09T03:47:59.931+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1577). If this happens, `KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse` will fail. Trace: ``` Gradle Test Run :streams:test > Gradle Test Executor 7 > KafkaStreamsTest > shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse() FAILED java.lang.AssertionError: Expected: but: was at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse(KafkaStreamsTest.java:986) ``` Please check code https://github.com/aoli-al/kafka/tree/KAFKA-159, and run `./gradlew :streams:test --tests \"org.apache.kafka.streams.KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\"` to reproduce the failure.","output":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (h","output":"Resolved","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (h","output":"Major","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (h","output":"KafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of t","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (h","output":"Fixed","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse\n\nKafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/k\n\nConversation:\nuser: Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithClose\nassistant: Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all test\nassistant: mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concur\nassistant: Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","output":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"KafkaStreams does not synchronize with CloseThread after shutdown thread starts at line https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1571 So it is possible for the shutdown helper update the state of the KafkaStreams (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1530) before `waitOnState` is called (https://github.com/apache/kafka/blob/c1163549081561cade03bbc6a29bfe6caad332a2/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1577). If this happens, `KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse` will fail. Trace: ``` Gradle Test","output":"Flaky test in KafkaStreamsTest::shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse","metadata":{"issue_id":"KAFKA-18418","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Ao Li"},"reporter":{"displayName":"Ao Li"},"created":"2025-01-06T19:37:46.000+0000","updated":"2025-01-09T18:20:27.000+0000","resolved":"2025-01-09T18:20:27.000+0000","resolution":"Fixed","labels":[],"components":["streams","unit tests"],"comment_count":4,"comments":[{"id":"17910404","author":{"displayName":"Ao Li"},"body":"Actually, many tests fail because of this: KafkaStreamsTest.shouldNotBlockInCloseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated KafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration KafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated KafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown Maybe its worth to find a fix in the KafkaStreams instead of fixing each unit test.","body_raw":"Actually, many tests fail because of this:\r\n\r\nKafkaStreamsTest.shouldNotBlockInCloseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration\r\nKafkaStreamsTest.shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupFalseWhenThreadsHaventTerminated\r\nKafkaStreamsTest.shouldThrowOnCleanupWhileShuttingDown\r\n\r\nMaybe its worth to find a fix in the KafkaStreams instead of fixing each unit test. ","created":"2025-01-06T21:01:27.118+0000","updated":"2025-01-06T21:03:29.615+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910473","author":{"displayName":"Matthias J. Sax"},"body":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find! However, I don't think that your PR actually fully fixes it – left a commend on the PR. The fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","body_raw":"Thanks for filing this ticket. I understand the race condition for `shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse()` – great find!\r\n\r\nHowever, I don't think that your PR actually fully fixes it – left a commend on the PR.\r\n\r\nThe fix I propose should hopefully fix all tests? – I don't think there is a bug in `KafkaStreams` class, so nothing to be fixed there – it's really a testing setup race-condition IMHO.","created":"2025-01-07T02:14:02.279+0000","updated":"2025-01-07T02:14:02.279+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910687","author":{"displayName":"Ao Li"},"body":"mjsax Thanks for your comments! I've submitted a new PR based on your review. This time, I also tested the new patch using https://github.com/cmu-pasta/fray, and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","body_raw":"[~mjsax] Thanks for your comments! I've submitted a new PR based on your review. \r\n\r\nThis time, I also tested the new patch using [Fray|https://github.com/cmu-pasta/fray], and it did not report any issue after 10 minutes. (p.s. if you are interested, Fray is a concurrency testing framework we built to test concurrent programs under different thread interleavings deterministically. You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray!)","created":"2025-01-07T16:37:01.486+0000","updated":"2025-01-07T16:37:01.486+0000","updateAuthor":{"displayName":"Ao Li"}},{"id":"17910910","author":{"displayName":"Matthias J. Sax"},"body":"Thanks. Interesting. {quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray {quote} Yes, I did notice. Super helpful! \\cc cadonna lucasbru bbejeck alisa23 ableegoldman","body_raw":"Thanks. Interesting.\r\n{quote}You may have noticed that I submitted many bug reports related to concurrency issues, and they were all found by Fray\r\n{quote}\r\nYes, I did notice. Super helpful!\r\n\r\n\\cc [~cadonna] [~lucasbru] [~bbejeck] [~alisa23] [~ableegoldman] ","created":"2025-01-08T03:02:23.265+0000","updated":"2025-01-08T03:02:23.265+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove controlled.shutdown.max.retries and controlled.shutdown.retry.backoff.ms\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18417","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T19:24:45.000+0000","updated":"2025-01-08T09:16:58.000+0000","resolved":"2025-01-08T09:16:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910392","author":{"displayName":"Ismael Juma"},"body":"We should also mention this in the zk to kraft migration guide.","body_raw":"We should also mention this in the zk to kraft migration guide.","created":"2025-01-06T20:37:35.255+0000","updated":"2025-01-06T20:37:35.255+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17910555","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this, thakns:)","body_raw":"I'm working on this, thakns:)","created":"2025-01-07T09:02:13.174+0000","updated":"2025-01-07T09:02:13.174+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17910987","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","body_raw":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d\r\n\r\n4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","created":"2025-01-08T09:16:58.441+0000","updated":"2025-01-08T09:16:58.441+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove controlled.shutdown.max.retries and controlled.shutdown.retry.backoff.ms\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18417","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T19:24:45.000+0000","updated":"2025-01-08T09:16:58.000+0000","resolved":"2025-01-08T09:16:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910392","author":{"displayName":"Ismael Juma"},"body":"We should also mention this in the zk to kraft migration guide.","body_raw":"We should also mention this in the zk to kraft migration guide.","created":"2025-01-06T20:37:35.255+0000","updated":"2025-01-06T20:37:35.255+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17910555","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this, thakns:)","body_raw":"I'm working on this, thakns:)","created":"2025-01-07T09:02:13.174+0000","updated":"2025-01-07T09:02:13.174+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17910987","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","body_raw":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d\r\n\r\n4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","created":"2025-01-08T09:16:58.441+0000","updated":"2025-01-08T09:16:58.441+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove controlled.shutdown.max.retries and controlled.shutdown.retry.backoff.ms\n\n","output":"","metadata":{"issue_id":"KAFKA-18417","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T19:24:45.000+0000","updated":"2025-01-08T09:16:58.000+0000","resolved":"2025-01-08T09:16:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910392","author":{"displayName":"Ismael Juma"},"body":"We should also mention this in the zk to kraft migration guide.","body_raw":"We should also mention this in the zk to kraft migration guide.","created":"2025-01-06T20:37:35.255+0000","updated":"2025-01-06T20:37:35.255+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17910555","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this, thakns:)","body_raw":"I'm working on this, thakns:)","created":"2025-01-07T09:02:13.174+0000","updated":"2025-01-07T09:02:13.174+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17910987","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","body_raw":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d\r\n\r\n4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","created":"2025-01-08T09:16:58.441+0000","updated":"2025-01-08T09:16:58.441+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove controlled.shutdown.max.retries and controlled.shutdown.retry.backoff.ms\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18417","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T19:24:45.000+0000","updated":"2025-01-08T09:16:58.000+0000","resolved":"2025-01-08T09:16:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910392","author":{"displayName":"Ismael Juma"},"body":"We should also mention this in the zk to kraft migration guide.","body_raw":"We should also mention this in the zk to kraft migration guide.","created":"2025-01-06T20:37:35.255+0000","updated":"2025-01-06T20:37:35.255+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17910555","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this, thakns:)","body_raw":"I'm working on this, thakns:)","created":"2025-01-07T09:02:13.174+0000","updated":"2025-01-07T09:02:13.174+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17910987","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","body_raw":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d\r\n\r\n4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","created":"2025-01-08T09:16:58.441+0000","updated":"2025-01-08T09:16:58.441+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove controlled.shutdown.max.retries and controlled.shutdown.retry.backoff.ms\n\n\n\nConversation:\nuser: We should also mention this in the zk to kraft migration guide.\nassistant: I'm working on this, thakns:)\nassistant: trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","output":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","metadata":{"issue_id":"KAFKA-18417","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T19:24:45.000+0000","updated":"2025-01-08T09:16:58.000+0000","resolved":"2025-01-08T09:16:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910392","author":{"displayName":"Ismael Juma"},"body":"We should also mention this in the zk to kraft migration guide.","body_raw":"We should also mention this in the zk to kraft migration guide.","created":"2025-01-06T20:37:35.255+0000","updated":"2025-01-06T20:37:35.255+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17910555","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this, thakns:)","body_raw":"I'm working on this, thakns:)","created":"2025-01-07T09:02:13.174+0000","updated":"2025-01-07T09:02:13.174+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17910987","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d 4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","body_raw":"trunk: https://github.com/apache/kafka/commit/f95726a211b795f1783b2a2979e631a396283f3d\r\n\r\n4.0: https://github.com/apache/kafka/commit/8031cc2caae696d07a618ead2f088cbfc307b30e","created":"2025-01-08T09:16:58.441+0000","updated":"2025-01-08T09:16:58.441+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Ensure we capture all flaky/failing tests in report\n\nI noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a section for failing/flaky tests that are not recent. For example, looking at [https://ge.apache.org/scans/tests?search.names=Git%20Repository&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=America%2FNew_York&search.values=https:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.sortField=FLAKY#] we have org.apache.kafka.clients.consumer.internals.ApplicationEventHandlerTest which is quite flaky but is missing from the report.","output":"Ensure we capture all flaky/failing tests in report","metadata":{"issue_id":"KAFKA-18416","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2025-01-06T15:59:19.000+0000","updated":"2025-01-06T15:59:19.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Ensure we capture all flaky/failing tests in report\n\nI noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a section for failing/flaky tests that are not recent. For example, looking at [https://ge.apache.org/scans/tests?search.names=Git%20Repository&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&sear","output":"Open","metadata":{"issue_id":"KAFKA-18416","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2025-01-06T15:59:19.000+0000","updated":"2025-01-06T15:59:19.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Ensure we capture all flaky/failing tests in report\n\nI noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a section for failing/flaky tests that are not recent. For example, looking at [https://ge.apache.org/scans/tests?search.names=Git%20Repository&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&sear","output":"Major","metadata":{"issue_id":"KAFKA-18416","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2025-01-06T15:59:19.000+0000","updated":"2025-01-06T15:59:19.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Ensure we capture all flaky/failing tests in report\n\nI noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a section for failing/flaky tests that are not recent. For example, looking at [https://ge.apache.org/scans/tests?search.names=Git%20Repository&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&sear","output":"I noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a sec","metadata":{"issue_id":"KAFKA-18416","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2025-01-06T15:59:19.000+0000","updated":"2025-01-06T15:59:19.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I noticed in our Develocity report that we are missing a catch-all section for failing/flaky tests. We currently have these sections: * Quarantined tests which are continuing to fail * Tests that recently began failing (regressions) * Quarantined tests which have started to pass We are missing a section for failing/flaky tests that are not recent. For example, looking at [https://ge.apache.org/scans/tests?search.names=Git%20Repository&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=America%2FNew_York&search.values=https:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.sortField=FLAKY#] we have org.apache.kafka.clients.consumer.internals.ApplicationEventHandlerTest which is quite flaky but is missing from the report.","output":"Ensure we capture all flaky/failing tests in report","metadata":{"issue_id":"KAFKA-18416","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2025-01-06T15:59:19.000+0000","updated":"2025-01-06T15:59:19.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize\n\nFails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize\n\nFails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Resolved","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize\n\nFails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Major","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize\n\nFails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Fails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size w","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize\n\nFails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Fixed","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Fails with org.opentest4j.AssertionFailedError: expected: but was: [https://github.com/apache/kafka/actions/runs/12600961241/job/35121534600?pr=17099] Looks like race condition. I guess this could easily happen if the background thread removes the event from the queue (record metric for queue size with 0) before the app thread records the queue size with 1 (which happens after adding the event to the queue).","output":"Flaky ApplicationEventHandlerTest testRecordApplicationEventQueueSize","metadata":{"issue_id":"KAFKA-18415","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"Lianet Magrans"},"reporter":{"displayName":"Lianet Magrans"},"created":"2025-01-06T15:52:29.000+0000","updated":"2025-01-08T16:16:59.000+0000","resolved":"2025-01-08T13:42:55.000+0000","resolution":"Fixed","labels":["consumer-threading-refactor"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.","output":"Remove KRaftRegistrationResult","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.","output":"Resolved","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.","output":"Major","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.","output":"This trait is actually unused since we are removing ZK code.","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.","output":"Fixed","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove KRaftRegistrationResult\n\nThis trait is actually unused since we are removing ZK code.\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","output":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","metadata":{"issue_id":"KAFKA-18414","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T15:17:58.000+0000","updated":"2025-01-07T20:14:05.000+0000","resolved":"2025-01-07T20:14:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910794","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7 4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","body_raw":"trunk: https://github.com/apache/kafka/commit/2a073a14d2d9f87ebb16abedf06e397ff0e46cc7\r\n\r\n4.0: https://github.com/apache/kafka/commit/51b87c7a3437753cf0524163e6f41285f9b554e7","created":"2025-01-07T20:14:05.786+0000","updated":"2025-01-07T20:14:05.786+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove AdminZkClient\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18413","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T14:54:15.000+0000","updated":"2025-01-17T14:17:16.000+0000","resolved":"2025-01-17T14:17:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910597","author":{"displayName":"黃竣陽"},"body":"Should also remove TopicAlreadyMarkedForDeletionException","body_raw":"Should also remove TopicAlreadyMarkedForDeletionException","created":"2025-01-07T12:11:58.923+0000","updated":"2025-01-07T12:11:58.923+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910606","author":{"displayName":"TengYao Chi"},"body":"You're right, I will add it to this ticket.","body_raw":"You're right, I will add it to this ticket.","created":"2025-01-07T12:29:16.745+0000","updated":"2025-01-07T12:29:16.745+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913920","author":{"displayName":"TengYao Chi"},"body":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","body_raw":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","created":"2025-01-17T02:30:09.742+0000","updated":"2025-01-17T02:30:09.742+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove AdminZkClient\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18413","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T14:54:15.000+0000","updated":"2025-01-17T14:17:16.000+0000","resolved":"2025-01-17T14:17:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910597","author":{"displayName":"黃竣陽"},"body":"Should also remove TopicAlreadyMarkedForDeletionException","body_raw":"Should also remove TopicAlreadyMarkedForDeletionException","created":"2025-01-07T12:11:58.923+0000","updated":"2025-01-07T12:11:58.923+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910606","author":{"displayName":"TengYao Chi"},"body":"You're right, I will add it to this ticket.","body_raw":"You're right, I will add it to this ticket.","created":"2025-01-07T12:29:16.745+0000","updated":"2025-01-07T12:29:16.745+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913920","author":{"displayName":"TengYao Chi"},"body":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","body_raw":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","created":"2025-01-17T02:30:09.742+0000","updated":"2025-01-17T02:30:09.742+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove AdminZkClient\n\n","output":"","metadata":{"issue_id":"KAFKA-18413","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T14:54:15.000+0000","updated":"2025-01-17T14:17:16.000+0000","resolved":"2025-01-17T14:17:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910597","author":{"displayName":"黃竣陽"},"body":"Should also remove TopicAlreadyMarkedForDeletionException","body_raw":"Should also remove TopicAlreadyMarkedForDeletionException","created":"2025-01-07T12:11:58.923+0000","updated":"2025-01-07T12:11:58.923+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910606","author":{"displayName":"TengYao Chi"},"body":"You're right, I will add it to this ticket.","body_raw":"You're right, I will add it to this ticket.","created":"2025-01-07T12:29:16.745+0000","updated":"2025-01-07T12:29:16.745+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913920","author":{"displayName":"TengYao Chi"},"body":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","body_raw":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","created":"2025-01-17T02:30:09.742+0000","updated":"2025-01-17T02:30:09.742+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove AdminZkClient\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18413","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T14:54:15.000+0000","updated":"2025-01-17T14:17:16.000+0000","resolved":"2025-01-17T14:17:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910597","author":{"displayName":"黃竣陽"},"body":"Should also remove TopicAlreadyMarkedForDeletionException","body_raw":"Should also remove TopicAlreadyMarkedForDeletionException","created":"2025-01-07T12:11:58.923+0000","updated":"2025-01-07T12:11:58.923+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910606","author":{"displayName":"TengYao Chi"},"body":"You're right, I will add it to this ticket.","body_raw":"You're right, I will add it to this ticket.","created":"2025-01-07T12:29:16.745+0000","updated":"2025-01-07T12:29:16.745+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913920","author":{"displayName":"TengYao Chi"},"body":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","body_raw":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","created":"2025-01-17T02:30:09.742+0000","updated":"2025-01-17T02:30:09.742+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove AdminZkClient\n\n\n\nConversation:\nuser: Should also remove TopicAlreadyMarkedForDeletionException\nassistant: You're right, I will add it to this ticket.\nassistant: It looks like TopicAlreadyMarkedForDeletionException has been deleted.","output":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","metadata":{"issue_id":"KAFKA-18413","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-06T14:54:15.000+0000","updated":"2025-01-17T14:17:16.000+0000","resolved":"2025-01-17T14:17:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910597","author":{"displayName":"黃竣陽"},"body":"Should also remove TopicAlreadyMarkedForDeletionException","body_raw":"Should also remove TopicAlreadyMarkedForDeletionException","created":"2025-01-07T12:11:58.923+0000","updated":"2025-01-07T12:11:58.923+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910606","author":{"displayName":"TengYao Chi"},"body":"You're right, I will add it to this ticket.","body_raw":"You're right, I will add it to this ticket.","created":"2025-01-07T12:29:16.745+0000","updated":"2025-01-07T12:29:16.745+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17913920","author":{"displayName":"TengYao Chi"},"body":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","body_raw":"It looks like TopicAlreadyMarkedForDeletionException has been deleted.","created":"2025-01-17T02:30:09.742+0000","updated":"2025-01-17T02:30:09.742+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager","output":"Remove ZkProducerIdManager","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager","output":"Resolved","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager","output":"Major","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager","output":"as we remove KafkaServer, we also can remove ZkProducerIdManager","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager","output":"Fixed","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkProducerIdManager\n\nas we remove KafkaServer, we also can remove ZkProducerIdManager\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","output":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","metadata":{"issue_id":"KAFKA-18411","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T14:44:38.000+0000","updated":"2025-01-07T20:27:54.000+0000","resolved":"2025-01-07T20:27:54.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910797","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139 4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","body_raw":"trunk: https://github.com/apache/kafka/commit/6aef94e9ecc9cb3e56b7eacaa2ab4b94a3c8a139\r\n\r\n4.0: https://github.com/apache/kafka/commit/b61ca0f5cc3e6f99f12b92ef02a1aff373ff6b43","created":"2025-01-07T20:27:54.587+0000","updated":"2025-01-07T20:27:54.587+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Should GroupMetadataMessageFormatter print new records too?\n\nAt the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","output":"Should GroupMetadataMessageFormatter print new records too?","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Should GroupMetadataMessageFormatter print new records too?\n\nAt the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","output":"Open","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Should GroupMetadataMessageFormatter print new records too?\n\nAt the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","output":"Major","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Should GroupMetadataMessageFormatter print new records too?\n\nAt the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","output":"At the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Should GroupMetadataMessageFormatter print new records too?\n\nAt the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer,\n\nConversation:\nuser: Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].\nassistant: Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address thi\nassistant: chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so use","output":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so use","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"At the moment, GroupMetadataMessageFormatter only prints out the metadata record of the classic groups. It seems that we should extend it to also print out the metadata of other groups (e.g. consumer, share, stream, etc.). Thoughts?","output":"Should GroupMetadataMessageFormatter print new records too?","metadata":{"issue_id":"KAFKA-18410","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:52:08.000+0000","updated":"2025-01-06T15:21:45.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17910243","author":{"displayName":"PoAn Yang"},"body":"Hi dajac, it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","body_raw":"Hi [~dajac], it looks like the issue is related to KAFKA-18201. If you don't mind, I can handle this issue in my PR [https://github.com/apache/kafka/pull/18198].","created":"2025-01-06T14:33:47.843+0000","updated":"2025-01-06T14:33:47.843+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910250","author":{"displayName":"Chia-Ping Tsai"},"body":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality. 1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view. 2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.","body_raw":"Parsing metadata as comprehensively as possible is highly desirable. However, we must carefully consider how to extend the formatter functionality.\r\n\r\n1. Extending GroupMetadataMessageFormatter: While this approach offers flexibility, it might obscure specific metadata details for users. To address this, we could introduce a configuration mechanism to allow users to specify which metadata fields they wish to view.\r\n2. Creating dedicated formatters: This approach, exemplified by existing formatters like TransactionLogMessageFormatter and OffsetsMessageFormatter, provides granular control over metadata presentation. However, it may lead to an increase in the number of formatters, which could potentially become cumbersome to manage.\r\n\r\n","created":"2025-01-06T14:46:05.537+0000","updated":"2025-01-06T14:46:05.537+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910270","author":{"displayName":"PoAn Yang"},"body":"chia7712 thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","body_raw":"[~chia7712] thanks for the comment. I would prefer to extend GroupMetadataMessageFormatter. If we add one record type one formatter, it's hard for user to have an overview of current state with single command. We could extend GroupMetadataMessageFormatter with config like group id or group type, so user can use it to filter which group they want to see. The ConsoleConsumer also can set message formatter arguments, so this way is feasible.","created":"2025-01-06T15:21:45.660+0000","updated":"2025-01-06T15:21:45.660+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"ShareGroupStateMessageFormatter should use ApiMessageFormatter","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"Resolved","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"Major","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"ShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"Fixed","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"ShareGroupStateMessageFormatter should use ApiMessageFormatter\n\nShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.\n\nConversation:\nuser: Hi dajac , may I take over this issue? Thank you!\nassistant: brandboat Sure. Go for it.","output":"brandboat Sure. Go for it.","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"ShareGroupStateMessageFormatter should extend ApiMessageFormatter in order to have a consistent handling of records of coordinators.","output":"ShareGroupStateMessageFormatter should use ApiMessageFormatter","metadata":{"issue_id":"KAFKA-18409","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"David Jacot"},"created":"2025-01-06T13:51:02.000+0000","updated":"2025-03-31T14:45:19.000+0000","resolved":"2025-03-31T14:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910232","author":{"displayName":"Kuan Po Tseng"},"body":"Hi dajac , may I take over this issue? Thank you!","body_raw":"Hi [~dajac] , may I take over this issue? Thank you!","created":"2025-01-06T13:52:39.116+0000","updated":"2025-01-06T13:52:39.116+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910233","author":{"displayName":"David Jacot"},"body":"brandboat Sure. Go for it.","body_raw":"[~brandboat] Sure. Go for it.","created":"2025-01-06T13:54:22.137+0000","updated":"2025-01-06T13:54:22.137+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"tweak the 'tag' field for BrokerHeartbeatRequest.json, BrokerRegistrationChangeRecord.json and RegisterBrokerRecord.json\n\n\"tag\": \"0\" -> \"tag\": 0","output":"Resolved","metadata":{"issue_id":"KAFKA-18408","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T13:09:04.000+0000","updated":"2025-01-07T20:17:08.000+0000","resolved":"2025-01-07T20:17:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910219","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!\r\n\r\n ","created":"2025-01-06T13:12:17.076+0000","updated":"2025-01-06T13:12:17.076+0000","updateAuthor":{"displayName":"Mingdao Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"tweak the 'tag' field for BrokerHeartbeatRequest.json, BrokerRegistrationChangeRecord.json and RegisterBrokerRecord.json\n\n\"tag\": \"0\" -> \"tag\": 0","output":"Trivial","metadata":{"issue_id":"KAFKA-18408","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T13:09:04.000+0000","updated":"2025-01-07T20:17:08.000+0000","resolved":"2025-01-07T20:17:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910219","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!\r\n\r\n ","created":"2025-01-06T13:12:17.076+0000","updated":"2025-01-06T13:12:17.076+0000","updateAuthor":{"displayName":"Mingdao Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: tweak the 'tag' field for BrokerHeartbeatRequest.json, BrokerRegistrationChangeRecord.json and RegisterBrokerRecord.json\n\n\"tag\": \"0\" -> \"tag\": 0","output":"\"tag\": \"0\" -> \"tag\": 0","metadata":{"issue_id":"KAFKA-18408","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T13:09:04.000+0000","updated":"2025-01-07T20:17:08.000+0000","resolved":"2025-01-07T20:17:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910219","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!\r\n\r\n ","created":"2025-01-06T13:12:17.076+0000","updated":"2025-01-06T13:12:17.076+0000","updateAuthor":{"displayName":"Mingdao Yang"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"tweak the 'tag' field for BrokerHeartbeatRequest.json, BrokerRegistrationChangeRecord.json and RegisterBrokerRecord.json\n\n\"tag\": \"0\" -> \"tag\": 0","output":"Fixed","metadata":{"issue_id":"KAFKA-18408","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T13:09:04.000+0000","updated":"2025-01-07T20:17:08.000+0000","resolved":"2025-01-07T20:17:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910219","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!\r\n\r\n ","created":"2025-01-06T13:12:17.076+0000","updated":"2025-01-06T13:12:17.076+0000","updateAuthor":{"displayName":"Mingdao Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"tweak the 'tag' field for BrokerHeartbeatRequest.json, BrokerRegistrationChangeRecord.json and RegisterBrokerRecord.json\n\n\"tag\": \"0\" -> \"tag\": 0\n\nConversation:\nuser: Hi, I’d like to work on it. Thank you!","output":"Hi, I’d like to work on it. Thank you!","metadata":{"issue_id":"KAFKA-18408","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-06T13:09:04.000+0000","updated":"2025-01-07T20:17:08.000+0000","resolved":"2025-01-07T20:17:08.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910219","author":{"displayName":"Mingdao Yang"},"body":"Hi, I’d like to work on it. Thank you!","body_raw":"Hi, I’d like to work on it. Thank you!\r\n\r\n ","created":"2025-01-06T13:12:17.076+0000","updated":"2025-01-06T13:12:17.076+0000","updateAuthor":{"displayName":"Mingdao Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Resolved","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Major","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Once we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Fixed","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Once we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository","output":"Remove ZkAdminManager, DelayedCreatePartitions, CreatePartitionsMetadata, ZkConfigRepository, DelayedDeleteTopics","metadata":{"issue_id":"KAFKA-18407","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:55:57.000+0000","updated":"2025-01-20T13:01:55.000+0000","resolved":"2025-01-16T18:37:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Remove ZkBrokerEpochManager","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Resolved","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Major","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Once we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Fixed","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkBrokerEpochManager\n\nOnce we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","output":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Once we delete ZkSupport https://issues.apache.org/jira/browse/KAFKA-18399 , we will be able to remove ZkBrokerEpochManager","output":"Remove ZkBrokerEpochManager","metadata":{"issue_id":"KAFKA-18406","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-06T12:47:02.000+0000","updated":"2025-01-16T11:39:38.000+0000","resolved":"2025-01-16T11:39:38.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e] 4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","body_raw":"trunk: [https://github.com/apache/kafka/commit/762bbcb711d5e477b656c88183588b32c7dedd0e]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/f333f9f340f5633cd59f7346b8da7bd77eca0cce","created":"2025-01-16T11:39:38.968+0000","updated":"2025-01-16T11:39:38.968+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZooKeeper logic from DynamicBrokerConfig\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18405","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-06T10:18:51.000+0000","updated":"2025-01-16T11:28:29.000+0000","resolved":"2025-01-16T11:28:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910110","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, I missed you are the first assignee.","body_raw":"Sorry for [~yangpoan], I missed you are the first assignee.","created":"2025-01-06T10:22:40.667+0000","updated":"2025-01-06T10:22:40.667+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910112","author":{"displayName":"Mickael Maison"},"body":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]","body_raw":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See:\r\n- [https://github.com/apache/kafka/pull/18384#discussion_r1902251279|https://github.com/apache/kafka/pull/18384#discussion_r1902251279]\r\n- [https://issues.apache.org/jira/browse/KAFKA-18403]","created":"2025-01-06T10:23:35.557+0000","updated":"2025-01-06T10:23:35.557+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17910114","author":{"displayName":"PoAn Yang"},"body":"Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","body_raw":"Hi [~mimaison], thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","created":"2025-01-06T10:26:22.187+0000","updated":"2025-01-06T10:26:22.187+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913685","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","body_raw":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","created":"2025-01-16T11:28:29.604+0000","updated":"2025-01-16T11:28:29.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZooKeeper logic from DynamicBrokerConfig\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18405","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-06T10:18:51.000+0000","updated":"2025-01-16T11:28:29.000+0000","resolved":"2025-01-16T11:28:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910110","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, I missed you are the first assignee.","body_raw":"Sorry for [~yangpoan], I missed you are the first assignee.","created":"2025-01-06T10:22:40.667+0000","updated":"2025-01-06T10:22:40.667+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910112","author":{"displayName":"Mickael Maison"},"body":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]","body_raw":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See:\r\n- [https://github.com/apache/kafka/pull/18384#discussion_r1902251279|https://github.com/apache/kafka/pull/18384#discussion_r1902251279]\r\n- [https://issues.apache.org/jira/browse/KAFKA-18403]","created":"2025-01-06T10:23:35.557+0000","updated":"2025-01-06T10:23:35.557+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17910114","author":{"displayName":"PoAn Yang"},"body":"Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","body_raw":"Hi [~mimaison], thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","created":"2025-01-06T10:26:22.187+0000","updated":"2025-01-06T10:26:22.187+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913685","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","body_raw":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","created":"2025-01-16T11:28:29.604+0000","updated":"2025-01-16T11:28:29.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZooKeeper logic from DynamicBrokerConfig\n\n","output":"","metadata":{"issue_id":"KAFKA-18405","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-06T10:18:51.000+0000","updated":"2025-01-16T11:28:29.000+0000","resolved":"2025-01-16T11:28:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910110","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, I missed you are the first assignee.","body_raw":"Sorry for [~yangpoan], I missed you are the first assignee.","created":"2025-01-06T10:22:40.667+0000","updated":"2025-01-06T10:22:40.667+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910112","author":{"displayName":"Mickael Maison"},"body":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]","body_raw":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See:\r\n- [https://github.com/apache/kafka/pull/18384#discussion_r1902251279|https://github.com/apache/kafka/pull/18384#discussion_r1902251279]\r\n- [https://issues.apache.org/jira/browse/KAFKA-18403]","created":"2025-01-06T10:23:35.557+0000","updated":"2025-01-06T10:23:35.557+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17910114","author":{"displayName":"PoAn Yang"},"body":"Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","body_raw":"Hi [~mimaison], thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","created":"2025-01-06T10:26:22.187+0000","updated":"2025-01-06T10:26:22.187+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913685","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","body_raw":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","created":"2025-01-16T11:28:29.604+0000","updated":"2025-01-16T11:28:29.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZooKeeper logic from DynamicBrokerConfig\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18405","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-06T10:18:51.000+0000","updated":"2025-01-16T11:28:29.000+0000","resolved":"2025-01-16T11:28:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910110","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, I missed you are the first assignee.","body_raw":"Sorry for [~yangpoan], I missed you are the first assignee.","created":"2025-01-06T10:22:40.667+0000","updated":"2025-01-06T10:22:40.667+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910112","author":{"displayName":"Mickael Maison"},"body":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]","body_raw":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See:\r\n- [https://github.com/apache/kafka/pull/18384#discussion_r1902251279|https://github.com/apache/kafka/pull/18384#discussion_r1902251279]\r\n- [https://issues.apache.org/jira/browse/KAFKA-18403]","created":"2025-01-06T10:23:35.557+0000","updated":"2025-01-06T10:23:35.557+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17910114","author":{"displayName":"PoAn Yang"},"body":"Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","body_raw":"Hi [~mimaison], thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","created":"2025-01-06T10:26:22.187+0000","updated":"2025-01-06T10:26:22.187+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913685","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","body_raw":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","created":"2025-01-16T11:28:29.604+0000","updated":"2025-01-16T11:28:29.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZooKeeper logic from DynamicBrokerConfig\n\n\n\nConversation:\nuser: Sorry for yangpoan, I missed you are the first assignee.\nassistant: We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]\nassistant: Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.\nassistant: trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","output":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","metadata":{"issue_id":"KAFKA-18405","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-06T10:18:51.000+0000","updated":"2025-01-16T11:28:29.000+0000","resolved":"2025-01-16T11:28:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17910110","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, I missed you are the first assignee.","body_raw":"Sorry for [~yangpoan], I missed you are the first assignee.","created":"2025-01-06T10:22:40.667+0000","updated":"2025-01-06T10:22:40.667+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17910112","author":{"displayName":"Mickael Maison"},"body":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See: - https://github.com/apache/kafka/pull/18384#discussion_r1902251279 - [https://issues.apache.org/jira/browse/KAFKA-18403]","body_raw":"We need to consider what to do in DynamicListenerConfig.reconfigure(). See:\r\n- [https://github.com/apache/kafka/pull/18384#discussion_r1902251279|https://github.com/apache/kafka/pull/18384#discussion_r1902251279]\r\n- [https://issues.apache.org/jira/browse/KAFKA-18403]","created":"2025-01-06T10:23:35.557+0000","updated":"2025-01-06T10:23:35.557+0000","updateAuthor":{"displayName":"Mickael Maison"}},{"id":"17910114","author":{"displayName":"PoAn Yang"},"body":"Hi mimaison, thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","body_raw":"Hi [~mimaison], thanks for the information. I will do some investigation and create a PR after [https://github.com/apache/kafka/pull/18384] is merged.","created":"2025-01-06T10:26:22.187+0000","updated":"2025-01-06T10:26:22.187+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913685","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238] 4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","body_raw":"trunk: [https://github.com/apache/kafka/commit/25f2ed090a7fc815225e2161bff964488f101238]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/6df29771ba7929d8c92e004c78fc310217a153de","created":"2025-01-16T11:28:29.604+0000","updated":"2025-01-16T11:28:29.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Check all dynamic properites, which can't support in KRaft\n\n`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"Check all dynamic properites, which can't support in KRaft","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Check all dynamic properites, which can't support in KRaft\n\n`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"Resolved","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Check all dynamic properites, which can't support in KRaft\n\n`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"Major","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Check all dynamic properites, which can't support in KRaft\n\n`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Check all dynamic properites, which can't support in KRaft\n\n`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"Duplicate","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"`advertised.listeners` this property should not dynamic change in Kraft mode, we should check other properties, and remove these properties from Dynamic map","output":"Check all dynamic properites, which can't support in KRaft","metadata":{"issue_id":"KAFKA-18403","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2025-01-05T09:18:20.000+0000","updated":"2025-01-18T00:46:47.000+0000","resolved":"2025-01-18T00:46:47.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Optimise of comparison on org.apache.kafka.common.Uuid\n\nThe the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.compare(this.mostSigBits, val.mostSigBits); return mostSigBits != 0 ? mostSigBits : Long.compare(this.leastSigBits, val.leastSigBits); } {code}","output":"Optimise of comparison on org.apache.kafka.common.Uuid","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Optimise of comparison on org.apache.kafka.common.Uuid\n\nThe the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.compare(this.mostSigBits, val.mostSigBits); return mostSigBits != 0 ? mostSigBits : Long.compare(this.leastSigBits, val.leastSigBits); } {code}","output":"In Progress","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Optimise of comparison on org.apache.kafka.common.Uuid\n\nThe the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.compare(this.mostSigBits, val.mostSigBits); return mostSigBits != 0 ? mostSigBits : Long.compare(this.leastSigBits, val.leastSigBits); } {code}","output":"Minor","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Optimise of comparison on org.apache.kafka.common.Uuid\n\nThe the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.compare(this.mostSigBits, val.mostSigBits); return mostSigBits != 0 ? mostSigBits : Long.compare(this.leastSigBits, val.leastSigBits); } {code}","output":"The the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Optimise of comparison on org.apache.kafka.common.Uuid\n\nThe the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally s\n\nConversation:\nuser: Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\nassistant: Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK\nassistant: Moving to the next release as we're now in code freeze for 4.1.0.","output":"Moving to the next release as we're now in code freeze for 4.1.0.","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The the reference implementation for org.apache.kafka.common.Uuid compareTo public method as of java.util.UUID {code:java} @Override public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers int mostSigBits = Long.compare(this.mostSigBits, val.mostSigBits); return mostSigBits != 0 ? mostSigBits : Long.compare(this.leastSigBits, val.leastSigBits); } {code}","output":"Optimise of comparison on org.apache.kafka.common.Uuid","metadata":{"issue_id":"KAFKA-18402","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"In Progress","priority":"Minor","assignee":{"displayName":"Said BOUDJELDA"},"reporter":{"displayName":"Said BOUDJELDA"},"created":"2025-01-04T18:54:35.000+0000","updated":"2025-08-06T03:56:43.000+0000","resolved":null,"labels":["common"],"components":["clients"],"comment_count":3,"comments":[{"id":"17909786","author":{"displayName":"Said BOUDJELDA"},"body":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627","body_raw":"Here is a pull request for this JIRA https://github.com/apache/kafka/pull/13627\r\n ","created":"2025-01-04T18:58:02.840+0000","updated":"2025-01-04T18:58:02.840+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17909963","author":{"displayName":"Said BOUDJELDA"},"body":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK","body_raw":"Regarding the licensing and the contribution guides it's more wise to provide own optimisation for compareTo method on Uuid class, rather then getting inspired by the one of OpenJDK \r\n","created":"2025-01-05T23:57:51.932+0000","updated":"2025-01-05T23:57:51.932+0000","updateAuthor":{"displayName":"Said BOUDJELDA"}},{"id":"17986050","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:22:17.727+0000","updated":"2025-06-25T09:22:17.727+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnRequest to the broker while in transaction coordinator, the txn state is still in EMPTY, resulting in an error from the broker. This issue can be reproduced with the test in below. I'm unsure if this behavior is expected. If it's not, one potential fix could be to follow the approach used in TV_0 and TV_1, where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {code:java} @ClusterTests({ @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}), @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}), @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)}) }) public void testProducerEndTransaction2(ClusterInstance cluster) throws InterruptedException { Map properties = new HashMap<>(); properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"foobar\"); properties.put(ProducerConfig.CLIENT_ID_CONFIG, \"test\"); properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); try (Producer producer1 = cluster.producer(properties)) { producer1.initTransactions(); producer1.beginTransaction(); producer1.commitTransaction(); // In TV_2, we'll get InvalidTxnStateException } } {code} Another test case, which is essentially the same as the previous one, starts with a transaction that includes records, and then proceeds to start the next transaction. When using transaction version 2, we encounter an error, but this time it's a different error from the one seen in the previous case. {code:java} @ClusterTests({ @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}), @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}), @ClusterTest(brokers = 3, features = { @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)}) }) public void testProducerEndTransaction(ClusterInstance cluster) { Map properties = new HashMap<>(); properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"foobar\"); properties.put(ProducerConfig.CLIENT_ID_CONFIG, \"test\"); properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); try (Producer producer1 = cluster.producer(properties)) { producer1.initTransactions(); producer1.beginTransaction(); producer1.send(new ProducerRecord<>(\"test\", \"key\".getBytes(), \"value\".getBytes())); producer1.commitTransaction(); producer1.beginTransaction(); producer1.commitTransaction(); // In TV_2, we'll get ProducerFencedException } } {code} [0]: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L857-L865]","output":"Transaction version 2 does not support commit transaction without records","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnReque","output":"Resolved","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnReque","output":"Blocker","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnReque","output":"This issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending ","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnReque","output":"Fixed","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Transaction version 2 does not support commit transaction without records\n\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records wh\n\nConversation:\nuser: Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.\nassistant: {quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added \nassistant: {quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed user\nassistant: I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.\nassistant: I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented s","output":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206. In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario. Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnRequest to the broker while in transaction coordinator, the txn state is still in EMPTY, resulting in an error from the broker. This issue can be reproduced with the test in below. I'm unsure if this behavior is expected. If it's not, one potential fix could be to follow the approach used in TV_0 and TV_","output":"Transaction version 2 does not support commit transaction without records","metadata":{"issue_id":"KAFKA-18401","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2025-01-04T10:10:09.000+0000","updated":"2025-01-17T18:31:35.000+0000","resolved":"2025-01-15T18:44:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":10,"comments":[{"id":"17910742","author":{"displayName":"Justine Olshan"},"body":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","body_raw":"Hmm. This should not be an issue as we should have fixed the state machine. I can take a closer look.","created":"2025-01-07T18:12:31.867+0000","updated":"2025-01-07T18:12:31.867+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910745","author":{"displayName":"Justine Olshan"},"body":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change. {quote} We can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures) I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content.","body_raw":"{quote}where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\r\n{quote}\r\n\r\nWe can not do this because we don't know for certain that the transaction actually added the partitions on the coordinator side due to the 2 hop process. We should always be able to send an EndTransaction abort regardless of whether partitions were added or not (in the case of failures)\r\n\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. ","created":"2025-01-07T18:17:31.576+0000","updated":"2025-01-07T18:17:54.272+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910766","author":{"displayName":"Chia-Ping Tsai"},"body":"{quote} I guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. {quote} I believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code: {code:java} producer.beginTransaction(); sendRecords(producer); producer.commitTransaction(); {code} it seems users have to check the count of sent records. for example: {code:java} producer.beginTransaction(); var count = sendRecords(producer); if (count > 0) producer.commitTransaction(); {code} However, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction {code:java} var needBegin = true; if (needBegin) { producer.beginTransaction(); needBegin = true; } var count = sendRecords(producer); if (count > 0) { producer.commitTransaction(); needBegin = true; } {code} Please correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.","body_raw":"{quote}\r\nI guess under this we don't allow an EndTxn commit because we can't say for certain if a partition has been added. I'm trying to understand the use case for committing a transaction with no content. \r\n{quote}\r\n\r\nI believe the core issue stems from a change in behavior that previously allowed users to call commitTransaction without having sent any records. If this behavioral change is indeed necessary, the documentation for commitTransaction should be updated to explicitly reflect this modification. Furthermore, I'm currently considering how to refactor the following code:\r\n\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nsendRecords(producer);\r\nproducer.commitTransaction();\r\n{code}\r\n\r\n\r\nit seems users have to check the count of sent records. for example:\r\n\r\n{code:java}\r\nproducer.beginTransaction();\r\nvar count = sendRecords(producer);\r\nif (count > 0) producer.commitTransaction();\r\n{code}\r\n\r\nHowever, I think `beginTransaction` can't be executed repeatedly, so we need extra check for beginTransaction\r\n\r\n{code:java}\r\nvar needBegin = true;\r\n\r\nif (needBegin) {\r\n  producer.beginTransaction();\r\n  needBegin = true;\r\n}\r\nvar count = sendRecords(producer);\r\nif (count > 0) {\r\n  producer.commitTransaction();\r\n  needBegin = true;\r\n}\r\n{code}\r\n\r\nPlease correct me if I have misunderstood anything. If the above example is valid, we should also document this behavior for users.\r\n","created":"2025-01-07T19:01:12.050+0000","updated":"2025-01-07T19:01:12.050+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910771","author":{"displayName":"Justine Olshan"},"body":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it.","body_raw":"I'm interested in the method sendRecords that sometimes doesn't send records. It appears in streams for example, we don't start a transaction until we have data to send. So I'm wondering if there is a case where we need to start a transaction without having any records/offsets to contribute to it. ","created":"2025-01-07T19:10:33.282+0000","updated":"2025-01-07T19:10:33.282+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910773","author":{"displayName":"Justine Olshan"},"body":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","body_raw":"I do understand the concern about a regression in the sense that this was previously supported and now it is not. So I will think on how that can be improved. One option is to allow commits if we never attempted a send on the producer. The only question for me on that is how it will be implemented server side.","created":"2025-01-07T19:11:50.932+0000","updated":"2025-01-07T19:11:50.932+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910858","author":{"displayName":"Justine Olshan"},"body":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","body_raw":"Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.","created":"2025-01-08T00:33:35.905+0000","updated":"2025-01-08T00:33:35.905+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910859","author":{"displayName":"Justine Olshan"},"body":"brandboat I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","body_raw":"[~brandboat] I see you assigned yourself here. Do you think you can implement the above? I can also take it. Just let me know in the next day or so as we are approaching the code freeze. Thanks!","created":"2025-01-08T00:34:52.612+0000","updated":"2025-01-08T00:34:52.612+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910864","author":{"displayName":"Kuan Po Tseng"},"body":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty. Thanks, jolshan. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","body_raw":"bq. Discussed this with a teammate and we have a way for this to work without changing the server. In the case where we call commitTransaction and the producer never called send, we can return successfully without sending the EndTxn request to the server. This should require minimal changes and allow for the backwards compatibilty.\r\n\r\nThanks, [~jolshan]. This is exactly what I'm trying to implement. Over the past few days, I’ve been struggling with whether \"calling commitTransaction without calling send\" is the right way to address this issue. There’s a corner case where if the acknowledgment (ack) is 0, the send might fail, but the producer wouldn’t be able to detect that failure. I'm not sure how to handle this case... Alternatively, perhaps we could ignore this side effect and simply detect whether {{send}} was called within a transaction and do not check if response is ok or not and then check if we can call {{{}commitTransaction{}}}?","created":"2025-01-08T00:50:15.401+0000","updated":"2025-01-08T00:50:15.401+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17910924","author":{"displayName":"Justine Olshan"},"body":"Hi brandboat We can't use transactions without acks=all. Does that clear up some confusion? I do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.","body_raw":"Hi [~brandboat] \r\nWe can't use transactions without acks=all. Does that clear up some confusion?\r\n\r\nI do agree though that it would be best to only return a success on commitTransaction and not send the EndTxn request if the producer never called send during the transaction. If it called send at least once, we are left in a weird state where we don't know if the transaction was added or not in some cases when send fails. In that case, we shouldn't try to commit and should fail if the producer attempts to.\r\n\r\n ","created":"2025-01-08T04:36:26.919+0000","updated":"2025-01-08T04:39:25.573+0000","updateAuthor":{"displayName":"Justine Olshan"}},{"id":"17910925","author":{"displayName":"Kuan Po Tseng"},"body":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","body_raw":"Yes... I just realized that, thanks for the explanation. I'll file a PR ASAP.","created":"2025-01-08T04:37:58.827+0000","updated":"2025-01-08T04:37:58.827+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Don't use YYYY when formatting/parsing dates in Java client\n\nSee this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"Don't use YYYY when formatting/parsing dates in Java client","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Don't use YYYY when formatting/parsing dates in Java client\n\nSee this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"Resolved","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Don't use YYYY when formatting/parsing dates in Java client\n\nSee this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"Major","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Don't use YYYY when formatting/parsing dates in Java client\n\nSee this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"See this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Don't use YYYY when formatting/parsing dates in Java client\n\nSee this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"Fixed","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"See this blog post for background information: [https://medium.com/@dangoldin/javas-simpledateformat-yyyy-vs-yyyy-f3e785523589] Look at: * ConsumerGroupCommandOptions * ShareGroupCommandOptions * ClientUtils","output":"Don't use YYYY when formatting/parsing dates in Java client","metadata":{"issue_id":"KAFKA-18400","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Kirk True"},"created":"2025-01-03T19:49:11.000+0000","updated":"2025-01-11T18:30:17.000+0000","resolved":"2025-01-11T18:30:17.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Remove ZooKeeper from KafkaApis","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Resolved","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Major","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Once we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Fixed","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZooKeeper from KafkaApis\n\nOnce we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.\n\nConversation:\nuser: We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.\nassistant: all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","output":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Once we delete KafkaServer (KAFKA-17616), we will be able to strip all the ZooKeeper references from KafkaApis.","output":"Remove ZooKeeper from KafkaApis","metadata":{"issue_id":"KAFKA-18399","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Mickael Maison"},"created":"2025-01-03T17:48:21.000+0000","updated":"2025-01-20T08:06:09.000+0000","resolved":"2025-01-15T15:37:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909689","author":{"displayName":"PoAn Yang"},"body":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","body_raw":"We can also remove ZkSupport in this Jira. It looks like it's only used in KafkaApis and KafkaServer.","created":"2025-01-04T00:31:27.756+0000","updated":"2025-01-04T00:31:27.756+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17913348","author":{"displayName":"Chia-Ping Tsai"},"body":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","body_raw":"all related PRs are merged to trunk and 4.0 - there are some follow-ups which have separate jiras","created":"2025-01-15T15:37:04.605+0000","updated":"2025-01-15T15:37:04.605+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Log a warning if actual topic configs are inconsistent with the required topic configs\n\n","output":"Open","metadata":{"issue_id":"KAFKA-18398","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2025-01-03T12:28:13.000+0000","updated":"2025-01-14T18:06:34.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":4,"comments":[{"id":"17909659","author":{"displayName":"Matthias J. Sax"},"body":"I think we need to be careful with this ticket – cf the one I just linked. In general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","body_raw":"I think we need to be careful with this ticket – cf the one I just linked.\r\n\r\nIn general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","created":"2025-01-03T19:35:48.344+0000","updated":"2025-01-03T19:35:48.344+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910033","author":{"displayName":"Lucas Brutschy"},"body":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that. mjsax Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","body_raw":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that.\r\n\r\n[~mjsax] Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","created":"2025-01-06T08:02:18.268+0000","updated":"2025-01-06T08:02:18.268+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17910328","author":{"displayName":"Matthias J. Sax"},"body":"I think there is two things: * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)? * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?) I am open to other suggestions of course. The main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","body_raw":"I think there is two things:\r\n * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)?\r\n * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?)\r\n\r\nI am open to other suggestions of course.\r\n\r\nThe main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","created":"2025-01-06T17:23:35.970+0000","updated":"2025-01-06T17:23:35.970+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910552","author":{"displayName":"Lucas Brutschy"},"body":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once. So it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","body_raw":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once.\r\n\r\nSo it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","created":"2025-01-07T08:43:02.609+0000","updated":"2025-01-07T08:43:02.609+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Log a warning if actual topic configs are inconsistent with the required topic configs\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18398","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2025-01-03T12:28:13.000+0000","updated":"2025-01-14T18:06:34.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":4,"comments":[{"id":"17909659","author":{"displayName":"Matthias J. Sax"},"body":"I think we need to be careful with this ticket – cf the one I just linked. In general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","body_raw":"I think we need to be careful with this ticket – cf the one I just linked.\r\n\r\nIn general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","created":"2025-01-03T19:35:48.344+0000","updated":"2025-01-03T19:35:48.344+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910033","author":{"displayName":"Lucas Brutschy"},"body":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that. mjsax Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","body_raw":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that.\r\n\r\n[~mjsax] Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","created":"2025-01-06T08:02:18.268+0000","updated":"2025-01-06T08:02:18.268+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17910328","author":{"displayName":"Matthias J. Sax"},"body":"I think there is two things: * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)? * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?) I am open to other suggestions of course. The main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","body_raw":"I think there is two things:\r\n * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)?\r\n * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?)\r\n\r\nI am open to other suggestions of course.\r\n\r\nThe main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","created":"2025-01-06T17:23:35.970+0000","updated":"2025-01-06T17:23:35.970+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910552","author":{"displayName":"Lucas Brutschy"},"body":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once. So it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","body_raw":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once.\r\n\r\nSo it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","created":"2025-01-07T08:43:02.609+0000","updated":"2025-01-07T08:43:02.609+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Log a warning if actual topic configs are inconsistent with the required topic configs\n\n","output":"","metadata":{"issue_id":"KAFKA-18398","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2025-01-03T12:28:13.000+0000","updated":"2025-01-14T18:06:34.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":4,"comments":[{"id":"17909659","author":{"displayName":"Matthias J. Sax"},"body":"I think we need to be careful with this ticket – cf the one I just linked. In general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","body_raw":"I think we need to be careful with this ticket – cf the one I just linked.\r\n\r\nIn general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","created":"2025-01-03T19:35:48.344+0000","updated":"2025-01-03T19:35:48.344+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910033","author":{"displayName":"Lucas Brutschy"},"body":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that. mjsax Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","body_raw":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that.\r\n\r\n[~mjsax] Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","created":"2025-01-06T08:02:18.268+0000","updated":"2025-01-06T08:02:18.268+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17910328","author":{"displayName":"Matthias J. Sax"},"body":"I think there is two things: * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)? * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?) I am open to other suggestions of course. The main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","body_raw":"I think there is two things:\r\n * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)?\r\n * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?)\r\n\r\nI am open to other suggestions of course.\r\n\r\nThe main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","created":"2025-01-06T17:23:35.970+0000","updated":"2025-01-06T17:23:35.970+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910552","author":{"displayName":"Lucas Brutschy"},"body":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once. So it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","body_raw":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once.\r\n\r\nSo it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","created":"2025-01-07T08:43:02.609+0000","updated":"2025-01-07T08:43:02.609+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Log a warning if actual topic configs are inconsistent with the required topic configs\n\n\n\nConversation:\nuser: I think we need to be careful with this ticket – cf the one I just linked. In general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff..\nassistant: It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that. mjsax Do you suggest to not log a warning at all? Or do you think there is a subset of configs for\nassistant: I think there is two things: * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)? * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus\nassistant: Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would ","output":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would ","metadata":{"issue_id":"KAFKA-18398","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2025-01-03T12:28:13.000+0000","updated":"2025-01-14T18:06:34.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":4,"comments":[{"id":"17909659","author":{"displayName":"Matthias J. Sax"},"body":"I think we need to be careful with this ticket – cf the one I just linked. In general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","body_raw":"I think we need to be careful with this ticket – cf the one I just linked.\r\n\r\nIn general, we should really define what it absolutely necessary to check, and limit to these cases. Kafka Streams as always benefited to have good defaults, but not enforce them and give advanced users ways to change stuff... Logging warning would of course not break stuff and preserve flexibility, but can still be very annoying if unnecessary.","created":"2025-01-03T19:35:48.344+0000","updated":"2025-01-03T19:35:48.344+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910033","author":{"displayName":"Lucas Brutschy"},"body":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that. mjsax Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","body_raw":"It seems to me the linked ticket is all the more a reason to log a warning, right? If a certain internal topic is configured incorrectly because the topology was evolved, the user may want to fix that.\r\n\r\n[~mjsax] Do you suggest to not log a warning at all? Or do you think there is a subset of configs for which we shouldn't log warnings?","created":"2025-01-06T08:02:18.268+0000","updated":"2025-01-06T08:02:18.268+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17910328","author":{"displayName":"Matthias J. Sax"},"body":"I think there is two things: * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)? * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?) I am open to other suggestions of course. The main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","body_raw":"I think there is two things:\r\n * Is there any actually _invalid_ configs, which we should disallow? For this case, we should maybe log a WARN (or even fail)?\r\n * If there is diff between code and configs, but it's a totally valid config, we should not log but just apply the config to the topic, and thus address K7803. (Of course, the side effect will be, that if somebody changes a topic config \"in the background\", KS code would overwrite it – but I guess that is desired?)\r\n\r\nI am open to other suggestions of course.\r\n\r\nThe main point of linking both tickets was, that given the current setup, people might change topic config in the background w/o updating the code. Thus could lead to many spurious WARN logs if we start to compare code and topic config and log a WARN for everything we do not expect.","created":"2025-01-06T17:23:35.970+0000","updated":"2025-01-06T17:23:35.970+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17910552","author":{"displayName":"Lucas Brutschy"},"body":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once. So it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","body_raw":"Yes, we could make this slightly less annoying by distinguishing the invalid configs from non-default configs. Maybe it would still be good to output an \"INFO\" log for the non-default ones, as people may want to know about it. It also seems to be a \"good\" log message. It's perfectly clear and would only be emitted once.\r\n\r\nSo it seems to me the ticket is valid? This came out of thread where the wrong `cleanup.policy` for repartition topics caused problems. So we could restrict this ticket to log `cleanup.policy` with WARN and other configs as INFO, and then add more configs on demand?","created":"2025-01-07T08:43:02.609+0000","updated":"2025-01-07T08:43:02.609+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix the scenario where acknowledgement callback is being called on null acknowledgements\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18397","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Shivsundar R"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2025-01-03T07:48:46.000+0000","updated":"2025-01-09T13:21:57.000+0000","resolved":"2025-01-08T15:26:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910109","author":{"displayName":"Shivsundar R"},"body":"peterxcli , I have a fix for this, can I take this ticket up?","body_raw":"[~peterxcli] , I have a fix for this, can I take this ticket up?","created":"2025-01-06T10:19:57.922+0000","updated":"2025-01-06T10:19:57.922+0000","updateAuthor":{"displayName":"Shivsundar R"}},{"id":"17910141","author":{"displayName":"Chu Cheng Li"},"body":"Sure, thanks!","body_raw":"Sure, thanks!","created":"2025-01-06T10:59:19.566+0000","updated":"2025-01-06T10:59:19.566+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix the scenario where acknowledgement callback is being called on null acknowledgements\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18397","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Shivsundar R"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2025-01-03T07:48:46.000+0000","updated":"2025-01-09T13:21:57.000+0000","resolved":"2025-01-08T15:26:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910109","author":{"displayName":"Shivsundar R"},"body":"peterxcli , I have a fix for this, can I take this ticket up?","body_raw":"[~peterxcli] , I have a fix for this, can I take this ticket up?","created":"2025-01-06T10:19:57.922+0000","updated":"2025-01-06T10:19:57.922+0000","updateAuthor":{"displayName":"Shivsundar R"}},{"id":"17910141","author":{"displayName":"Chu Cheng Li"},"body":"Sure, thanks!","body_raw":"Sure, thanks!","created":"2025-01-06T10:59:19.566+0000","updated":"2025-01-06T10:59:19.566+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix the scenario where acknowledgement callback is being called on null acknowledgements\n\n","output":"","metadata":{"issue_id":"KAFKA-18397","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Shivsundar R"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2025-01-03T07:48:46.000+0000","updated":"2025-01-09T13:21:57.000+0000","resolved":"2025-01-08T15:26:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910109","author":{"displayName":"Shivsundar R"},"body":"peterxcli , I have a fix for this, can I take this ticket up?","body_raw":"[~peterxcli] , I have a fix for this, can I take this ticket up?","created":"2025-01-06T10:19:57.922+0000","updated":"2025-01-06T10:19:57.922+0000","updateAuthor":{"displayName":"Shivsundar R"}},{"id":"17910141","author":{"displayName":"Chu Cheng Li"},"body":"Sure, thanks!","body_raw":"Sure, thanks!","created":"2025-01-06T10:59:19.566+0000","updated":"2025-01-06T10:59:19.566+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix the scenario where acknowledgement callback is being called on null acknowledgements\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18397","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Shivsundar R"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2025-01-03T07:48:46.000+0000","updated":"2025-01-09T13:21:57.000+0000","resolved":"2025-01-08T15:26:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910109","author":{"displayName":"Shivsundar R"},"body":"peterxcli , I have a fix for this, can I take this ticket up?","body_raw":"[~peterxcli] , I have a fix for this, can I take this ticket up?","created":"2025-01-06T10:19:57.922+0000","updated":"2025-01-06T10:19:57.922+0000","updateAuthor":{"displayName":"Shivsundar R"}},{"id":"17910141","author":{"displayName":"Chu Cheng Li"},"body":"Sure, thanks!","body_raw":"Sure, thanks!","created":"2025-01-06T10:59:19.566+0000","updated":"2025-01-06T10:59:19.566+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix the scenario where acknowledgement callback is being called on null acknowledgements\n\n\n\nConversation:\nuser: peterxcli , I have a fix for this, can I take this ticket up?\nassistant: Sure, thanks!","output":"Sure, thanks!","metadata":{"issue_id":"KAFKA-18397","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Shivsundar R"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2025-01-03T07:48:46.000+0000","updated":"2025-01-09T13:21:57.000+0000","resolved":"2025-01-08T15:26:09.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910109","author":{"displayName":"Shivsundar R"},"body":"peterxcli , I have a fix for this, can I take this ticket up?","body_raw":"[~peterxcli] , I have a fix for this, can I take this ticket up?","created":"2025-01-06T10:19:57.922+0000","updated":"2025-01-06T10:19:57.922+0000","updateAuthor":{"displayName":"Shivsundar R"}},{"id":"17910141","author":{"displayName":"Chu Cheng Li"},"body":"Sure, thanks!","body_raw":"Sure, thanks!","created":"2025-01-06T10:59:19.566+0000","updated":"2025-01-06T10:59:19.566+0000","updateAuthor":{"displayName":"Chu Cheng Li"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"Resolved","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"Blocker","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"After log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"Fixed","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper\n\nAfter log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.\n\nConversation:\nuser: frankvicky Is this something that we must do in 4.0?\nassistant: Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configur\nassistant: frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.\nassistant: I will try to get this done before next week.","output":"I will try to get this done before next week.","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"After log4j migration, we need to update the logging configuration in {{KafkaDockerWrapper}} from log4j1 to log4j2.","output":"Migrate log4j1 configuration to log4j2 in KafkaDockerWrapper","metadata":{"issue_id":"KAFKA-18396","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2025-01-03T03:55:05.000+0000","updated":"2025-02-18T07:22:53.000+0000","resolved":"2025-02-11T07:59:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":4,"comments":[{"id":"17914688","author":{"displayName":"David Jacot"},"body":"frankvicky Is this something that we must do in 4.0?","body_raw":"[~frankvicky] Is this something that we must do in 4.0?","created":"2025-01-20T13:48:43.964+0000","updated":"2025-01-20T13:48:43.964+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17914693","author":{"displayName":"TengYao Chi"},"body":"Hi dajac In short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2. The reason why this issue hasn't been addressed yet: In log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format. I'm still working on it.","body_raw":"Hi [~dajac] \r\n\r\nIn short, this is not a must-do item for version 4.0, as `log4j-1.2-api` already provides log4j1 compatibility mode for log4j2.\r\n\r\nThe reason why this issue hasn't been addressed yet:\r\nIn log4j1, we used properties files as the configuration format, which has a flat structure where new configuration entries can override old ones. However, in log4j2, we decided to use YAML as our configuration format, which has a hierarchical structure. Since we accept multiple configuration sources, we need to handle configuration conflicts and ensure new configuration entries can properly override old ones. This is straightforward in the properties format but becomes tricky in the YAML format.\r\n\r\nI'm still working on it.","created":"2025-01-20T14:13:38.468+0000","updated":"2025-01-20T14:13:45.719+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17923752","author":{"displayName":"Chia-Ping Tsai"},"body":"frankvicky I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","body_raw":"[~frankvicky] I think this is a blocker issue as by default we mount the log4j2 yaml - and users will expect those envs should work as before.","created":"2025-02-04T15:37:41.287+0000","updated":"2025-02-04T15:37:41.287+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17923753","author":{"displayName":"TengYao Chi"},"body":"I will try to get this done before next week.","body_raw":"I will try to get this done before next week.","created":"2025-02-04T15:44:38.924+0000","updated":"2025-02-04T15:44:38.924+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Initialize KafkaRaftMetrics without QuorumState to prevent circularity\n\nTo implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","output":"Initialize KafkaRaftMetrics without QuorumState to prevent circularity","metadata":{"issue_id":"KAFKA-18395","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"Kevin Wu"},"created":"2025-01-03T00:07:46.000+0000","updated":"2025-01-22T18:41:45.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Initialize KafkaRaftMetrics without QuorumState to prevent circularity\n\nTo implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","output":"Open","metadata":{"issue_id":"KAFKA-18395","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"Kevin Wu"},"created":"2025-01-03T00:07:46.000+0000","updated":"2025-01-22T18:41:45.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Initialize KafkaRaftMetrics without QuorumState to prevent circularity\n\nTo implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","output":"Major","metadata":{"issue_id":"KAFKA-18395","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"Kevin Wu"},"created":"2025-01-03T00:07:46.000+0000","updated":"2025-01-22T18:41:45.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Initialize KafkaRaftMetrics without QuorumState to prevent circularity\n\nTo implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","output":"To implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","metadata":{"issue_id":"KAFKA-18395","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"Kevin Wu"},"created":"2025-01-03T00:07:46.000+0000","updated":"2025-01-22T18:41:45.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"To implement https://issues.apache.org/jira/browse/KAFKA-16524, `QuorumState` needs to be removed from `KafkaRaftMetrics` constructor to avoid a circularity. That PR's approach is to move `QuorumState` to a `KafkaRaftMetrics#initialize` method for now.","output":"Initialize KafkaRaftMetrics without QuorumState to prevent circularity","metadata":{"issue_id":"KAFKA-18395","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"Kevin Wu"},"created":"2025-01-03T00:07:46.000+0000","updated":"2025-01-22T18:41:45.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Formalize durable and in-memory election state changes\n\nWe essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change For the second case, we should explicitly check no persisted or in-memory election state has changed. For the first case, we should have the in-memory election state derive from the persisted election state change. (It should be impossible for the two to differ)","output":"Formalize durable and in-memory election state changes","metadata":{"issue_id":"KAFKA-18394","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T19:09:48.000+0000","updated":"2025-01-03T17:14:47.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Formalize durable and in-memory election state changes\n\nWe essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change For the second case, we should explicitly check no persisted or in-memory election state has changed. For the first case, we should have the in-memory election state derive from the persisted election s","output":"Open","metadata":{"issue_id":"KAFKA-18394","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T19:09:48.000+0000","updated":"2025-01-03T17:14:47.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Formalize durable and in-memory election state changes\n\nWe essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change For the second case, we should explicitly check no persisted or in-memory election state has changed. For the first case, we should have the in-memory election state derive from the persisted election s","output":"Major","metadata":{"issue_id":"KAFKA-18394","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T19:09:48.000+0000","updated":"2025-01-03T17:14:47.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Formalize durable and in-memory election state changes\n\nWe essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change For the second case, we should explicitly check no persisted or in-memory election state has changed. For the first case, we should have the in-memory election state derive from the persisted election s","output":"We essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change Fo","metadata":{"issue_id":"KAFKA-18394","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T19:09:48.000+0000","updated":"2025-01-03T17:14:47.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We essentially have two paths of epoch state transitions - 1. one which involves updating on-disk election state and then in-memory epoch state (which includes in-memory election state changes) 2. one which does not involve any changed election state and just needs an in-memory epoch state change For the second case, we should explicitly check no persisted or in-memory election state has changed. For the first case, we should have the in-memory election state derive from the persisted election state change. (It should be impossible for the two to differ)","output":"Formalize durable and in-memory election state changes","metadata":{"issue_id":"KAFKA-18394","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T19:09:48.000+0000","updated":"2025-01-03T17:14:47.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove code deprecated to implement KIP-1043\n\nThe following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","output":"Remove code deprecated to implement KIP-1043","metadata":{"issue_id":"KAFKA-18393","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T13:25:17.000+0000","updated":"2025-02-26T07:40:49.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove code deprecated to implement KIP-1043\n\nThe following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","output":"Open","metadata":{"issue_id":"KAFKA-18393","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T13:25:17.000+0000","updated":"2025-02-26T07:40:49.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove code deprecated to implement KIP-1043\n\nThe following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","output":"Blocker","metadata":{"issue_id":"KAFKA-18393","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T13:25:17.000+0000","updated":"2025-02-26T07:40:49.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove code deprecated to implement KIP-1043\n\nThe following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","output":"The following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","metadata":{"issue_id":"KAFKA-18393","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T13:25:17.000+0000","updated":"2025-02-26T07:40:49.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The following deprecated code is removed in AK 5.0 (all in o.a.k.clients.admin package): * Admin.listConsumerGroups * ConsumerGroupDescription constructors * ConsumerGroupDescription.state() * ListConsumerGroupsOptions * ListConsumerGroupsResult * ConsumerGroupState * ConsumerGroupListing","output":"Remove code deprecated to implement KIP-1043","metadata":{"issue_id":"KAFKA-18393","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Blocker","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T13:25:17.000+0000","updated":"2025-02-26T07:40:49.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Require client-generated member IDs for ShareGroupHeartbeat\n\nThis implements KIP-1082 for share groups.","output":"Resolved","metadata":{"issue_id":"KAFKA-18392","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T12:01:22.000+0000","updated":"2025-01-22T08:58:50.000+0000","resolved":"2025-01-22T08:58:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Require client-generated member IDs for ShareGroupHeartbeat\n\nThis implements KIP-1082 for share groups.","output":"Major","metadata":{"issue_id":"KAFKA-18392","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T12:01:22.000+0000","updated":"2025-01-22T08:58:50.000+0000","resolved":"2025-01-22T08:58:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Require client-generated member IDs for ShareGroupHeartbeat\n\nThis implements KIP-1082 for share groups.","output":"This implements KIP-1082 for share groups.","metadata":{"issue_id":"KAFKA-18392","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T12:01:22.000+0000","updated":"2025-01-22T08:58:50.000+0000","resolved":"2025-01-22T08:58:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Require client-generated member IDs for ShareGroupHeartbeat\n\nThis implements KIP-1082 for share groups.","output":"Fixed","metadata":{"issue_id":"KAFKA-18392","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2025-01-02T12:01:22.000+0000","updated":"2025-01-22T08:58:50.000+0000","resolved":"2025-01-22T08:58:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Skip running \"Flaky Test Report\" job on forks\n\nJob {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Skip running \"Flaky Test Report\" job on forks","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Skip running \"Flaky Test Report\" job on forks\n\nJob {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Resolved","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Skip running \"Flaky Test Report\" job on forks\n\nJob {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Minor","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Skip running \"Flaky Test Report\" job on forks\n\nJob {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Job {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKE","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Skip running \"Flaky Test Report\" job on forks\n\nJob {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Fixed","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Job {code:java} name: Flaky Test Report on: workflow_dispatch: # Let us run manually schedule: - cron: '0 6 * * *' # Run daily at 6am UTC jobs: flaky-test-report: name: Flaky Test Report {code} keeps running and failing on forked repositories due to missing secrets {code:java} DEVELOCITY_ACCESS_TOKEN: ${{ secrets.DV_API_ACCESS }}{code} Update ci to skip the run on forked repositories to decrease number of unnecessary emails and failed builds. !image-2025-01-02-12-28-26-839.png!","output":"Skip running \"Flaky Test Report\" job on forks","metadata":{"issue_id":"KAFKA-18391","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2025-01-02T11:29:08.000+0000","updated":"2025-01-17T09:19:03.000+0000","resolved":"2025-01-17T09:19:03.000+0000","resolution":"Fixed","labels":[],"components":["core"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder\n\nsee https://github.com/apache/kafka/pull/18232#discussion_r1900425014","output":"Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder","metadata":{"issue_id":"KAFKA-18390","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-02T11:26:03.000+0000","updated":"2025-06-25T09:42:24.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17986073","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:42:24.964+0000","updated":"2025-06-25T09:42:24.964+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder\n\nsee https://github.com/apache/kafka/pull/18232#discussion_r1900425014","output":"Open","metadata":{"issue_id":"KAFKA-18390","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-02T11:26:03.000+0000","updated":"2025-06-25T09:42:24.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17986073","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:42:24.964+0000","updated":"2025-06-25T09:42:24.964+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder\n\nsee https://github.com/apache/kafka/pull/18232#discussion_r1900425014","output":"Major","metadata":{"issue_id":"KAFKA-18390","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-02T11:26:03.000+0000","updated":"2025-06-25T09:42:24.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17986073","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:42:24.964+0000","updated":"2025-06-25T09:42:24.964+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder\n\nsee https://github.com/apache/kafka/pull/18232#discussion_r1900425014","output":"see https://github.com/apache/kafka/pull/18232#discussion_r1900425014","metadata":{"issue_id":"KAFKA-18390","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-02T11:26:03.000+0000","updated":"2025-06-25T09:42:24.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17986073","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:42:24.964+0000","updated":"2025-06-25T09:42:24.964+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Use LinkedHashMap instead of Map in creating MetricName and SensorBuilder\n\nsee https://github.com/apache/kafka/pull/18232#discussion_r1900425014\n\nConversation:\nuser: Moving to the next release as we're now in code freeze for 4.1.0.","output":"Moving to the next release as we're now in code freeze for 4.1.0.","metadata":{"issue_id":"KAFKA-18390","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-02T11:26:03.000+0000","updated":"2025-06-25T09:42:24.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17986073","author":{"displayName":"Mickael Maison"},"body":"Moving to the next release as we're now in code freeze for 4.1.0.","body_raw":"Moving to the next release as we're now in code freeze for 4.1.0.","created":"2025-06-25T09:42:24.964+0000","updated":"2025-06-25T09:42:24.964+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Do not lose votedKey on transition to LeaderState\n\nAfter KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","output":"Do not lose votedKey on transition to LeaderState","metadata":{"issue_id":"KAFKA-18389","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T07:05:28.000+0000","updated":"2025-01-16T14:50:17.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Do not lose votedKey on transition to LeaderState\n\nAfter KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","output":"Open","metadata":{"issue_id":"KAFKA-18389","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T07:05:28.000+0000","updated":"2025-01-16T14:50:17.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Do not lose votedKey on transition to LeaderState\n\nAfter KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","output":"Minor","metadata":{"issue_id":"KAFKA-18389","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T07:05:28.000+0000","updated":"2025-01-16T14:50:17.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Do not lose votedKey on transition to LeaderState\n\nAfter KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","output":"After KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","metadata":{"issue_id":"KAFKA-18389","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T07:05:28.000+0000","updated":"2025-01-16T14:50:17.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"After KAFKA-17642 is merged in, all epoch state transitions (other than the transition to Leader) will preserve votedKey and leaderId state. We should make the transition to LeaderState consistent with this.","output":"Do not lose votedKey on transition to LeaderState","metadata":{"issue_id":"KAFKA-18389","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Alyssa Huang"},"created":"2025-01-02T07:05:28.000+0000","updated":"2025-01-16T14:50:17.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well","output":"test-kraft-server-start.sh should use log4j2.yaml","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well","output":"Resolved","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well","output":"Blocker","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well","output":"as title, and we should remove kraft-log4j.properties as well","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well","output":"Fixed","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"test-kraft-server-start.sh should use log4j2.yaml\n\nas title, and we should remove kraft-log4j.properties as well\n\nConversation:\nuser: Hi chia7712, may I take this issue if you're not working on it? Thanks.\nassistant: trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","output":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","metadata":{"issue_id":"KAFKA-18388","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2025-01-01T12:58:54.000+0000","updated":"2025-01-06T20:23:11.000+0000","resolved":"2025-01-06T20:23:11.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909131","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, may I take this issue if you're not working on it? Thanks.","body_raw":"Hi [~chia7712], may I take this issue if you're not working on it? Thanks.","created":"2025-01-01T13:05:11.692+0000","updated":"2025-01-01T13:05:11.692+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17910384","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345 4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","body_raw":"trunk: https://github.com/apache/kafka/commit/a52aedd6ff9ade0230c0f41b473e8fbdfa2e0345\r\n\r\n4.0: https://github.com/apache/kafka/commit/efdfa0184259a41e0c22359df36a169c8d97214d","created":"2025-01-06T20:23:11.290+0000","updated":"2025-01-06T20:23:11.290+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove UpdateMetadata and LeaderAndIsr RPC\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18387","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-01T12:54:26.000+0000","updated":"2025-01-08T11:17:32.000+0000","resolved":"2025-01-08T11:17:32.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909672","author":{"displayName":"Ismael Juma"},"body":"We should probably remove `LeaderAndIsr` at the same time.","body_raw":"We should probably remove `LeaderAndIsr` at the same time.","created":"2025-01-03T20:44:24.249+0000","updated":"2025-01-03T20:44:24.249+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17909688","author":{"displayName":"TaiJuWu"},"body":"Rename title to include this task.","body_raw":"Rename title to include this task.","created":"2025-01-04T00:07:05.862+0000","updated":"2025-01-04T00:07:05.862+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911023","author":{"displayName":"Chia-Ping Tsai"},"body":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","body_raw":"it seems [~taijuwu] has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","created":"2025-01-08T11:17:32.720+0000","updated":"2025-01-08T11:17:32.720+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove UpdateMetadata and LeaderAndIsr RPC\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18387","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-01T12:54:26.000+0000","updated":"2025-01-08T11:17:32.000+0000","resolved":"2025-01-08T11:17:32.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909672","author":{"displayName":"Ismael Juma"},"body":"We should probably remove `LeaderAndIsr` at the same time.","body_raw":"We should probably remove `LeaderAndIsr` at the same time.","created":"2025-01-03T20:44:24.249+0000","updated":"2025-01-03T20:44:24.249+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17909688","author":{"displayName":"TaiJuWu"},"body":"Rename title to include this task.","body_raw":"Rename title to include this task.","created":"2025-01-04T00:07:05.862+0000","updated":"2025-01-04T00:07:05.862+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911023","author":{"displayName":"Chia-Ping Tsai"},"body":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","body_raw":"it seems [~taijuwu] has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","created":"2025-01-08T11:17:32.720+0000","updated":"2025-01-08T11:17:32.720+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove UpdateMetadata and LeaderAndIsr RPC\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18387","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-01T12:54:26.000+0000","updated":"2025-01-08T11:17:32.000+0000","resolved":"2025-01-08T11:17:32.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909672","author":{"displayName":"Ismael Juma"},"body":"We should probably remove `LeaderAndIsr` at the same time.","body_raw":"We should probably remove `LeaderAndIsr` at the same time.","created":"2025-01-03T20:44:24.249+0000","updated":"2025-01-03T20:44:24.249+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17909688","author":{"displayName":"TaiJuWu"},"body":"Rename title to include this task.","body_raw":"Rename title to include this task.","created":"2025-01-04T00:07:05.862+0000","updated":"2025-01-04T00:07:05.862+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911023","author":{"displayName":"Chia-Ping Tsai"},"body":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","body_raw":"it seems [~taijuwu] has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","created":"2025-01-08T11:17:32.720+0000","updated":"2025-01-08T11:17:32.720+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove UpdateMetadata and LeaderAndIsr RPC\n\nas title","output":"Duplicate","metadata":{"issue_id":"KAFKA-18387","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-01T12:54:26.000+0000","updated":"2025-01-08T11:17:32.000+0000","resolved":"2025-01-08T11:17:32.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909672","author":{"displayName":"Ismael Juma"},"body":"We should probably remove `LeaderAndIsr` at the same time.","body_raw":"We should probably remove `LeaderAndIsr` at the same time.","created":"2025-01-03T20:44:24.249+0000","updated":"2025-01-03T20:44:24.249+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17909688","author":{"displayName":"TaiJuWu"},"body":"Rename title to include this task.","body_raw":"Rename title to include this task.","created":"2025-01-04T00:07:05.862+0000","updated":"2025-01-04T00:07:05.862+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911023","author":{"displayName":"Chia-Ping Tsai"},"body":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","body_raw":"it seems [~taijuwu] has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","created":"2025-01-08T11:17:32.720+0000","updated":"2025-01-08T11:17:32.720+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove UpdateMetadata and LeaderAndIsr RPC\n\nas title\n\nConversation:\nuser: We should probably remove `LeaderAndIsr` at the same time.\nassistant: Rename title to include this task.\nassistant: it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","output":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","metadata":{"issue_id":"KAFKA-18387","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2025-01-01T12:54:26.000+0000","updated":"2025-01-08T11:17:32.000+0000","resolved":"2025-01-08T11:17:32.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909672","author":{"displayName":"Ismael Juma"},"body":"We should probably remove `LeaderAndIsr` at the same time.","body_raw":"We should probably remove `LeaderAndIsr` at the same time.","created":"2025-01-03T20:44:24.249+0000","updated":"2025-01-03T20:44:24.249+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17909688","author":{"displayName":"TaiJuWu"},"body":"Rename title to include this task.","body_raw":"Rename title to include this task.","created":"2025-01-04T00:07:05.862+0000","updated":"2025-01-04T00:07:05.862+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911023","author":{"displayName":"Chia-Ping Tsai"},"body":"it seems taijuwu has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","body_raw":"it seems [~taijuwu] has filed PR (https://github.com/apache/kafka/pull/18422) for KAFKA-18399","created":"2025-01-08T11:17:32.720+0000","updated":"2025-01-08T11:17:32.720+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) dc64->dc88 [2025-01-01 08:05:55,652] INFO [AdminClient clientId=dc64->dc88] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager:267) dc64->dc88 org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata [2025-01-01 08:05:55,653] INFO App info kafka.admin.client for dc64->dc88 unregistered (org.apache.kafka.common.utils.AppInfoParser:88) dc64->dc88 [2025-01-01 08:05:55,653] INFO [AdminClient clientId=dc64->dc88] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager:267) dc64->dc88 org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata [2025-01-01 08:05:55,653] INFO [AdminClient clientId=dc64->dc88] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient:1450) dc64->dc88 [2025-01-01 08:05:55,657] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684) dc64->dc88 [2025-01-01 08:05:55,658] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688) dc64->dc88 [2025-01-01 08:05:55,658] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694) dc64->dc88 [2025-01-01 08:05:55,658] ERROR Stopping due to error (org.apache.kafka.connect.mirror.MirrorMaker:360)[main] org.apache.kafka.connect.errors.ConnectException: Failed to connect to and describe Kafka cluster. Check worker's broker connection and security properties. at org.apache.kafka.connect.runtime.WorkerConfig.lookupKafkaClusterId(WorkerConfig.java:305) at org.apache.kafka.connect.runtime.WorkerConfig.lookupKafkaClusterId(WorkerConfig.java:285) at org.apache.kafka.connect.runtime.WorkerConfig.kafkaClusterId(WorkerConfig.java:415) at org.apache.kafka.connect.mirror.MirrorMaker.addHerder(MirrorMaker.java:252) at java.base/java.lang.Iterable.forEach(Unknown Source) at org.apache.kafka.connect.mirror.MirrorMaker. (MirrorMaker.java:158) at org.apache.kafka.connect.mirror.MirrorMaker. (MirrorMaker.java:170) at org.apache.kafka.connect.mirror.MirrorMaker. (MirrorMaker.java:174) at org.apache.kafka.connect.mirror.MirrorMaker.main(MirrorMaker.java:347) Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source) at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165) at org.apache.kafka.connect.runtime.WorkerConfig.lookupKafkaClusterId(WorkerConfig.java:299) ... 8 more Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes [2025-01-01 08:05:55,687] INFO Stopped http_8083@6705fb02\\{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)[JettyShutdownThread] ``` The configuration of mirrormaker is: ``` clusters = dc64, dc88 dc64.bootstrap.servers = 192.168.2.64:13399 dc88.bootstrap.servers = 192.168.2.88:13399 dc64->dc88.enabled = true dc64->dc88.topics = .* dc88->dc64.enabled = true dc88->dc64.topics = .* replication.factor=1 tasks.max=6 emit.checkpoints.interval.seconds=5 dc64.producer.acks=all dc64.producer.batch.size=50000 dc64.consumer.auto.offset.reset=latest dc88.consumer.auto.offset.reset=latest dc64.consumer.max.poll.interval.ms=20000 dc88.consumer.max.poll.interval.ms=20000 refresh.topics.enabled=true refresh.topics.interval.seconds=5 refresh.groups.enabled=true refresh.groups.interval.seconds=5 dedicated.mode.enable.internal.rest = true dc64.scheduled.rebalance.max.delay.ms=20000 dc88.scheduled.rebalance.max.delay.ms=20000 checkpoints.topic.replication.factor=1 heartbeats.topic.replication.factor=1 offset-syncs.topic.replication.factor=1 offset.storage.replication.factor=1 status.storage.replication.factor=1 config.storage.replication.factor=1 ```","output":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) ","output":"Resolved","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) ","output":"Major","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) ","output":"When using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-0","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) ","output":"Won't Fix","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off\n\nWhen using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue \n\nConversation:\nuser: Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc\nassistant: Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health\nassistant: Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did j","output":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did j","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"When using Kubernetes deployment with MirrorMaker v3.7.1 and deploying one Kafka node in each data center (DC1 and DC2), if DC1 is powered off, DC2 will encounter a CrashLoopBackOff error. This issue is different from the one described in KAFKA-17784. Please find the report log below: ```log [2025-01-01 08:05:53,432] WARN [AdminClient clientId=dc64->dc88] Connection to node -1 (/192.168.2.88:13399) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:830) dc64->dc88 [2025-01-01 08:05:55,652] INFO [AdminClient clientId=dc64->dc88] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager:267) dc64->dc88 org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata [2025-01-01 08:05","output":"Mirror Maker2 Pod CrashLoopBackoff When one DC is powered off","metadata":{"issue_id":"KAFKA-18386","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"George Yang"},"created":"2025-01-01T08:55:12.000+0000","updated":"2025-01-30T18:00:26.000+0000","resolved":"2025-01-30T18:00:26.000+0000","resolution":"Won't Fix","labels":[],"components":["mirrormaker"],"comment_count":3,"comments":[{"id":"17909154","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 Thanks for the ticket. This is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash. And in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","body_raw":"Hi [~yitian998] Thanks for the ticket.\r\n\r\nThis is the expected behavior of MM2 (and Connect) when the cluster storing the underlying Connect topics is unavailable. Connect workers use Kafka to discover other workers, assign work, come to a consensus on the running configuration, provide observability, etc. Without the underlying Kafka, Connect would be so severely degraded that it is preferable to crash.\r\n\r\nAnd in the MM2 use case, if one or both of the Kafka clusters is unavailable, MM2 cannot meaningfully operate. I would suggest stopping your MM2 deployments in advance of shutting down your Kafka deployments in order to avoid nuisance crashes.","created":"2025-01-01T19:52:27.319+0000","updated":"2025-01-01T19:52:27.319+0000","updateAuthor":{"displayName":"Greg Harris"}},{"id":"17909292","author":{"displayName":"George Yang"},"body":"Thank you gharris1727 for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas: # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash? # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts. Any suggestion would be welcome.","body_raw":"Thank you [~gharris1727] for your timely response. It seems that we don't have a good way to avoid the crash, but I have two ideas:\r\n # Could we implement a backoff algorithm in MirrorMaker 2 to delay the crash?\r\n # We could add livenessProbe and readinessProbe in the mirrormaker.yml file to perform health checks, which could help reduce the frequency of restarts.\r\n\r\nAny suggestion would be welcome.","created":"2025-01-02T11:52:07.940+0000","updated":"2025-01-02T11:52:07.940+0000","updateAuthor":{"displayName":"George Yang"}},{"id":"17922476","author":{"displayName":"Greg Harris"},"body":"Hi yitian998 . 1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect. 2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect]","body_raw":"Hi [~yitian998] .\r\n\r\n1. That sounds like something that you should be able to configure in your environment: [https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#configurable-container-restart-delay], and isn't appropriate to implement within Connect.\r\n2. You can explore that option. We did just add a dedicated healthcheck endpoint to facilitate liveness checks: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1017%3A+Health+check+endpoint+for+Kafka+Connect] ","created":"2025-01-30T18:00:13.787+0000","updated":"2025-01-30T18:00:13.787+0000","updateAuthor":{"displayName":"Greg Harris"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"maybeHandleCommonResponse doesn't seem to handle all cases\n\nIt is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETCH request to the bootstrap servers until a bootstrap server provides the leader id and leader's endpoint in the FETCH response. This response is supposed to be handled by maybeHandleCommonResponse but this logic doesn't seem correct: {code:java} } else if (epoch == quorum.epoch() && leaderId.isPresent() && !quorum.hasLeader()) { // Since we are transitioning to Follower, we will only forward the // request to the handler if there is no error. Otherwise, we will let // the request be retried immediately (if needed) after the transition. // This handling allows an observer to discover the leader and append // to the log in the same Fetch request. transitionToFollower(epoch, leaderId.getAsInt(), leaderEndpoints, currentTimeMs); if (error == Errors.NONE) { return Optional.empty(); } else { return Optional.of(value: true); } } {code} One possible fix is to check if the locally known leader endpoints is empty in the predicate for the if expression.","output":"maybeHandleCommonResponse doesn't seem to handle all cases","metadata":{"issue_id":"KAFKA-18385","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-31T21:50:48.000+0000","updated":"2024-12-31T21:50:48.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"maybeHandleCommonResponse doesn't seem to handle all cases\n\nIt is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETCH request to the bootstrap servers until a bootstrap server provides the leader id and leader's endpoint in the FETCH response. This response is supposed to be handled by maybeHandleCommonResponse but","output":"Open","metadata":{"issue_id":"KAFKA-18385","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-31T21:50:48.000+0000","updated":"2024-12-31T21:50:48.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"maybeHandleCommonResponse doesn't seem to handle all cases\n\nIt is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETCH request to the bootstrap servers until a bootstrap server provides the leader id and leader's endpoint in the FETCH response. This response is supposed to be handled by maybeHandleCommonResponse but","output":"Major","metadata":{"issue_id":"KAFKA-18385","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-31T21:50:48.000+0000","updated":"2024-12-31T21:50:48.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: maybeHandleCommonResponse doesn't seem to handle all cases\n\nIt is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETCH request to the bootstrap servers until a bootstrap server provides the leader id and leader's endpoint in the FETCH response. This response is supposed to be handled by maybeHandleCommonResponse but","output":"It is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETC","metadata":{"issue_id":"KAFKA-18385","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-31T21:50:48.000+0000","updated":"2024-12-31T21:50:48.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"It is possible for a replica to know the leader id but not the leader's endpoint. This can happen when the replica restart and the voter set that includes the leader has not bee replicated in the log. In this case the replica will start as unattached with a leader id. In unattached it will send FETCH request to the bootstrap servers until a bootstrap server provides the leader id and leader's endpoint in the FETCH response. This response is supposed to be handled by maybeHandleCommonResponse but this logic doesn't seem correct: {code:java} } else if (epoch == quorum.epoch() && leaderId.isPresent() && !quorum.hasLeader()) { // Since we are transitioning to Follower, we will only forward the // request to the handler if there is no error. Otherwise, we will let // the request be retried imme","output":"maybeHandleCommonResponse doesn't seem to handle all cases","metadata":{"issue_id":"KAFKA-18385","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Open","priority":"Major","assignee":{"displayName":"José Armando García Sancio"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-31T21:50:48.000+0000","updated":"2024-12-31T21:50:48.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove reserved.broker.max.id and broker.id.generation.enable\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18383","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-31T02:11:56.000+0000","updated":"2025-01-29T19:00:07.000+0000","resolved":"2025-01-29T19:00:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922185","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","body_raw":"trunk:  https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535\r\n\r\n4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","created":"2025-01-29T19:00:07.349+0000","updated":"2025-01-29T19:00:07.349+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove reserved.broker.max.id and broker.id.generation.enable\n\n","output":"Blocker","metadata":{"issue_id":"KAFKA-18383","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-31T02:11:56.000+0000","updated":"2025-01-29T19:00:07.000+0000","resolved":"2025-01-29T19:00:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922185","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","body_raw":"trunk:  https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535\r\n\r\n4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","created":"2025-01-29T19:00:07.349+0000","updated":"2025-01-29T19:00:07.349+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove reserved.broker.max.id and broker.id.generation.enable\n\n","output":"","metadata":{"issue_id":"KAFKA-18383","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-31T02:11:56.000+0000","updated":"2025-01-29T19:00:07.000+0000","resolved":"2025-01-29T19:00:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922185","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","body_raw":"trunk:  https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535\r\n\r\n4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","created":"2025-01-29T19:00:07.349+0000","updated":"2025-01-29T19:00:07.349+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove reserved.broker.max.id and broker.id.generation.enable\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18383","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-31T02:11:56.000+0000","updated":"2025-01-29T19:00:07.000+0000","resolved":"2025-01-29T19:00:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922185","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","body_raw":"trunk:  https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535\r\n\r\n4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","created":"2025-01-29T19:00:07.349+0000","updated":"2025-01-29T19:00:07.349+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove reserved.broker.max.id and broker.id.generation.enable\n\n\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","output":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","metadata":{"issue_id":"KAFKA-18383","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-31T02:11:56.000+0000","updated":"2025-01-29T19:00:07.000+0000","resolved":"2025-01-29T19:00:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17922185","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535 4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","body_raw":"trunk:  https://github.com/apache/kafka/commit/4dd0bcbde85a599a22a41db2d832fc7e71805535\r\n\r\n4.0: https://github.com/apache/kafka/commit/ffb99a45f0ecebbde8df2255fa0bb0c4fd6033e2","created":"2025-01-29T19:00:07.349+0000","updated":"2025-01-29T19:00:07.349+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","output":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","output":"Resolved","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","output":"Major","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","output":"https://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712","output":"Duplicate","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove KafkaApisTest `raftSupport` and change test to Kraft mode\n\nhttps://github.com/apache/kafka/pull/18352#pullrequestreview-2525842712\n\nConversation:\nuser: This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.\nassistant: This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","output":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","metadata":{"issue_id":"KAFKA-18380","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-31T00:37:32.000+0000","updated":"2025-01-15T01:42:00.000+0000","resolved":"2025-01-15T01:42:00.000+0000","resolution":"Duplicate","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910662","author":{"displayName":"黃竣陽"},"body":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","body_raw":"This Jira would be merge into https://issues.apache.org/jira/browse/KAFKA-18399 , I will close it.","created":"2025-01-07T15:24:03.642+0000","updated":"2025-01-07T15:24:03.642+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17911448","author":{"displayName":"黃竣陽"},"body":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","body_raw":"This issue will wait for https://issues.apache.org/jira/browse/KAFKA-18399, and clean up KafkaApisTest","created":"2025-01-09T11:49:21.989+0000","updated":"2025-01-09T11:49:21.989+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Enforce resigned cannot transition to any other state in same epoch\n\n","output":"Open","metadata":{"issue_id":"KAFKA-18379","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T19:20:53.000+0000","updated":"2025-10-27T03:33:22.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908988","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this issue, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this issue, thanks :)","created":"2024-12-30T23:05:18.202+0000","updated":"2024-12-30T23:05:18.202+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909204","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","body_raw":"Thanks [~frankvicky]! \r\nI can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","created":"2025-01-02T07:11:40.905+0000","updated":"2025-01-02T07:11:40.905+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909206","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","body_raw":"Hi [~alyssahuang] \r\n\r\nThanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","created":"2025-01-02T07:23:21.907+0000","updated":"2025-01-02T07:23:21.907+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Enforce resigned cannot transition to any other state in same epoch\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18379","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T19:20:53.000+0000","updated":"2025-10-27T03:33:22.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908988","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this issue, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this issue, thanks :)","created":"2024-12-30T23:05:18.202+0000","updated":"2024-12-30T23:05:18.202+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909204","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","body_raw":"Thanks [~frankvicky]! \r\nI can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","created":"2025-01-02T07:11:40.905+0000","updated":"2025-01-02T07:11:40.905+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909206","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","body_raw":"Hi [~alyssahuang] \r\n\r\nThanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","created":"2025-01-02T07:23:21.907+0000","updated":"2025-01-02T07:23:21.907+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Enforce resigned cannot transition to any other state in same epoch\n\n","output":"","metadata":{"issue_id":"KAFKA-18379","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T19:20:53.000+0000","updated":"2025-10-27T03:33:22.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908988","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this issue, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this issue, thanks :)","created":"2024-12-30T23:05:18.202+0000","updated":"2024-12-30T23:05:18.202+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909204","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","body_raw":"Thanks [~frankvicky]! \r\nI can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","created":"2025-01-02T07:11:40.905+0000","updated":"2025-01-02T07:11:40.905+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909206","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","body_raw":"Hi [~alyssahuang] \r\n\r\nThanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","created":"2025-01-02T07:23:21.907+0000","updated":"2025-01-02T07:23:21.907+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Enforce resigned cannot transition to any other state in same epoch\n\n\n\nConversation:\nuser: Hi alyssahuang I would take over this issue, thanks :)\nassistant: Thanks frankvicky! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-\nassistant: Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","output":"Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","metadata":{"issue_id":"KAFKA-18379","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T19:20:53.000+0000","updated":"2025-10-27T03:33:22.000+0000","resolved":null,"labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908988","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this issue, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this issue, thanks :)","created":"2024-12-30T23:05:18.202+0000","updated":"2024-12-30T23:05:18.202+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909204","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","body_raw":"Thanks [~frankvicky]! \r\nI can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.","created":"2025-01-02T07:11:40.905+0000","updated":"2025-01-02T07:11:40.905+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909206","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","body_raw":"Hi [~alyssahuang] \r\n\r\nThanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)","created":"2025-01-02T07:23:21.907+0000","updated":"2025-01-02T07:23:21.907+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"standardize raft quorum state transitions\n\nStandardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","output":"standardize raft quorum state transitions","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"standardize raft quorum state transitions\n\nStandardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","output":"Open","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"standardize raft quorum state transitions\n\nStandardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","output":"Minor","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: standardize raft quorum state transitions\n\nStandardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","output":"Standardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"standardize raft quorum state transitions\n\nStandardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and dur\n\nConversation:\nuser: Hi alyssahuang I would take over this one, thanks :)","output":"Hi alyssahuang I would take over this one, thanks :)","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Standardize what transitionToX, onBecomeX accomplish. Currently it is not consistent nor clear which method is responsible for doing what (e.g. checking if transition is legal, doing in-memory and durable transition, resetting connections, etc.)","output":"standardize raft quorum state transitions","metadata":{"issue_id":"KAFKA-18378","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:54:58.000+0000","updated":"2025-01-17T14:54:44.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":1,"comments":[{"id":"17908990","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take over this one, thanks :)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take over this one, thanks :)","created":"2024-12-30T23:12:29.468+0000","updated":"2024-12-30T23:12:29.468+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Investigate transition out of resigned state\n\nPreviously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the replica to discover any new leader more quickly (say in the case that the node was removed from quorum, the new leader would not be sending beginquorum requests to this node). However, there is a flaw in that logic - resigned must increase its epoch when transitioning, so any replica that communicates with this replica would then have to bump its epoch. We want to achieve a balance of allowing other replicas to make progress in their elections and potentially become leader, while not having resigned replicas sit doing nothing for too long.","output":"Investigate transition out of resigned state","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Investigate transition out of resigned state\n\nPreviously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the replica to discover any new leader more quickly (say in the case that the node was removed from quorum, the new leader would not be sending beginquorum requests to this node). However, there is a flaw in t","output":"Open","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Investigate transition out of resigned state\n\nPreviously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the replica to discover any new leader more quickly (say in the case that the node was removed from quorum, the new leader would not be sending beginquorum requests to this node). However, there is a flaw in t","output":"Major","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Investigate transition out of resigned state\n\nPreviously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the replica to discover any new leader more quickly (say in the case that the node was removed from quorum, the new leader would not be sending beginquorum requests to this node). However, there is a flaw in t","output":"Previously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the repli","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Investigate transition out of resigned state\n\nPreviously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuo\n\nConversation:\nuser: Hi alyssahuang I would take a look at this issue.:)\nassistant: Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica fro\nassistant: Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and\nassistant: Hi alyssahuang No problem ;)","output":"Hi alyssahuang No problem ;)","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Previously, this ticket was for investigating if we can short circuit to unattached on receiving all endquorum acks. Resigned state keeps track of `unackedVoters` and on pollResigned only sends endQuorum requests to those voters. We thought it might be useful to short circuit this to allow the replica to discover any new leader more quickly (say in the case that the node was removed from quorum, the new leader would not be sending beginquorum requests to this node). However, there is a flaw in that logic - resigned must increase its epoch when transitioning, so any replica that communicates with this replica would then have to bump its epoch. We want to achieve a balance of allowing other replicas to make progress in their elections and potentially become leader, while not having resigned ","output":"Investigate transition out of resigned state","metadata":{"issue_id":"KAFKA-18377","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-30T18:33:30.000+0000","updated":"2025-01-17T14:53:29.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":4,"comments":[{"id":"17908989","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang I would take a look at this issue.:)","body_raw":"Hi [~alyssahuang] \r\n\r\nI would take a look at this issue.:)","created":"2024-12-30T23:07:41.208+0000","updated":"2024-12-30T23:07:41.208+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17909207","author":{"displayName":"Alyssa Huang"},"body":"Thanks frankvicky! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. Thinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached - Time=0 (Replica1 resigns and sends end quorum requests out) - Replica1 - (Resigned, epoch=5) - Replica2 - (Follower, epoch=5, leaderId=1) Time=1 (Replica1 receives end quorum response from all replicas) - Replica1 - (Unattached, epoch=6) - Replica2 - (Prospective, epoch=5) Once Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). I'll update this thread tomorrow after discussing with jsancio","body_raw":"Thanks [~frankvicky]! Sorry for not indicating this clearly on the ticket, but this one definitely relies on KAFKA-17642 which will make Resigned transition to Unattached with epoch + 1 instead of Candidate. \r\n\r\nThinking about it more, adding a short circuit may reduce the chance of a different replica from being elected (which could be desirable if this replica is behaving poorly). Let's say all replicas have received the end quorum request. Any \"preferred candidates\" designated in the end quorum epoch request may have their fetch timeouts overridden (the most preferred will get overridden to 0). Only the most preferred successor gets a guaranteed chance to start an election before the resigned replica transitions to Unattached and then Prospective immediately with this short circuit. Not only does this defeat the purpose of the preferred successor list being greater than 0, this doesn't even give the most preferred successor a good chance to get elected because its epoch will be lower than the resigned replica's after it transitions to unattached -\r\nTime=0 (Replica1 resigns and sends end quorum requests out)\r\n - Replica1 - (Resigned, epoch=5)\r\n - Replica2 - (Follower, epoch=5, leaderId=1)\r\n\r\nTime=1 (Replica1 receives end quorum response from all replicas)\r\n - Replica1 - (Unattached, epoch=6)\r\n - Replica2 - (Prospective, epoch=5)\r\n\r\nOnce Replica1 sends a fetch to Replica2, or Replica2 receives any response from Replica1 for that matter, Replica2 will need to bump its epoch (voiding any election it had started). \r\n\r\nI'll update this thread tomorrow after discussing with [~jsancio] ","created":"2025-01-02T07:36:28.067+0000","updated":"2025-01-02T07:39:29.636+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909413","author":{"displayName":"Alyssa Huang"},"body":"Hey frankvicky, after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","body_raw":"Hey [~frankvicky], after discussing with Jose, we decided to repurpose this ticket for investigating what resigned state behavior should be. It seems this may be quite a bit more complicated than we expected, as we need resigned replicas to bump their epoch when transitioning out of resigned state, and want to prevent them from disrupting the quorum. For now I'll re-assign this to myself since this relies on KAFKA-17642 and is a bit more complicated than I expected.","created":"2025-01-02T19:52:24.714+0000","updated":"2025-01-02T19:52:24.714+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17909460","author":{"displayName":"TengYao Chi"},"body":"Hi alyssahuang No problem ;)","body_raw":"Hi [~alyssahuang] \r\n\r\nNo problem ;)","created":"2025-01-03T04:58:33.803+0000","updated":"2025-01-03T04:58:33.803+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"High CPU load when AsyncKafkaConsumer uses a small max poll value\n\nWe stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two major contributors to the CPU cycles # AsyncKafkaConsumer.updateFetchPosition (addAndGet & new CheckAndUpdatePositionEvent()) # AbstractFetch.fetchablePartitions from the fetchrequestmanager for AsyncKafkaConsumer.updateFetchPosition - It seems like * UUID generation can become quite expensive. This is particularly noticeable when creating large number of events * ConsumerUtils.getResult, which uses future.get() also consumes quite a bit of CPU cycles for fetchablePartitions, FetchBuffer.bufferedPartitions which uses Java ConcurrentLinkedQueue.forEach also consumes quite a bit of CPUs.","output":"High CPU load when AsyncKafkaConsumer uses a small max poll value","metadata":{"issue_id":"KAFKA-18376","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Philip Nee"},"created":"2024-12-30T17:37:31.000+0000","updated":"2025-10-31T20:56:17.000+0000","resolved":null,"labels":["async-kafka-consumer-performance","consumer-threading-refactor","performance"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"High CPU load when AsyncKafkaConsumer uses a small max poll value\n\nWe stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two ","output":"Patch Available","metadata":{"issue_id":"KAFKA-18376","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Philip Nee"},"created":"2024-12-30T17:37:31.000+0000","updated":"2025-10-31T20:56:17.000+0000","resolved":null,"labels":["async-kafka-consumer-performance","consumer-threading-refactor","performance"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"High CPU load when AsyncKafkaConsumer uses a small max poll value\n\nWe stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two ","output":"Major","metadata":{"issue_id":"KAFKA-18376","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Philip Nee"},"created":"2024-12-30T17:37:31.000+0000","updated":"2025-10-31T20:56:17.000+0000","resolved":null,"labels":["async-kafka-consumer-performance","consumer-threading-refactor","performance"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: High CPU load when AsyncKafkaConsumer uses a small max poll value\n\nWe stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two ","output":"We stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll v","metadata":{"issue_id":"KAFKA-18376","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Philip Nee"},"created":"2024-12-30T17:37:31.000+0000","updated":"2025-10-31T20:56:17.000+0000","resolved":null,"labels":["async-kafka-consumer-performance","consumer-threading-refactor","performance"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage. Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer. As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two major contributors to the CPU cycles # AsyncKafkaConsumer.updateFetchPosition (addAndGet & new CheckAndUpdatePositionEvent()) # AbstractFetch.fetchablePartitions from the fetchrequestmanager for AsyncKafkaConsumer.updateFetchPosition - It seems like * UUID generation can become quite expensive. This","output":"High CPU load when AsyncKafkaConsumer uses a small max poll value","metadata":{"issue_id":"KAFKA-18376","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"Kirk True"},"reporter":{"displayName":"Philip Nee"},"created":"2024-12-30T17:37:31.000+0000","updated":"2025-10-31T20:56:17.000+0000","resolved":null,"labels":["async-kafka-consumer-performance","consumer-threading-refactor","performance"],"components":["clients","consumer"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbind-annotations-2.16.2.jar is missing in the license file warning: jetty-ee10-servlet-12.0.15.jar is missing in the license file warning: jakarta.ws.rs-api-3.1.0.jar is missing in the license file warning: snakeyaml-2.2.jar is missing in the license file warning: jakarta.validation-api-3.0.2.jar is missing in the license file warning: jetty-session-12.0.15.jar is missing in the license file warning: jackson-jakarta-rs-base-2.16.2.jar is missing in the license file warning: jersey-common-3.1.9.jar is missing in the license file warning: jakarta.activation-2.0.1.jar is missing in the license file warning: jakarta.annotation-api-2.1.1.jar is missing in the license file warning: jetty-client-12.0.15.jar is missing in the license file warning: jetty-ee10-servlets-12.0.15.jar is missing in the license file warning: jakarta.activation-api-2.1.0.jar is missing in the license file warning: hk2-api-3.0.6.jar is missing in the license file warning: jersey-server-3.1.9.jar is missing in the license file warning: jersey-container-servlet-core-3.1.9.jar is missing in the license file warning: jakarta.inject-api-2.0.1.jar is missing in the license file warning: jetty-http-12.0.15.jar is missing in the license file warning: jetty-io-12.0.15.jar is missing in the license file warning: jackson-jakarta-rs-json-provider-2.16.2.jar is missing in the license file warning: jersey-container-servlet-3.1.9.jar is missing in the license file warning: error_prone_annotations-2.14.0.jar is missing in the license file warning: jetty-util-12.0.15.jar is missing in the license file warning: jakarta.xml.bind-api-3.0.1.jar is missing in the license file warning: jetty-server-12.0.15.jar is missing in the license file warning: jakarta.servlet-api-6.0.0.jar is missing in the license file warning: aopalliance-repackaged-3.0.6.jar is missing in the license file","output":"Update the LICENSE-binary","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbin","output":"Resolved","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbin","output":"Blocker","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbin","output":"the following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbin","output":"Fixed","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Update the LICENSE-binary\n\nthe following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the lice\n\nConversation:\nuser: chia7712 If you have not started working on this, I will handle it. :)\nassistant: trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","output":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"the following list is generated by https://github.com/apache/kafka/pull/18299 warning: jersey-hk2-3.1.9.jar is missing in the license file warning: jetty-alpn-client-12.0.15.jar is missing in the license file warning: jetty-security-12.0.15.jar is missing in the license file warning: hk2-utils-3.0.6.jar is missing in the license file warning: hk2-locator-3.0.6.jar is missing in the license file warning: jersey-client-3.1.9.jar is missing in the license file warning: jackson-module-jakarta-xmlbind-annotations-2.16.2.jar is missing in the license file warning: jetty-ee10-servlet-12.0.15.jar is missing in the license file warning: jakarta.ws.rs-api-3.1.0.jar is missing in the license file warning: snakeyaml-2.2.jar is missing in the license file warning: jakarta.validation-api-3.0.2.jar is mi","output":"Update the LICENSE-binary","metadata":{"issue_id":"KAFKA-18375","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:50:27.000+0000","updated":"2025-01-01T12:55:15.000+0000","resolved":"2025-01-01T12:55:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908995","author":{"displayName":"kangning.li"},"body":"chia7712 If you have not started working on this, I will handle it. :)","body_raw":"[~chia7712]   If you have not started working on this, I will handle it.   :)","created":"2024-12-31T00:56:51.063+0000","updated":"2024-12-31T00:56:51.063+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909130","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da 4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","body_raw":"trunk: https://github.com/apache/kafka/commit/ce5bd197c163c290635fa9db38e3f90bcd18e9da\r\n\r\n4.0: https://github.com/apache/kafka/commit/0e2b562427064de3b3f366de72815088a4768e47","created":"2025-01-01T12:55:15.105+0000","updated":"2025-01-01T12:55:15.105+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove EncryptingPasswordEncoder\n\nIt is no longer used","output":"Resolved","metadata":{"issue_id":"KAFKA-18374","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:21:16.000+0000","updated":"2025-01-06T19:52:22.000+0000","resolved":"2025-01-06T19:52:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908909","author":{"displayName":"Mingdao Yang"},"body":"chia7712 I’d like to work on it. Thank you!","body_raw":"[~chia7712] I’d like to work on it. Thank you!","created":"2024-12-30T16:26:15.451+0000","updated":"2024-12-30T16:26:15.451+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17910380","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","body_raw":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5\r\n\r\n4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","created":"2025-01-06T19:52:22.265+0000","updated":"2025-01-06T19:52:22.265+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove EncryptingPasswordEncoder\n\nIt is no longer used","output":"Major","metadata":{"issue_id":"KAFKA-18374","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:21:16.000+0000","updated":"2025-01-06T19:52:22.000+0000","resolved":"2025-01-06T19:52:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908909","author":{"displayName":"Mingdao Yang"},"body":"chia7712 I’d like to work on it. Thank you!","body_raw":"[~chia7712] I’d like to work on it. Thank you!","created":"2024-12-30T16:26:15.451+0000","updated":"2024-12-30T16:26:15.451+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17910380","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","body_raw":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5\r\n\r\n4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","created":"2025-01-06T19:52:22.265+0000","updated":"2025-01-06T19:52:22.265+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove EncryptingPasswordEncoder\n\nIt is no longer used","output":"It is no longer used","metadata":{"issue_id":"KAFKA-18374","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:21:16.000+0000","updated":"2025-01-06T19:52:22.000+0000","resolved":"2025-01-06T19:52:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908909","author":{"displayName":"Mingdao Yang"},"body":"chia7712 I’d like to work on it. Thank you!","body_raw":"[~chia7712] I’d like to work on it. Thank you!","created":"2024-12-30T16:26:15.451+0000","updated":"2024-12-30T16:26:15.451+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17910380","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","body_raw":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5\r\n\r\n4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","created":"2025-01-06T19:52:22.265+0000","updated":"2025-01-06T19:52:22.265+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove EncryptingPasswordEncoder\n\nIt is no longer used","output":"Fixed","metadata":{"issue_id":"KAFKA-18374","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:21:16.000+0000","updated":"2025-01-06T19:52:22.000+0000","resolved":"2025-01-06T19:52:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908909","author":{"displayName":"Mingdao Yang"},"body":"chia7712 I’d like to work on it. Thank you!","body_raw":"[~chia7712] I’d like to work on it. Thank you!","created":"2024-12-30T16:26:15.451+0000","updated":"2024-12-30T16:26:15.451+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17910380","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","body_raw":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5\r\n\r\n4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","created":"2025-01-06T19:52:22.265+0000","updated":"2025-01-06T19:52:22.265+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove EncryptingPasswordEncoder\n\nIt is no longer used\n\nConversation:\nuser: chia7712 I’d like to work on it. Thank you!\nassistant: trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","output":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","metadata":{"issue_id":"KAFKA-18374","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Mingdao Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-30T16:21:16.000+0000","updated":"2025-01-06T19:52:22.000+0000","resolved":"2025-01-06T19:52:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908909","author":{"displayName":"Mingdao Yang"},"body":"chia7712 I’d like to work on it. Thank you!","body_raw":"[~chia7712] I’d like to work on it. Thank you!","created":"2024-12-30T16:26:15.451+0000","updated":"2024-12-30T16:26:15.451+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17910380","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5 4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","body_raw":"trunk: https://github.com/apache/kafka/commit/23e77ed2d44b53deb968231345a0888f86f8ccb5\r\n\r\n4.0: https://github.com/apache/kafka/commit/01dcba56616d9cf4056c48e21f051822e37bcc5d","created":"2025-01-06T19:52:22.265+0000","updated":"2025-01-06T19:52:22.265+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkMetadataCache\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18373","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-30T15:21:32.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-17T11:01:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914410","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","body_raw":"Sorry for [~yangpoan], `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient ","created":"2025-01-19T02:05:04.062+0000","updated":"2025-01-19T02:05:04.062+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkMetadataCache\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18373","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-30T15:21:32.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-17T11:01:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914410","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","body_raw":"Sorry for [~yangpoan], `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient ","created":"2025-01-19T02:05:04.062+0000","updated":"2025-01-19T02:05:04.062+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkMetadataCache\n\n","output":"","metadata":{"issue_id":"KAFKA-18373","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-30T15:21:32.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-17T11:01:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914410","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","body_raw":"Sorry for [~yangpoan], `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient ","created":"2025-01-19T02:05:04.062+0000","updated":"2025-01-19T02:05:04.062+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkMetadataCache\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18373","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-30T15:21:32.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-17T11:01:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914410","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","body_raw":"Sorry for [~yangpoan], `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient ","created":"2025-01-19T02:05:04.062+0000","updated":"2025-01-19T02:05:04.062+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkMetadataCache\n\n\n\nConversation:\nuser: Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","output":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","metadata":{"issue_id":"KAFKA-18373","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-30T15:21:32.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-17T11:01:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914410","author":{"displayName":"黃竣陽"},"body":"Sorry for yangpoan, `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient","body_raw":"Sorry for [~yangpoan], `ZkFinalizedFeatureCache` should containe in this Jira, but I open another Jira for it, make the process inconvenient ","created":"2025-01-19T02:05:04.062+0000","updated":"2025-01-19T02:05:04.062+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zookeeper config in config_property.py\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18372","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-30T13:50:43.000+0000","updated":"2024-12-31T14:43:47.000+0000","resolved":"2024-12-31T14:43:47.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909071","author":{"displayName":"TaiJuWu"},"body":"Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.","body_raw":"Could we remove zk config in system test?\r\nThe system test include compatibility test and the old version need to set them.","created":"2024-12-31T14:38:56.736+0000","updated":"2024-12-31T14:38:56.736+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17909072","author":{"displayName":"黃竣陽"},"body":"You are right, we should test zk in e2e, I will close this JIra","body_raw":"You are right, we should test zk in e2e, I will close this JIra","created":"2024-12-31T14:43:34.034+0000","updated":"2024-12-31T14:43:34.034+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zookeeper config in config_property.py\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18372","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-30T13:50:43.000+0000","updated":"2024-12-31T14:43:47.000+0000","resolved":"2024-12-31T14:43:47.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909071","author":{"displayName":"TaiJuWu"},"body":"Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.","body_raw":"Could we remove zk config in system test?\r\nThe system test include compatibility test and the old version need to set them.","created":"2024-12-31T14:38:56.736+0000","updated":"2024-12-31T14:38:56.736+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17909072","author":{"displayName":"黃竣陽"},"body":"You are right, we should test zk in e2e, I will close this JIra","body_raw":"You are right, we should test zk in e2e, I will close this JIra","created":"2024-12-31T14:43:34.034+0000","updated":"2024-12-31T14:43:34.034+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zookeeper config in config_property.py\n\n","output":"","metadata":{"issue_id":"KAFKA-18372","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-30T13:50:43.000+0000","updated":"2024-12-31T14:43:47.000+0000","resolved":"2024-12-31T14:43:47.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909071","author":{"displayName":"TaiJuWu"},"body":"Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.","body_raw":"Could we remove zk config in system test?\r\nThe system test include compatibility test and the old version need to set them.","created":"2024-12-31T14:38:56.736+0000","updated":"2024-12-31T14:38:56.736+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17909072","author":{"displayName":"黃竣陽"},"body":"You are right, we should test zk in e2e, I will close this JIra","body_raw":"You are right, we should test zk in e2e, I will close this JIra","created":"2024-12-31T14:43:34.034+0000","updated":"2024-12-31T14:43:34.034+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zookeeper config in config_property.py\n\n","output":"Won't Fix","metadata":{"issue_id":"KAFKA-18372","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-30T13:50:43.000+0000","updated":"2024-12-31T14:43:47.000+0000","resolved":"2024-12-31T14:43:47.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909071","author":{"displayName":"TaiJuWu"},"body":"Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.","body_raw":"Could we remove zk config in system test?\r\nThe system test include compatibility test and the old version need to set them.","created":"2024-12-31T14:38:56.736+0000","updated":"2024-12-31T14:38:56.736+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17909072","author":{"displayName":"黃竣陽"},"body":"You are right, we should test zk in e2e, I will close this JIra","body_raw":"You are right, we should test zk in e2e, I will close this JIra","created":"2024-12-31T14:43:34.034+0000","updated":"2024-12-31T14:43:34.034+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zookeeper config in config_property.py\n\n\n\nConversation:\nuser: Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.\nassistant: You are right, we should test zk in e2e, I will close this JIra","output":"You are right, we should test zk in e2e, I will close this JIra","metadata":{"issue_id":"KAFKA-18372","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-30T13:50:43.000+0000","updated":"2024-12-31T14:43:47.000+0000","resolved":"2024-12-31T14:43:47.000+0000","resolution":"Won't Fix","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17909071","author":{"displayName":"TaiJuWu"},"body":"Could we remove zk config in system test? The system test include compatibility test and the old version need to set them.","body_raw":"Could we remove zk config in system test?\r\nThe system test include compatibility test and the old version need to set them.","created":"2024-12-31T14:38:56.736+0000","updated":"2024-12-31T14:38:56.736+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17909072","author":{"displayName":"黃竣陽"},"body":"You are right, we should test zk in e2e, I will close this JIra","body_raw":"You are right, we should test zk in e2e, I will close this JIra","created":"2024-12-31T14:43:34.034+0000","updated":"2024-12-31T14:43:34.034+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstrap.servers=server1:9094, security.protocol=SASL_SSL, password=CLEARTEXT, ssl.truststore.location=/etc/pki/java/cacerts, ssl.keystore.password=CLEARTEXT, sasl.mechanism=SCRAM-SHA-512, ssl.key.password=CLEARTEXT, sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"username\" password=\"CLEARTEXT\";, ssl.truststore.password=CLEARTEXT, …{code} Issue is related to using toString() method of TopicBasedRemoteLogMetadataManagerConfig, that prints maps of consumerProps and producerProps without masking. Current workaround: logger for class TopicBasedRemoteLogMetadataManagerConfig can be disabled to not expose sensitive data. Expected behavior: sensitive configuration data masked automatically in logs.","output":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstra","output":"Resolved","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstra","output":"Major","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstra","output":"{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFacto","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstra","output":"Fixed","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs\n\n{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopic\n\nConversation:\nuser: Possible fix is available here: https://github.com/apache/kafka/pull/18349","output":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"{code:java} [2024-12-20 14:52:56,805] INFO Successfully configured topic-based RLMM with config: TopicBasedRemoteLogMetadataManagerConfig{clientIdPrefix='__remote_log_metadata_client_6', metadataTopicPartitionsCount=50, consumeWaitMs=120000, metadataTopicRetentionMs=-1, metadataTopicReplicationFactor=3, initializationRetryMaxTimeoutMs=120000, initializationRetryIntervalMs=100, commonProps={request.timeout.ms=10000, ssl.client.auth=none, ssl.keystore.location=/etc/kafka/ssl/keystore.p12, bootstrap.servers=server1:9094, security.protocol=SASL_SSL, password=CLEARTEXT, ssl.truststore.location=/etc/pki/java/cacerts, ssl.keystore.password=CLEARTEXT, sasl.mechanism=SCRAM-SHA-512, ssl.key.password=CLEARTEXT, sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username","output":"TopicBasedRemoteLogMetadataManagerConfig exposes sensitive configuration data in logs","metadata":{"issue_id":"KAFKA-18371","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Vadym Zhytkevych"},"reporter":{"displayName":"Vadym Zhytkevych"},"created":"2024-12-30T08:42:51.000+0000","updated":"2025-02-27T22:35:52.000+0000","resolved":"2025-02-27T22:35:46.000+0000","resolution":"Fixed","labels":["storage","tiered-storage"],"components":["core"],"comment_count":1,"comments":[{"id":"17908809","author":{"displayName":"Vadym Zhytkevych"},"body":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","body_raw":"Possible fix is available here: https://github.com/apache/kafka/pull/18349","created":"2024-12-30T08:53:37.271+0000","updated":"2024-12-30T08:53:37.271+0000","updateAuthor":{"displayName":"Vadym Zhytkevych"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ReplicaFetcherThreadTest\n\nAs title","output":"Resolved","metadata":{"issue_id":"KAFKA-18370","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-30T02:16:40.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-21T14:05:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912029","author":{"displayName":"TaiJuWu"},"body":"These tests will be removed after 4.0","body_raw":"These tests will be removed after 4.0","created":"2025-01-10T16:21:29.409+0000","updated":"2025-01-10T16:21:42.347+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17914983","author":{"displayName":"David Jacot"},"body":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","body_raw":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","created":"2025-01-21T13:49:50.667+0000","updated":"2025-01-21T13:49:50.667+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ReplicaFetcherThreadTest\n\nAs title","output":"Major","metadata":{"issue_id":"KAFKA-18370","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-30T02:16:40.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-21T14:05:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912029","author":{"displayName":"TaiJuWu"},"body":"These tests will be removed after 4.0","body_raw":"These tests will be removed after 4.0","created":"2025-01-10T16:21:29.409+0000","updated":"2025-01-10T16:21:42.347+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17914983","author":{"displayName":"David Jacot"},"body":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","body_raw":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","created":"2025-01-21T13:49:50.667+0000","updated":"2025-01-21T13:49:50.667+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix to Kraft or remove tests associate with Zk Broker config in ReplicaFetcherThreadTest\n\nAs title","output":"As title","metadata":{"issue_id":"KAFKA-18370","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-30T02:16:40.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-21T14:05:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912029","author":{"displayName":"TaiJuWu"},"body":"These tests will be removed after 4.0","body_raw":"These tests will be removed after 4.0","created":"2025-01-10T16:21:29.409+0000","updated":"2025-01-10T16:21:42.347+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17914983","author":{"displayName":"David Jacot"},"body":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","body_raw":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","created":"2025-01-21T13:49:50.667+0000","updated":"2025-01-21T13:49:50.667+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ReplicaFetcherThreadTest\n\nAs title","output":"Fixed","metadata":{"issue_id":"KAFKA-18370","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-30T02:16:40.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-21T14:05:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912029","author":{"displayName":"TaiJuWu"},"body":"These tests will be removed after 4.0","body_raw":"These tests will be removed after 4.0","created":"2025-01-10T16:21:29.409+0000","updated":"2025-01-10T16:21:42.347+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17914983","author":{"displayName":"David Jacot"},"body":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","body_raw":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","created":"2025-01-21T13:49:50.667+0000","updated":"2025-01-21T13:49:50.667+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ReplicaFetcherThreadTest\n\nAs title\n\nConversation:\nuser: These tests will be removed after 4.0\nassistant: Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","output":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","metadata":{"issue_id":"KAFKA-18370","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-30T02:16:40.000+0000","updated":"2025-01-21T14:05:32.000+0000","resolved":"2025-01-21T14:05:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17912029","author":{"displayName":"TaiJuWu"},"body":"These tests will be removed after 4.0","body_raw":"These tests will be removed after 4.0","created":"2025-01-10T16:21:29.409+0000","updated":"2025-01-10T16:21:42.347+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17914983","author":{"displayName":"David Jacot"},"body":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","body_raw":"Changed the fix version to 4.1 as it seems that we don't want to do this in 4.0.","created":"2025-01-21T13:49:50.667+0000","updated":"2025-01-21T13:49:50.667+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"State updater's *-ratio metrics are incorrect\n\nh2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1101-L1115] These metrics are averages, that are supposed to indicate \"The fraction of time the thread spent on \\{action}\". But the metrics don't actually do that. h2. Issue Let me explain this with an example: For simplicity's sake, let's consider the following example involving just {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} and ignoring the existence of the other two metrics. Let's say the thread did: * {{999}} iterations with {{standby-update}} taking {{1ms}} in each iteration and no {{checkpoint}} happening ({{{}0ms{}}}). * {{1}} iteration with {{standby-update}} taking {{1ms}} and {{checkpoint}} taking {{9000ms}} The thread spent {{10s}} working, of which it spent {{1s}} on {{standby-updates}} and {{9s}} on checkpoint, so the fraction of time it spent on checkpoint (checkpoint-ratio) is {{{}~0.001 (0.1%){}}}. Or at least that is what the metrics will say. I would instead argue that it spent {{9s/10s == 0.9 == 90%}} on checkpoint. If you agree with my logic, then you agree that this metrics is incorrect. The problem is that the code computes a ratio for each iteration, and then averages those ratios out, producing a number devoid of statistical meaning or practical application. It ignores the fact that the one iteration that took {{{}9s{}}}, should have a much higher weight than those quick 1ms iterations. {{(999*(0ms/1ms) + 1*(9000ms/9001ms))/1000 ~= 0.001}} h2. Solution What we would like to see instead is either a ratio of average/total times, not an average of ratios. I don't think this can be easily realised within the existing metrics system. So instead, what I propose as a solution is to report {{duration-total}} and/or {{duration-rate}} (with the unit of seconds per second) metric for each of {{{}idle{}}}, {{{}active-restore{}}}, {{{}standby-restore{}}}, {{{}checkpoint{}}}. The observers of these metrics, when needed, could then derive the actual ratio of time spent on each operation for example as {{{}checkpoint-ratio = checkpoint-duration-rate / (idle-duration-rate + active-restore-duration-rate + standby-restore-duration-rate + checkpoint-duration-rate){}}}. Or by performing an analogical calculation on deltas of the {{total}} metrics. I can submit a PR once there's an agreement on the correct way to fix this.","output":"State updater's *-ratio metrics are incorrect","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"State updater's *-ratio metrics are incorrect\n\nh2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1101-L1115] These metrics are averages, that are supposed to indicate \"The fraction of time the thread spent on \\{action}\". But the metrics don't actually do that. h2. Issue Let me explain this with an","output":"Open","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"State updater's *-ratio metrics are incorrect\n\nh2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1101-L1115] These metrics are averages, that are supposed to indicate \"The fraction of time the thread spent on \\{action}\". But the metrics don't actually do that. h2. Issue Let me explain this with an","output":"Major","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: State updater's *-ratio metrics are incorrect\n\nh2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1101-L1115] These metrics are averages, that are supposed to indicate \"The fraction of time the thread spent on \\{action}\". But the metrics don't actually do that. h2. Issue Let me explain this with an","output":"h2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"State updater's *-ratio metrics are incorrect\n\nh2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/\n\nConversation:\nuser: Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `S\nassistant: Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, \nassistant: Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.\nassistant: Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might wa\nassistant: {quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","output":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"h2. Background {{DefaultStateUpdater}} defines {{{}idle-ratio{}}}, {{{}active-restore-ratio{}}}, {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} metrics here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java#L1101-L1115] These metrics are averages, that are supposed to indicate \"The fraction of time the thread spent on \\{action}\". But the metrics don't actually do that. h2. Issue Let me explain this with an example: For simplicity's sake, let's consider the following example involving just {{{}standby-update-ratio{}}}, {{checkpoint-ratio}} and ignoring the existence of the other two metrics. Let's say the thread did: * {{999}} iterations with {{standby-update}} taking {{1ms}} in each iteration and no ","output":"State updater's *-ratio metrics are incorrect","metadata":{"issue_id":"KAFKA-18369","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"Rafał Sumisławski"},"created":"2024-12-29T19:08:33.000+0000","updated":"2025-01-21T20:15:51.000+0000","resolved":null,"labels":[],"components":["streams"],"comment_count":8,"comments":[{"id":"17909259","author":{"displayName":"Lucas Brutschy"},"body":"Hi sumislawski. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed. Should we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228 If you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility In summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","body_raw":"Hi [~sumislawski]. I agree that something is off here. However, not sure (yet), we have to solve it by reporting different metrics. Note that metrics are part of the public interface of Kafka and thus require a KIP to be changed.\r\n\r\nShould we report an average here in the first place? If we look at the `StreamThread`, we report similar metrics, but do not use an average stat, but report the value directly:\r\n\r\nhttps://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java#L228\r\n\r\nIf you look at the KIP that introduced these metrics, there is no mention of averaging the ratios. https://cwiki.apache.org/confluence/display/KAFKA/KIP-869%3A+Improve+Streams+State+Restoration+Visibility\r\n\r\nIn summary, I agree that this is a bug, but I'd propose to fix it by removing the averaging.","created":"2025-01-02T10:12:52.752+0000","updated":"2025-01-02T10:12:52.752+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17909788","author":{"displayName":"Rafał Sumisławski"},"body":"Hi lucasbru IMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","body_raw":"Hi [~lucasbru] \r\n\r\nIMO changing it from average to value doesn't solve the problem. I was about to open a similar ticket about the `StreamThread` metrics, just didn't find the time yet. The explanation of the issue is different, but the end result is mostly the same. Let me explain this in the new ticket, and I'll link it here once it's open.","created":"2025-01-04T19:34:58.104+0000","updated":"2025-01-04T19:34:58.104+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17910037","author":{"displayName":"Lucas Brutschy"},"body":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","body_raw":"Sure, that makes sense. Ideally, the metrics for both threads report the numbers the same way.","created":"2025-01-06T08:04:08.708+0000","updated":"2025-01-06T08:04:08.708+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"17913528","author":{"displayName":"Matthias J. Sax"},"body":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums. Given that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config metric.sample.window.ms = 30sec [default]).","body_raw":"Sounds like we should compute the metric using a window, what is already supported... We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n\r\nGiven that the current metric reports garbage, I don't think we need to do a KIP to change this – of course, we might want to update the KIP and docs to point out that we report this window based (ie config  metric.sample.window.ms = 30sec [default]).","created":"2025-01-16T02:58:22.269+0000","updated":"2025-01-16T02:58:22.269+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17914788","author":{"displayName":"Rafał Sumisławski"},"body":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums. {quote} Exactly. I know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","body_raw":"{quote}We sum up the actually time-values inside the window, and compute the ratio base on the sums.\r\n{quote}\r\nExactly.\r\n\r\nI know that summing inside a window is supported, but do we have a way to compute a ratio of two such sums?","created":"2025-01-20T20:06:49.240+0000","updated":"2025-01-20T20:06:49.240+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915761","author":{"displayName":"Matthias J. Sax"},"body":"Well, if we have two summed window, we can just divide both to report the metric, right? We would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","body_raw":"Well, if we have two summed window, we can just divide both to report the metric, right?\r\n\r\nWe would need a KIP for this change though, as `org.apache.kafka.common.metrics.stats` is public API – but should not be too difficult to get done.","created":"2025-01-21T16:51:58.965+0000","updated":"2025-01-21T16:54:18.961+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17915789","author":{"displayName":"Rafał Sumisławski"},"body":"lucasbru I described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615 mjsax Yes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. Could you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","body_raw":"[~lucasbru] \r\nI described the similar yet different problem affecting `StreamThead` in a separate ticket as promised: https://issues.apache.org/jira/browse/KAFKA-18615\r\n\r\n[~mjsax] \r\nYes, we need to just divide the windowed sum metrics. While not exposing the windowed sums to the outside world. Just the result of the division. \r\n\r\nCould you elaborate on what kind of changes in `org.apache.kafka.common.metrics.stats` you have in mind? A new Stat that would be exactly what we need: A pair of internal windowed sums divided on read?","created":"2025-01-21T18:02:19.901+0000","updated":"2025-01-21T18:02:19.901+0000","updateAuthor":{"displayName":"Rafał Sumisławski"}},{"id":"17915826","author":{"displayName":"Matthias J. Sax"},"body":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) But yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","body_raw":"Did not think about it in detail... – Guess it's up to whoever picks-up this ticket to figure it out and make a proposal. :) \r\n\r\nBut yes, something link `WindowedAvg` I guess (which would internally re-use `WindowedSum`)?","created":"2025-01-21T20:15:51.741+0000","updated":"2025-01-21T20:15:51.741+0000","updateAuthor":{"displayName":"Matthias J. Sax"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove TestUtils#MockZkConnect and remove zkConnect from TestUtils#createBrokerConfig\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18368","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:34:25.000+0000","updated":"2025-01-07T13:07:31.000+0000","resolved":"2025-01-07T13:07:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910624","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","body_raw":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77\r\n\r\n4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","created":"2025-01-07T13:07:31.889+0000","updated":"2025-01-07T13:07:31.889+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove TestUtils#MockZkConnect and remove zkConnect from TestUtils#createBrokerConfig\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18368","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:34:25.000+0000","updated":"2025-01-07T13:07:31.000+0000","resolved":"2025-01-07T13:07:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910624","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","body_raw":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77\r\n\r\n4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","created":"2025-01-07T13:07:31.889+0000","updated":"2025-01-07T13:07:31.889+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove TestUtils#MockZkConnect and remove zkConnect from TestUtils#createBrokerConfig\n\n","output":"","metadata":{"issue_id":"KAFKA-18368","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:34:25.000+0000","updated":"2025-01-07T13:07:31.000+0000","resolved":"2025-01-07T13:07:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910624","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","body_raw":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77\r\n\r\n4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","created":"2025-01-07T13:07:31.889+0000","updated":"2025-01-07T13:07:31.889+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove TestUtils#MockZkConnect and remove zkConnect from TestUtils#createBrokerConfig\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18368","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:34:25.000+0000","updated":"2025-01-07T13:07:31.000+0000","resolved":"2025-01-07T13:07:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910624","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","body_raw":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77\r\n\r\n4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","created":"2025-01-07T13:07:31.889+0000","updated":"2025-01-07T13:07:31.889+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove TestUtils#MockZkConnect and remove zkConnect from TestUtils#createBrokerConfig\n\n\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","output":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","metadata":{"issue_id":"KAFKA-18368","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:34:25.000+0000","updated":"2025-01-07T13:07:31.000+0000","resolved":"2025-01-07T13:07:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17910624","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77 4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","body_raw":"trunk: https://github.com/apache/kafka/commit/d874aa42f3d54f1474f2e617c792e48ed01f6a77\r\n\r\n4.0: https://github.com/apache/kafka/commit/3061e9ca14ffaade56785bc1f49ecbbe9689e9de","created":"2025-01-07T13:07:31.889+0000","updated":"2025-01-07T13:07:31.889+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove ZkConfigManager\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18367","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:21:57.000+0000","updated":"2025-01-01T13:20:29.000+0000","resolved":"2025-01-01T13:20:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908732","author":{"displayName":"kangning.li"},"body":"chia7712 cloud you assign it to me?","body_raw":"[~chia7712] cloud you assign it to me?","created":"2024-12-29T18:25:08.617+0000","updated":"2024-12-29T18:25:08.617+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909133","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","body_raw":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","created":"2025-01-01T13:20:29.396+0000","updated":"2025-01-01T13:20:29.396+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove ZkConfigManager\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18367","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:21:57.000+0000","updated":"2025-01-01T13:20:29.000+0000","resolved":"2025-01-01T13:20:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908732","author":{"displayName":"kangning.li"},"body":"chia7712 cloud you assign it to me?","body_raw":"[~chia7712] cloud you assign it to me?","created":"2024-12-29T18:25:08.617+0000","updated":"2024-12-29T18:25:08.617+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909133","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","body_raw":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","created":"2025-01-01T13:20:29.396+0000","updated":"2025-01-01T13:20:29.396+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove ZkConfigManager\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18367","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:21:57.000+0000","updated":"2025-01-01T13:20:29.000+0000","resolved":"2025-01-01T13:20:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908732","author":{"displayName":"kangning.li"},"body":"chia7712 cloud you assign it to me?","body_raw":"[~chia7712] cloud you assign it to me?","created":"2024-12-29T18:25:08.617+0000","updated":"2024-12-29T18:25:08.617+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909133","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","body_raw":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","created":"2025-01-01T13:20:29.396+0000","updated":"2025-01-01T13:20:29.396+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove ZkConfigManager\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18367","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:21:57.000+0000","updated":"2025-01-01T13:20:29.000+0000","resolved":"2025-01-01T13:20:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908732","author":{"displayName":"kangning.li"},"body":"chia7712 cloud you assign it to me?","body_raw":"[~chia7712] cloud you assign it to me?","created":"2024-12-29T18:25:08.617+0000","updated":"2024-12-29T18:25:08.617+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909133","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","body_raw":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","created":"2025-01-01T13:20:29.396+0000","updated":"2025-01-01T13:20:29.396+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove ZkConfigManager\n\nas title\n\nConversation:\nuser: chia7712 cloud you assign it to me?\nassistant: trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","output":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","metadata":{"issue_id":"KAFKA-18367","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T18:21:57.000+0000","updated":"2025-01-01T13:20:29.000+0000","resolved":"2025-01-01T13:20:29.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908732","author":{"displayName":"kangning.li"},"body":"chia7712 cloud you assign it to me?","body_raw":"[~chia7712] cloud you assign it to me?","created":"2024-12-29T18:25:08.617+0000","updated":"2024-12-29T18:25:08.617+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17909133","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e 4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","body_raw":"trunk: https://github.com/apache/kafka/commit/dbf4602c13bffa813506074ae1e9884de015086e\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a9fb4cfe01cb9d341df2fd51f474e2b4e6f3963","created":"2025-01-01T13:20:29.396+0000","updated":"2025-01-01T13:20:29.396+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it","output":"Remove KafkaConfig.interBrokerProtocolVersion","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it","output":"Resolved","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it","output":"Blocker","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it","output":"This config is only used in Zookeeper and we should remove it","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it","output":"Fixed","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove KafkaConfig.interBrokerProtocolVersion\n\nThis config is only used in Zookeeper and we should remove it\n\nConversation:\nuser: Hey m1a2st, are you working on this issue or can I take over it?\nassistant: Sorry, I am working it.\nassistant: This being tackled via KAFKA-18360\nassistant: reopen as follow-up of https://github.com/apache/kafka/pull/18566\nassistant: trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","output":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","metadata":{"issue_id":"KAFKA-18366","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:21:50.000+0000","updated":"2025-02-10T23:31:49.000+0000","resolved":"2025-02-10T23:31:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":5,"comments":[{"id":"17913994","author":{"displayName":"Ukpa Uchechi"},"body":"Hey m1a2st, are you working on this issue or can I take over it?","body_raw":"Hey [~m1a2st], are you working on this issue or can I take over it? ","created":"2025-01-17T08:17:05.408+0000","updated":"2025-01-17T08:17:05.408+0000","updateAuthor":{"displayName":"Ukpa Uchechi"}},{"id":"17914007","author":{"displayName":"黃竣陽"},"body":"Sorry, I am working it.","body_raw":"Sorry, I am working it.","created":"2025-01-17T08:43:22.981+0000","updated":"2025-01-17T08:43:22.981+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17914475","author":{"displayName":"Ismael Juma"},"body":"This being tackled via KAFKA-18360","body_raw":"This being tackled via KAFKA-18360","created":"2025-01-19T17:24:17.002+0000","updated":"2025-01-19T17:24:17.002+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924187","author":{"displayName":"Chia-Ping Tsai"},"body":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","body_raw":"reopen as follow-up of https://github.com/apache/kafka/pull/18566","created":"2025-02-05T17:45:11.322+0000","updated":"2025-02-05T17:45:11.322+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17925777","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563] 4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","body_raw":"trunk: [https://github.com/apache/kafka/commit/581e94840fd0503d46c36eb876fa80344b131563]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/8e8423fed0358fdfe6baaa483c2c2bccb410b23c","created":"2025-02-10T23:31:49.940+0000","updated":"2025-02-10T23:31:49.940+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zookeeper.connect in Test\n\nas title","output":"Resolved","metadata":{"issue_id":"KAFKA-18365","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:13:17.000+0000","updated":"2025-01-04T15:19:07.000+0000","resolved":"2025-01-04T15:19:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909764","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","body_raw":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f\r\n4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","created":"2025-01-04T15:19:07.702+0000","updated":"2025-01-04T15:19:07.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zookeeper.connect in Test\n\nas title","output":"Major","metadata":{"issue_id":"KAFKA-18365","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:13:17.000+0000","updated":"2025-01-04T15:19:07.000+0000","resolved":"2025-01-04T15:19:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909764","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","body_raw":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f\r\n4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","created":"2025-01-04T15:19:07.702+0000","updated":"2025-01-04T15:19:07.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zookeeper.connect in Test\n\nas title","output":"as title","metadata":{"issue_id":"KAFKA-18365","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:13:17.000+0000","updated":"2025-01-04T15:19:07.000+0000","resolved":"2025-01-04T15:19:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909764","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","body_raw":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f\r\n4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","created":"2025-01-04T15:19:07.702+0000","updated":"2025-01-04T15:19:07.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zookeeper.connect in Test\n\nas title","output":"Fixed","metadata":{"issue_id":"KAFKA-18365","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:13:17.000+0000","updated":"2025-01-04T15:19:07.000+0000","resolved":"2025-01-04T15:19:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909764","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","body_raw":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f\r\n4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","created":"2025-01-04T15:19:07.702+0000","updated":"2025-01-04T15:19:07.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zookeeper.connect in Test\n\nas title\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","output":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","metadata":{"issue_id":"KAFKA-18365","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T12:13:17.000+0000","updated":"2025-01-04T15:19:07.000+0000","resolved":"2025-01-04T15:19:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909764","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f 4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","body_raw":"trunk: https://github.com/apache/kafka/commit/a628d9bc4d82ddb63ca00f7bcd88a6a6788eae7f\r\n4.0: https://github.com/apache/kafka/commit/e29f0b7e265045b776cf9fd1c5820cdc94f4a54b","created":"2025-01-04T15:19:07.702+0000","updated":"2025-01-04T15:19:07.702+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"migrating from zk to kraft document","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"Resolved","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"Major","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"see [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"Fixed","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"migrating from zk to kraft document\n\nsee [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.\n\nConversation:\nuser: another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094\nassistant: trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","output":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"see [https://github.com/apache/kafka/pull/18329#discussion_r1898948528] We should have a single page to describe how to migrate from zk to Kraft and related change.","output":"migrating from zk to kraft document","metadata":{"issue_id":"KAFKA-18364","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T12:11:35.000+0000","updated":"2025-02-19T08:46:35.000+0000","resolved":"2025-01-15T21:02:55.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17910374","author":{"displayName":"Chia-Ping Tsai"},"body":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","body_raw":"another related removed configs: https://github.com/apache/kafka/pull/18384#discussion_r1904497094","created":"2025-01-06T19:01:05.607+0000","updated":"2025-01-06T19:01:05.607+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17913450","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d 4.0: https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","body_raw":"trunk:https://github.com/apache/kafka/commit/d82f03e98b72689f0619c7129decd28fa2b8951d\r\n\r\n \r\n\r\n4.0:  https://github.com/chia7712/kafka/commit/e206d51591681f358b71727118ba05ba7da5bbb4","created":"2025-01-15T21:02:55.733+0000","updated":"2025-01-15T21:02:55.733+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Clean up Server config documents which contained zookeeper\n\nWe remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"Clean up Server config documents which contained zookeeper","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Clean up Server config documents which contained zookeeper\n\nWe remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"Resolved","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Clean up Server config documents which contained zookeeper\n\nWe remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"Major","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Clean up Server config documents which contained zookeeper\n\nWe remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"We remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Clean up Server config documents which contained zookeeper\n\nWe remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"Fixed","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We remove zk in Kafka 4.0, we should check all server config which don't need to compare kraft and zookeeper.","output":"Clean up Server config documents which contained zookeeper","metadata":{"issue_id":"KAFKA-18363","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T11:56:13.000+0000","updated":"2025-01-26T02:01:34.000+0000","resolved":"2025-01-20T17:30:58.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove dataPlane prefix in SocketServer\n\nsee https://github.com/apache/kafka/pull/18329/files#r1898948164","output":"Remove dataPlane prefix in SocketServer","metadata":{"issue_id":"KAFKA-18362","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T11:28:16.000+0000","updated":"2024-12-30T11:05:53.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908835","author":{"displayName":"kangning.li"},"body":"hi taijuwu , I will hand this issue. :)","body_raw":"hi  [~taijuwu] , I will hand this issue.  :)","created":"2024-12-30T11:05:53.958+0000","updated":"2024-12-30T11:05:53.958+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove dataPlane prefix in SocketServer\n\nsee https://github.com/apache/kafka/pull/18329/files#r1898948164","output":"Open","metadata":{"issue_id":"KAFKA-18362","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T11:28:16.000+0000","updated":"2024-12-30T11:05:53.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908835","author":{"displayName":"kangning.li"},"body":"hi taijuwu , I will hand this issue. :)","body_raw":"hi  [~taijuwu] , I will hand this issue.  :)","created":"2024-12-30T11:05:53.958+0000","updated":"2024-12-30T11:05:53.958+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove dataPlane prefix in SocketServer\n\nsee https://github.com/apache/kafka/pull/18329/files#r1898948164","output":"Minor","metadata":{"issue_id":"KAFKA-18362","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T11:28:16.000+0000","updated":"2024-12-30T11:05:53.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908835","author":{"displayName":"kangning.li"},"body":"hi taijuwu , I will hand this issue. :)","body_raw":"hi  [~taijuwu] , I will hand this issue.  :)","created":"2024-12-30T11:05:53.958+0000","updated":"2024-12-30T11:05:53.958+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove dataPlane prefix in SocketServer\n\nsee https://github.com/apache/kafka/pull/18329/files#r1898948164","output":"see https://github.com/apache/kafka/pull/18329/files#r1898948164","metadata":{"issue_id":"KAFKA-18362","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T11:28:16.000+0000","updated":"2024-12-30T11:05:53.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908835","author":{"displayName":"kangning.li"},"body":"hi taijuwu , I will hand this issue. :)","body_raw":"hi  [~taijuwu] , I will hand this issue.  :)","created":"2024-12-30T11:05:53.958+0000","updated":"2024-12-30T11:05:53.958+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove dataPlane prefix in SocketServer\n\nsee https://github.com/apache/kafka/pull/18329/files#r1898948164\n\nConversation:\nuser: hi taijuwu , I will hand this issue. :)","output":"hi taijuwu , I will hand this issue. :)","metadata":{"issue_id":"KAFKA-18362","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Minor","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-29T11:28:16.000+0000","updated":"2024-12-30T11:05:53.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908835","author":{"displayName":"kangning.li"},"body":"hi taijuwu , I will hand this issue. :)","body_raw":"hi  [~taijuwu] , I will hand this issue.  :)","created":"2024-12-30T11:05:53.958+0000","updated":"2024-12-30T11:05:53.958+0000","updateAuthor":{"displayName":"kangning.li"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records","output":"Remove PasswordEncoderConfigs","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records","output":"Resolved","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records","output":"Major","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records","output":"in kfrat mode, we do not encrypt the values in records","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records","output":"Fixed","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove PasswordEncoderConfigs\n\nin kfrat mode, we do not encrypt the values in records\n\nConversation:\nuser: Hi chia7712 I would like to take over this issue :)\nassistant: trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","output":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","metadata":{"issue_id":"KAFKA-18361","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-29T10:36:26.000+0000","updated":"2024-12-30T17:33:02.000+0000","resolved":"2024-12-30T16:22:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908691","author":{"displayName":"TengYao Chi"},"body":"Hi chia7712 I would like to take over this issue :)","body_raw":"Hi [~chia7712] \r\n\r\nI would like to take over this issue :)","created":"2024-12-29T10:37:30.500+0000","updated":"2024-12-29T10:37:30.500+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17908908","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456 4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","body_raw":"trunk: https://github.com/apache/kafka/commit/3161115adae4de9f2b2ec6b87462ea10a8993456\r\n\r\n4.0: https://github.com/apache/kafka/commit/3575b41ba6a89d78f6b07c1c2950113ba61dbe82","created":"2024-12-30T16:22:33.529+0000","updated":"2024-12-30T16:22:33.529+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Remove zookeeper configurations","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Resolved","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Blocker","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Remove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Fixed","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zookeeper configurations\n\nRemove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","output":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Remove all config in [https://github.com/apache/kafka/blob/trunk/server/src/main/java/org/apache/kafka/server/config/ZkConfigs.java]","output":"Remove zookeeper configurations","metadata":{"issue_id":"KAFKA-18360","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Colin McCabe"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:45:51.000+0000","updated":"2025-02-06T14:25:30.000+0000","resolved":"2025-02-06T14:25:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17924570","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21 4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","body_raw":"trunk: https://github.com/apache/kafka/commit/b2b24086926b0affea0bf66de55bdc29b8663a21\r\n\r\n4.0: https://github.com/apache/kafka/commit/b6e6a3c68a2f87953317ed72f8f5fb17be8e6a44","created":"2025-02-06T14:25:30.055+0000","updated":"2025-02-06T14:25:30.055+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Set zkConnect to null in LocalLeaderEndPointTest, HighwatermarkPersistenceTest, IsrExpirationTest, ReplicaManagerQuotasTest, OffsetsForLeaderEpochTest\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18359","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:22:03.000+0000","updated":"2024-12-29T10:27:40.000+0000","resolved":"2024-12-29T10:27:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","body_raw":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824\r\n\r\n4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","created":"2024-12-29T10:27:40.427+0000","updated":"2024-12-29T10:27:40.427+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Set zkConnect to null in LocalLeaderEndPointTest, HighwatermarkPersistenceTest, IsrExpirationTest, ReplicaManagerQuotasTest, OffsetsForLeaderEpochTest\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18359","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:22:03.000+0000","updated":"2024-12-29T10:27:40.000+0000","resolved":"2024-12-29T10:27:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","body_raw":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824\r\n\r\n4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","created":"2024-12-29T10:27:40.427+0000","updated":"2024-12-29T10:27:40.427+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Set zkConnect to null in LocalLeaderEndPointTest, HighwatermarkPersistenceTest, IsrExpirationTest, ReplicaManagerQuotasTest, OffsetsForLeaderEpochTest\n\n","output":"","metadata":{"issue_id":"KAFKA-18359","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:22:03.000+0000","updated":"2024-12-29T10:27:40.000+0000","resolved":"2024-12-29T10:27:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","body_raw":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824\r\n\r\n4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","created":"2024-12-29T10:27:40.427+0000","updated":"2024-12-29T10:27:40.427+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Set zkConnect to null in LocalLeaderEndPointTest, HighwatermarkPersistenceTest, IsrExpirationTest, ReplicaManagerQuotasTest, OffsetsForLeaderEpochTest\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18359","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:22:03.000+0000","updated":"2024-12-29T10:27:40.000+0000","resolved":"2024-12-29T10:27:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","body_raw":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824\r\n\r\n4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","created":"2024-12-29T10:27:40.427+0000","updated":"2024-12-29T10:27:40.427+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Set zkConnect to null in LocalLeaderEndPointTest, HighwatermarkPersistenceTest, IsrExpirationTest, ReplicaManagerQuotasTest, OffsetsForLeaderEpochTest\n\n\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","output":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","metadata":{"issue_id":"KAFKA-18359","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-29T02:22:03.000+0000","updated":"2024-12-29T10:27:40.000+0000","resolved":"2024-12-29T10:27:40.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908689","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824 4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","body_raw":"trunk: https://github.com/apache/kafka/commit/91a2b58616d115f8d6ff4b71d6191a6dd9a17824\r\n\r\n4.0: https://github.com/apache/kafka/commit/2a391e4823e369ba4a92ac96b333fa7320dec981","created":"2024-12-29T10:27:40.427+0000","updated":"2024-12-29T10:27:40.427+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Replace Deprecated $buildDir variable in build.gradle\n\nThe $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"Replace Deprecated $buildDir variable in build.gradle","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Replace Deprecated $buildDir variable in build.gradle\n\nThe $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"Resolved","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Replace Deprecated $buildDir variable in build.gradle\n\nThe $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"Minor","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Replace Deprecated $buildDir variable in build.gradle\n\nThe $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"The $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Replace Deprecated $buildDir variable in build.gradle\n\nThe $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"Fixed","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The $buildDir variable is deprecated in build.gradle, we can replace it to ` layout.buildDirectory.get().asFile`, see the gradle upgrade document https://docs.gradle.org/current/userguide/upgrading_version_8.html#project_builddir","output":"Replace Deprecated $buildDir variable in build.gradle","metadata":{"issue_id":"KAFKA-18358","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-29T02:03:12.000+0000","updated":"2025-01-05T13:19:23.000+0000","resolved":"2025-01-05T13:19:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Support to dynamically configure the log level for JUL and logback\n\nWe currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","output":"Support to dynamically configure the log level for JUL and logback","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Support to dynamically configure the log level for JUL and logback\n\nWe currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","output":"Open","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Support to dynamically configure the log level for JUL and logback\n\nWe currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","output":"Major","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Support to dynamically configure the log level for JUL and logback\n\nWe currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","output":"We currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Support to dynamically configure the log level for JUL and logback\n\nWe currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other p\n\nConversation:\nuser: I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","output":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We currently support dynamic log level configuration only when the server is run with Log4j, which is our official logging implementation by default. However, it would be beneficial to support other popular SLF4J providers to further consolidate Kafka's logging functionality.","output":"Support to dynamically configure the log level for JUL and logback","metadata":{"issue_id":"KAFKA-18357","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Chia-Ping Tsai"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T14:49:44.000+0000","updated":"2024-12-28T16:48:28.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908624","author":{"displayName":"Ismael Juma"},"body":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","body_raw":"I don't think we should implement this ourselves. If there is a project that tackles that, we can use it. But it's not something that we should have to maintain, in my opinion. As far as I know, it hasn't been requested by users at all so far.","created":"2024-12-28T16:48:28.487+0000","updated":"2024-12-28T16:48:28.487+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Explicitly setting up instrumentation for inline mocking (Java 21+)\n\nWhen I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.4/58e850dde88f3cf20f41f659440bef33f6c4fe02/byte-buddy-agent-1.15.4.jar) WARNING: If a serviceability tool is in use, please run with -XX:+EnableDynamicAgentLoading to hide this warning WARNING: If a serviceability tool is not in use, please run with -Djdk.instrument.traceUsage for more information WARNING: Dynamic loading of agents will be disallowed by default in a future release {code}","output":"Explicitly setting up instrumentation for inline mocking (Java 21+)","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Explicitly setting up instrumentation for inline mocking (Java 21+)\n\nWhen I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradl","output":"Resolved","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Explicitly setting up instrumentation for inline mocking (Java 21+)\n\nWhen I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradl","output":"Major","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Explicitly setting up instrumentation for inline mocking (Java 21+)\n\nWhen I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradl","output":"When I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your buil","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Explicitly setting up instrumentation for inline mocking (Java 21+)\n\nWhen I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradl","output":"Fixed","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"When I run test in Java 21, I find there are some warning logs, we should imporve mokito to resolve these warning logs. {code:java} Mockito is currently self-attaching to enable the inline-mock-maker. This will no longer work in future releases of the JDK. Please add Mockito as an agent to your build what is described in Mockito's documentation: https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.3 WARNING: A Java agent has been loaded dynamically (/Users/ken/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.4/58e850dde88f3cf20f41f659440bef33f6c4fe02/byte-buddy-agent-1.15.4.jar) WARNING: If a serviceability tool is in use, please run with -XX:+EnableDynamicAgentLoading to hide this warning WARNING: If a serviceability tool is not in use, p","output":"Explicitly setting up instrumentation for inline mocking (Java 21+)","metadata":{"issue_id":"KAFKA-18356","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-28T12:13:33.000+0000","updated":"2025-09-26T08:31:23.000+0000","resolved":"2025-09-26T08:31:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-3-producer, transactionalId=xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-3] Connection to node 1 (b-1.xxxxxxx.yyyyyy.c2.kafka.xx-yyyyy.amazonaws.com/xx.xx.xxx.xxxx:yyyy) failed authentication due to: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: Failed to find AWS IAM Credentials [Caused by com.amazonaws.AmazonServiceException: Unauthorized (Service: null; Status Code: 401; Error Code: null; Request ID: null; Proxy: null)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTHENTICATION_FAILED state.\", \"thread_name\":\"kafka-producer-network-thread | xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-3-producer\", \"time\":\"2024-12-26T07:40:45.113067247Z\" } {code} * In some cases, the system recovers when the next record is polled and the Sink Node ( RecordCollectorImpl) throws the exception from the last message while processing * However, in couple of cases the following logs appears, approximately 5 minutes after the producer failure. ( {_}N{_}{_}o additional log statement to understand why thread stopped polling, however it seems heartbeat thread got the same exception as producer){_}. {code:java} { \"level\":\"WARN\", \"logger_name\":\"org.apache.kafka.clients.consumer.internals.ConsumerCoordinator\", \"message\":\"[Consumer clientId=xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-3-consumer, groupId=xxxxx-xxxx-lall-lio-step-executor_lio-se] consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.\", \"thread_name\":\"kafka-coordinator-heartbeat-thread | xxxxx-xxxx-lall-lio-step-executor_lio-se\", \"time\":\"2024-12-26T07:45:43.286428901Z\" } {code} * In such cases, the partition gets assigned to a new thread ( Thread 5), however the new thread keep throwing the following exception: {code:java} { \"level\":\"INFO\", \"logger_name\":\"org.apache.kafka.streams.processor.internals.TaskManager\", \"message\":\"stream-thread [xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-5] Encountered lock exception. Reattempting locking the state in the next iteration.\", \"stack_trace\":\"org.apache.kafka.streams.errors.LockException: stream-thread [xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-5] task [8_0] Failed to lock the state directory for task 8_0\\n\\tat org.apache.kafka.streams.processor.internals.StateManagerUtil.registerStateStores(StateManagerUtil.java:96)\\n\\tat org.apache.kafka.streams.processor.internals.StreamTask.initializeIfNeeded(StreamTask.java:258)\\n\\tat org.apache.kafka.streams.processor.internals.TaskManager.addTaskToStateUpdater(TaskManager.java:1010)\\n\\tat org.apache.kafka.streams.processor.internals.TaskManager.addTasksToStateUpdater(TaskManager.java:997)\\n\\tat org.apache.kafka.streams.processor.internals.TaskManager.checkStateUpdater(TaskManager.java:911)\\n\\tat org.apache.kafka.streams.processor.internals.StreamThread.checkStateUpdater(StreamThread.java:1188)\\n\\tat org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:996)\\n\\tat org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:711)\\n\\tat org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670)\\n\", \"thread_name\":\"xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-5\", \"time\":\"2024-12-26T07:50:53.904374419Z\" } {code} * We are using exception handler, however, in these failure cases our exception handler is not called for both producer and consumer exception. However in some authentication exception during consume/produce we see the handler being called. It seems that old thread didn't clean up its state: as the producer failures are cleaned up when processing next event ( which never happened due to consumer exception). Neither did consumer failure tried to release the lock.","output":"Stream thread blocks indefinitely for acquiring state directory lock","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-","output":"Resolved","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-","output":"Major","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-","output":"We are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-","output":"Fixed","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Stream thread blocks indefinitely for acquiring state directory lock\n\nWe are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening\n\nConversation:\nuser: ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval\nassistant: Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure wha\nassistant: I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stu\nassistant: Might be fixed via [https://github.com/apache/kafka/pull/20646] ?\nassistant: Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","output":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We are running Kafka streams based application in production and have noticed couple of times {*}lag on source topic partition start increasing{*}. Based on investigation, we found the below happening: * Thread responsible for the partition task gets Authentication exception ( MSK IAM authentication gives the transient exception) while producing record in the Sink {code:java} { \"level\":\"ERROR\", \"logger_name\":\"org.apache.kafka.clients.NetworkClient\", \"message\":\"[Producer clientId=xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-StreamThread-3-producer, transactionalId=xxxxx-xxxx-lall-lio-step-executor_lio-se-d0ee0299-9fd5-4fd0-8a42-cd49f49de845-3] Connection to node 1 (b-1.xxxxxxx.yyyyyy.c2.kafka.xx-yyyyy.amazonaws.com/xx.xx.xxx.xxxx:yyyy) failed authentication ","output":"Stream thread blocks indefinitely for acquiring state directory lock","metadata":{"issue_id":"KAFKA-18355","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"Ravi Gupta"},"created":"2024-12-28T10:49:48.000+0000","updated":"2025-10-08T21:45:27.000+0000","resolved":"2025-10-08T21:45:27.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":5,"comments":[{"id":"17909575","author":{"displayName":"Bruno Cadonna"},"body":"ravigupta Thanks for the ticket! This ticket contains multiple effects whose cause might be unrelated. First of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value. The lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. The increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else. Since the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209 This will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.","body_raw":"[~ravigupta] Thanks for the ticket!\r\nThis ticket contains multiple effects whose cause might be unrelated.\r\nFirst of all, we need to understand why your Streams application exceeds the consumer poll timeout. The consumer poll timeout is 5 min by default. You can increase it by setting {{max.poll.interval.ms}} to a higher value.\r\nThe lock exception happens because after your application exceeds the consumer poll timeout, a rebalance is triggered. The lock exception is not really an error. Note, that it is on INFO level and not on ERROR level. Streams will retry to acquire the lock and it will NOT re-throw the lock exception. That is also the reason your exception handler is not called. As far as we know the lock exception is harmless and it will not block the application indefinitely and it does not increase the lag. Except we overlooked a bug. \r\n  \r\nThe increased log messages regarding the lock exception are due to the state updater that is enabled by default in 3.8. The retries to get the loop run in a quite tight loop flooding the logs with that INFO message about the non-acquired lock. We have seen the lock exception often in our test infrastructure and each time it turned out to be harmless and the error was somewhere else.\r\n\r\nSince the increased logging is really annoying as you discovered, we introduced a backoff mechanism for retrying to acquire the lock on {{trunk}}: https://github.com/apache/kafka/pull/17209\r\nThis will be released with 4.0. We want to cherry-pick this change also to 3.9 and 3.8 patch releases.     \r\n\r\n    ","created":"2025-01-03T12:25:45.710+0000","updated":"2025-01-03T12:25:45.710+0000","updateAuthor":{"displayName":"Bruno Cadonna"}},{"id":"17913520","author":{"displayName":"Matthias J. Sax"},"body":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released. {quote}_however it seems heartbeat thread got the same exception as producer_ {quote} Not sure what this means?","body_raw":"Reading the ticket, it seems that the thread hitting the exception (ie, Authentication) does block? If the thread blocks, it would explain whey `poll()` is not called, and why the lock is not released.\r\n{quote}_however it seems heartbeat thread got the same exception as producer_\r\n{quote}\r\nNot sure what this means?","created":"2025-01-16T02:40:34.465+0000","updated":"2025-01-16T02:40:34.465+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17922495","author":{"displayName":"Lucas Brutschy"},"body":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information. ravigupta if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","body_raw":"I injected Authentication Errors into the producer and ran some tests with low {{{}max.poll.interval.ms{}}}, trying to reproduce this in 3.8.1, but no dice. I'm not sure if it will be possible to get to the bottom of this without further information.\r\n\r\n[~ravigupta] if there is indeed a thread getting stuck here (which is the most likely explanation), it would be extremely helpful to get a thread dump using {{{}jstack{}}}.","created":"2025-01-30T18:59:36.151+0000","updated":"2025-01-30T19:00:40.477+0000","updateAuthor":{"displayName":"Lucas Brutschy"}},{"id":"18028202","author":{"displayName":"Matthias J. Sax"},"body":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","body_raw":"Might be fixed via [https://github.com/apache/kafka/pull/20646] ?","created":"2025-10-07T20:56:18.447+0000","updated":"2025-10-07T20:56:18.447+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"18028320","author":{"displayName":"Lucas Brutschy"},"body":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","body_raw":"Yes, this could well be. This seems to be a bug in the state updater, which is enabled here according to the logs.","created":"2025-10-08T10:14:31.686+0000","updated":"2025-10-08T10:14:31.686+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"Use log4j2 APIs to refactor LogCaptureAppender","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"Resolved","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"Minor","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"Fixed","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Use log4j2 APIs to refactor LogCaptureAppender\n\n1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be\n\nConversation:\nuser: Hello, chia7712 , if you wont work on this, may I take the issue? Thank you\nassistant: trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","output":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"1. we can leverage Configurator.setLevel to configure level directly 2. Each LogCaptureAppender instance should have different name as log4j2 trace the appender by the name 3. LogLevelChange should be changed from list to map structure to ensure we don't reconfigure a class only once","output":"Use log4j2 APIs to refactor LogCaptureAppender","metadata":{"issue_id":"KAFKA-18354","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-28T07:04:51.000+0000","updated":"2024-12-29T14:57:53.000+0000","resolved":"2024-12-29T14:57:53.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908565","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-28T07:06:36.793+0000","updated":"2024-12-28T07:06:36.793+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17908708","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1 4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","body_raw":"trunk: https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1\r\n\r\n4.0: https://github.com/apache/kafka/commit/fd73b34193142aaf15a20d7efb4c7e53efb6c2ed","created":"2024-12-29T14:57:53.987+0000","updated":"2024-12-29T14:57:53.987+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zk config `control.plane.listener.name`\n\nAs title","output":"Resolved","metadata":{"issue_id":"KAFKA-18353","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-27T04:12:23.000+0000","updated":"2025-01-08T10:42:17.000+0000","resolved":"2025-01-08T10:42:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908373","author":{"displayName":"Chia-Ping Tsai"},"body":"\"control.plane.listener.name\"?","body_raw":"\"control.plane.listener.name\"?","created":"2024-12-27T04:15:22.893+0000","updated":"2024-12-27T04:15:22.893+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17908378","author":{"displayName":"TaiJuWu"},"body":"Yes, you are right, thanks!","body_raw":"Yes, you are right, thanks!","created":"2024-12-27T04:58:45.611+0000","updated":"2024-12-27T04:58:45.611+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911017","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","body_raw":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64\r\n\r\n4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","created":"2025-01-08T10:42:17.570+0000","updated":"2025-01-08T10:42:17.570+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zk config `control.plane.listener.name`\n\nAs title","output":"Blocker","metadata":{"issue_id":"KAFKA-18353","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-27T04:12:23.000+0000","updated":"2025-01-08T10:42:17.000+0000","resolved":"2025-01-08T10:42:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908373","author":{"displayName":"Chia-Ping Tsai"},"body":"\"control.plane.listener.name\"?","body_raw":"\"control.plane.listener.name\"?","created":"2024-12-27T04:15:22.893+0000","updated":"2024-12-27T04:15:22.893+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17908378","author":{"displayName":"TaiJuWu"},"body":"Yes, you are right, thanks!","body_raw":"Yes, you are right, thanks!","created":"2024-12-27T04:58:45.611+0000","updated":"2024-12-27T04:58:45.611+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911017","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","body_raw":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64\r\n\r\n4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","created":"2025-01-08T10:42:17.570+0000","updated":"2025-01-08T10:42:17.570+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zk config `control.plane.listener.name`\n\nAs title","output":"As title","metadata":{"issue_id":"KAFKA-18353","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-27T04:12:23.000+0000","updated":"2025-01-08T10:42:17.000+0000","resolved":"2025-01-08T10:42:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908373","author":{"displayName":"Chia-Ping Tsai"},"body":"\"control.plane.listener.name\"?","body_raw":"\"control.plane.listener.name\"?","created":"2024-12-27T04:15:22.893+0000","updated":"2024-12-27T04:15:22.893+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17908378","author":{"displayName":"TaiJuWu"},"body":"Yes, you are right, thanks!","body_raw":"Yes, you are right, thanks!","created":"2024-12-27T04:58:45.611+0000","updated":"2024-12-27T04:58:45.611+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911017","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","body_raw":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64\r\n\r\n4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","created":"2025-01-08T10:42:17.570+0000","updated":"2025-01-08T10:42:17.570+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zk config `control.plane.listener.name`\n\nAs title","output":"Fixed","metadata":{"issue_id":"KAFKA-18353","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-27T04:12:23.000+0000","updated":"2025-01-08T10:42:17.000+0000","resolved":"2025-01-08T10:42:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908373","author":{"displayName":"Chia-Ping Tsai"},"body":"\"control.plane.listener.name\"?","body_raw":"\"control.plane.listener.name\"?","created":"2024-12-27T04:15:22.893+0000","updated":"2024-12-27T04:15:22.893+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17908378","author":{"displayName":"TaiJuWu"},"body":"Yes, you are right, thanks!","body_raw":"Yes, you are right, thanks!","created":"2024-12-27T04:58:45.611+0000","updated":"2024-12-27T04:58:45.611+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911017","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","body_raw":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64\r\n\r\n4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","created":"2025-01-08T10:42:17.570+0000","updated":"2025-01-08T10:42:17.570+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zk config `control.plane.listener.name`\n\nAs title\n\nConversation:\nuser: \"control.plane.listener.name\"?\nassistant: Yes, you are right, thanks!\nassistant: trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","output":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","metadata":{"issue_id":"KAFKA-18353","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-27T04:12:23.000+0000","updated":"2025-01-08T10:42:17.000+0000","resolved":"2025-01-08T10:42:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17908373","author":{"displayName":"Chia-Ping Tsai"},"body":"\"control.plane.listener.name\"?","body_raw":"\"control.plane.listener.name\"?","created":"2024-12-27T04:15:22.893+0000","updated":"2024-12-27T04:15:22.893+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17908378","author":{"displayName":"TaiJuWu"},"body":"Yes, you are right, thanks!","body_raw":"Yes, you are right, thanks!","created":"2024-12-27T04:58:45.611+0000","updated":"2024-12-27T04:58:45.611+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911017","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64 4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","body_raw":"trunk: https://github.com/apache/kafka/commit/0c435e3855bda58d08e28001290ec305acdbbc64\r\n\r\n4.0: https://github.com/apache/kafka/commit/236e40b6cb67d8e91bfb108d42b782c3b3710ad2","created":"2025-01-08T10:42:17.570+0000","updated":"2025-01-08T10:42:17.570+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.","output":"DeleteGroups v0 incorrectly tagged as deprecated","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.","output":"Resolved","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.","output":"Major","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.","output":"The Sarama version listed in the KIP only supports v0.","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.","output":"Fixed","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"DeleteGroups v0 incorrectly tagged as deprecated\n\nThe Sarama version listed in the KIP only supports v0.\n\nConversation:\nuser: I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka\nassistant: Oh, sorry I didn't notice pr is already there.","output":"Oh, sorry I didn't notice pr is already there.","metadata":{"issue_id":"KAFKA-18352","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-26T16:30:47.000+0000","updated":"2024-12-27T05:41:06.000+0000","resolved":"2024-12-27T05:41:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908324","author":{"displayName":"Kuan Po Tseng"},"body":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","body_raw":"I think I can help with this issue, we need to add back DeleteGroups v0 back, right ? I saw they already removed from kafka","created":"2024-12-26T16:40:42.781+0000","updated":"2024-12-26T16:40:42.781+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}},{"id":"17908325","author":{"displayName":"Kuan Po Tseng"},"body":"Oh, sorry I didn't notice pr is already there.","body_raw":"Oh, sorry I didn't notice pr is already there.","created":"2024-12-26T16:41:21.729+0000","updated":"2024-12-26T16:41:21.729+0000","updateAuthor":{"displayName":"Kuan Po Tseng"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"remove `version` tag for docker-compose.yml\n\nDocument: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"remove `version` tag for docker-compose.yml","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"remove `version` tag for docker-compose.yml\n\nDocument: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"Resolved","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"remove `version` tag for docker-compose.yml\n\nDocument: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"Minor","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: remove `version` tag for docker-compose.yml\n\nDocument: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"Document: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"remove `version` tag for docker-compose.yml\n\nDocument: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"Fixed","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Document: [https://docs.docker.com/reference/compose-file/version-and-name/#version-top-level-element-obsolete] docker compose already obsolete `version` tag, thus we should remove it from docker-compose.yml, and avoid misunderstanding user","output":"remove `version` tag for docker-compose.yml","metadata":{"issue_id":"KAFKA-18351","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-26T16:09:08.000+0000","updated":"2025-09-22T14:57:42.000+0000","resolved":"2025-09-22T14:57:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zkBroker from RequestListenerType\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18350","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-26T13:03:37.000+0000","updated":"2025-01-30T13:53:04.000+0000","resolved":"2025-01-30T13:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914462","author":{"displayName":"PoAn Yang"},"body":"Reassign to m1a2st because this can be handled within KAFKA-18474.","body_raw":"Reassign to [~m1a2st] because this can be handled within KAFKA-18474.","created":"2025-01-19T15:26:01.344+0000","updated":"2025-01-19T15:26:01.344+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zkBroker from RequestListenerType\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18350","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-26T13:03:37.000+0000","updated":"2025-01-30T13:53:04.000+0000","resolved":"2025-01-30T13:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914462","author":{"displayName":"PoAn Yang"},"body":"Reassign to m1a2st because this can be handled within KAFKA-18474.","body_raw":"Reassign to [~m1a2st] because this can be handled within KAFKA-18474.","created":"2025-01-19T15:26:01.344+0000","updated":"2025-01-19T15:26:01.344+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zkBroker from RequestListenerType\n\n","output":"","metadata":{"issue_id":"KAFKA-18350","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-26T13:03:37.000+0000","updated":"2025-01-30T13:53:04.000+0000","resolved":"2025-01-30T13:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914462","author":{"displayName":"PoAn Yang"},"body":"Reassign to m1a2st because this can be handled within KAFKA-18474.","body_raw":"Reassign to [~m1a2st] because this can be handled within KAFKA-18474.","created":"2025-01-19T15:26:01.344+0000","updated":"2025-01-19T15:26:01.344+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zkBroker from RequestListenerType\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18350","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-26T13:03:37.000+0000","updated":"2025-01-30T13:53:04.000+0000","resolved":"2025-01-30T13:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914462","author":{"displayName":"PoAn Yang"},"body":"Reassign to m1a2st because this can be handled within KAFKA-18474.","body_raw":"Reassign to [~m1a2st] because this can be handled within KAFKA-18474.","created":"2025-01-19T15:26:01.344+0000","updated":"2025-01-19T15:26:01.344+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zkBroker from RequestListenerType\n\n\n\nConversation:\nuser: Reassign to m1a2st because this can be handled within KAFKA-18474.","output":"Reassign to m1a2st because this can be handled within KAFKA-18474.","metadata":{"issue_id":"KAFKA-18350","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-26T13:03:37.000+0000","updated":"2025-01-30T13:53:04.000+0000","resolved":"2025-01-30T13:53:04.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17914462","author":{"displayName":"PoAn Yang"},"body":"Reassign to m1a2st because this can be handled within KAFKA-18474.","body_raw":"Reassign to [~m1a2st] because this can be handled within KAFKA-18474.","created":"2025-01-19T15:26:01.344+0000","updated":"2025-01-19T15:26:01.344+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Consider add typing hint for system test\n\nOur system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","output":"Consider add typing hint for system test","metadata":{"issue_id":"KAFKA-18349","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-25T10:54:07.000+0000","updated":"2025-03-03T01:47:17.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Consider add typing hint for system test\n\nOur system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","output":"Open","metadata":{"issue_id":"KAFKA-18349","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-25T10:54:07.000+0000","updated":"2025-03-03T01:47:17.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Consider add typing hint for system test\n\nOur system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","output":"Major","metadata":{"issue_id":"KAFKA-18349","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-25T10:54:07.000+0000","updated":"2025-03-03T01:47:17.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Consider add typing hint for system test\n\nOur system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","output":"Our system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","metadata":{"issue_id":"KAFKA-18349","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-25T10:54:07.000+0000","updated":"2025-03-03T01:47:17.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Our system test is written by Python but it is not included typing hint right now. There is some pain for readability since we don't know the type when developing.","output":"Consider add typing hint for system test","metadata":{"issue_id":"KAFKA-18349","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Open","priority":"Major","assignee":null,"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-25T10:54:07.000+0000","updated":"2025-03-03T01:47:17.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","output":"Remove the deprecated MockConsumer#setException","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","output":"Resolved","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","output":"Major","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","output":"it was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)","output":"Fixed","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove the deprecated MockConsumer#setException\n\nit was deprecated by https://issues.apache.org/jira/browse/KAFKA-7941 (2.0.2, 2019)\n\nConversation:\nuser: chia7712 Could you assign this to me ? :)\nassistant: trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","output":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","metadata":{"issue_id":"KAFKA-18348","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"kangning.li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-25T07:31:35.000+0000","updated":"2024-12-27T04:40:33.000+0000","resolved":"2024-12-27T04:40:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17908154","author":{"displayName":"kangning.li"},"body":"chia7712 Could you assign this to me ? :)","body_raw":"[~chia7712]    Could you assign this to me ?  :)","created":"2024-12-25T07:34:59.901+0000","updated":"2024-12-25T07:34:59.901+0000","updateAuthor":{"displayName":"kangning.li"}},{"id":"17908375","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866 4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","body_raw":"trunk: https://github.com/apache/kafka/commit/6139840e98ccb55d8a2b24fb37f3fb7442917866\r\n\r\n4.0: https://github.com/apache/kafka/commit/67b6426660d675cb9c72ce3001d7c004333463e7","created":"2024-12-27T04:40:33.248+0000","updated":"2024-12-27T04:40:33.248+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add tools-log4j2.yaml to config and remove unsed tools-log4j.properties from config\n\n`kafka-run-class` also needs to be updated","output":"Resolved","metadata":{"issue_id":"KAFKA-18347","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2024-12-24T09:20:37.000+0000","updated":"2025-01-01T13:06:44.000+0000","resolved":"2025-01-01T13:06:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909132","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","body_raw":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5\r\n\r\n4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","created":"2025-01-01T13:06:44.608+0000","updated":"2025-01-01T13:06:44.608+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add tools-log4j2.yaml to config and remove unsed tools-log4j.properties from config\n\n`kafka-run-class` also needs to be updated","output":"Major","metadata":{"issue_id":"KAFKA-18347","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2024-12-24T09:20:37.000+0000","updated":"2025-01-01T13:06:44.000+0000","resolved":"2025-01-01T13:06:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909132","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","body_raw":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5\r\n\r\n4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","created":"2025-01-01T13:06:44.608+0000","updated":"2025-01-01T13:06:44.608+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add tools-log4j2.yaml to config and remove unsed tools-log4j.properties from config\n\n`kafka-run-class` also needs to be updated","output":"`kafka-run-class` also needs to be updated","metadata":{"issue_id":"KAFKA-18347","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2024-12-24T09:20:37.000+0000","updated":"2025-01-01T13:06:44.000+0000","resolved":"2025-01-01T13:06:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909132","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","body_raw":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5\r\n\r\n4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","created":"2025-01-01T13:06:44.608+0000","updated":"2025-01-01T13:06:44.608+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add tools-log4j2.yaml to config and remove unsed tools-log4j.properties from config\n\n`kafka-run-class` also needs to be updated","output":"Fixed","metadata":{"issue_id":"KAFKA-18347","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2024-12-24T09:20:37.000+0000","updated":"2025-01-01T13:06:44.000+0000","resolved":"2025-01-01T13:06:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909132","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","body_raw":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5\r\n\r\n4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","created":"2025-01-01T13:06:44.608+0000","updated":"2025-01-01T13:06:44.608+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add tools-log4j2.yaml to config and remove unsed tools-log4j.properties from config\n\n`kafka-run-class` also needs to be updated\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","output":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","metadata":{"issue_id":"KAFKA-18347","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"TengYao Chi"},"created":"2024-12-24T09:20:37.000+0000","updated":"2025-01-01T13:06:44.000+0000","resolved":"2025-01-01T13:06:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17909132","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5 4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","body_raw":"trunk: https://github.com/apache/kafka/commit/5ae4b62876045bcccbfcaac0a42842cef1cd25c5\r\n\r\n4.0: https://github.com/apache/kafka/commit/15f6c9b089dd8d7db3519cfc964ea4fba7709c60","created":"2025-01-01T13:06:44.608+0000","updated":"2025-01-01T13:06:44.608+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 182, in test_combined_mode_upgrade_downgrade self.run_upgrade_downgrade(from_kafka_version) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 160, in run_upgrade_downgrade self.run_produce_consume_validate(core_test_action=lambda: self.downgrade_to_version(starting_kafka_version)) File \"/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py\", line 105, in run_produce_consume_validate core_test_action(*args) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 160, in self.run_produce_consume_validate(core_test_action=lambda: self.downgrade_to_version(starting_kafka_version)) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 85, in downgrade_to_version self.kafka.controller_quorum.start_node(node) File \"/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py\", line 912, in start_node self.wait_for_start(node, monitor, timeout_sec) File \"/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py\", line 917, in wait_for_start err_msg=\"Kafka server didn't finish startup in %d seconds\" % timeout_sec) File \"/usr/local/lib/python3.7/dist-packages/ducktape/cluster/remoteaccount.py\", line 754, in wait_until allow_fail=True) == 0, **kwargs) File \"/usr/local/lib/python3.7/dist-packages/ducktape/utils/util.py\", line 58, in wait_until raise TimeoutError(err_msg() if callable(err_msg) else err_msg) from last_exception ducktape.errors.TimeoutError: Kafka server didn't finish startup in 60 seconds ---------------------------------------------------------------------------------------------------- test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_isolated_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=ISOLATED_KRAFT status: FAIL run time: 4 minutes 29.349 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 188, in test_isolated_mode_upgrade_downgrade self.run_upgrade_downgrade(from_kafka_version) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 160, in run_upgrade_downgrade self.run_produce_consume_validate(core_test_action=lambda: self.downgrade_to_version(starting_kafka_version)) File \"/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py\", line 105, in run_produce_consume_validate core_test_action(*args) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 160, in self.run_produce_consume_validate(core_test_action=lambda: self.downgrade_to_version(starting_kafka_version)) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kraft_upgrade_test.py\", line 85, in downgrade_to_version self.kafka.controller_quorum.start_node(node) File \"/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py\", line 912, in start_node self.wait_for_start(node, monitor, timeout_sec) File \"/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py\", line 917, in wait_for_start err_msg=\"Kafka server didn't finish startup in %d seconds\" % timeout_sec) File \"/usr/local/lib/python3.7/dist-packages/ducktape/cluster/remoteaccount.py\", line 754, in wait_until allow_fail=True) == 0, **kwargs) File \"/usr/local/lib/python3.7/dist-packages/ducktape/utils/util.py\", line 58, in wait_until raise TimeoutError(err_msg() if callable(err_msg) else err_msg) from last_exception ducktape.errors.TimeoutError: Kafka server didn't finish startup in 60 seconds","output":"Fix e2e TestKRaftUpgrade from v3.3.2","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/","output":"Resolved","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/","output":"Blocker","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/","output":"Follow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (m","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/","output":"Fixed","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix e2e TestKRaftUpgrade from v3.3.2\n\nFollow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 \n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","output":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Follow e2e fail test_id: kafkatest.tests.core.kraft_upgrade_test.TestKRaftUpgrade.test_combined_mode_upgrade_downgrade.from_kafka_version=3.3.2.metadata_quorum=COMBINED_KRAFT status: FAIL run time: 5 minutes 17.865 seconds TimeoutError(\"Kafka server didn't finish startup in 60 seconds\") Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/core/kra","output":"Fix e2e TestKRaftUpgrade from v3.3.2","metadata":{"issue_id":"KAFKA-18346","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-24T07:57:04.000+0000","updated":"2025-01-15T13:39:26.000+0000","resolved":"2025-01-15T13:39:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17913317","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961] 4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","body_raw":"trunk: [https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/5690489b6b6bbb2f36965b1c2c98eba98d660961","created":"2025-01-15T13:39:26.585+0000","updated":"2025-01-15T13:39:26.585+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved\n\nRight now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random range is sufficient to avoid repeat split votes in elections. > Using more randomness improves worst-case behavior: with a 50 ms random range, the worst-case completion time (over 1,000 trials) was 513 ms. We should investigate if it would make sense to replace the exponential backoff with a non-exponential backoff that is more straightforward. e.g. {code:java} int randomElectionBackoffMs() { long backoff = random(0, 50ms) return Math.min(state.remainingElectionTimeoutMs, backoff) }{code} Note, it's fine that the max is set to state.remainingElectionTimeoutMs because that value is initialized with randomness in it (up to another quorumConfig.electionTimeoutMs). The alternative could also be using a lower default electionTimeoutMs (like 300ms mentioned in the Raft paper) and on election failure, waiting the extent of state.remainingElectionTimeoutMs (which contains some jitter).","output":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved\n\nRight now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random ","output":"Resolved","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved\n\nRight now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random ","output":"Critical","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Investigate if binaryExponentialElectionBackoffMs can be removed or improved\n\nRight now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random ","output":"Right now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election ","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved\n\nRight now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random ","output":"Fixed","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Right now this exponential backoff method can return a min backoff of 200ms and cap out at the electionBackoffMaxMs (1s default). The logic for calculating this exponential backoff isn't very straightforward, and probably no longer needed now that Candidate transitions to Prospective after election loss. We can simplify states by not tracking \"retries\" as well. We _do_ still need a bit of randomness to ensure we can get past gridlocked elections, but as Diego's Raft thesis shows, 50ms of random range is sufficient to avoid repeat split votes in elections. > Using more randomness improves worst-case behavior: with a 50 ms random range, the worst-case completion time (over 1,000 trials) was 513 ms. We should investigate if it would make sense to replace the exponential backoff with a non-exp","output":"Investigate if binaryExponentialElectionBackoffMs can be removed or improved","metadata":{"issue_id":"KAFKA-18345","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Alyssa Huang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-24T00:01:14.000+0000","updated":"2025-05-23T16:35:52.000+0000","resolved":"2025-05-22T22:08:21.000+0000","resolution":"Fixed","labels":[],"components":["kraft"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Consider to distinguish between multiple \"positions\"\n\nKafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffset`, because \"position\" does not reflect the latest _processed_ record, but might be ahead of what the application code did process. If lag is computed as \"endOffset - committedOffset\", lag is always behind, ie, larger than the real lag, what might actually provide better semantics. – It seems undesired that the consumer lag metric could be smaller and the actual lag... We should consider to update the position of the consumer differently: # A simple changes could be, to update the position to the offset of the first/oldest record in a `poll()` call (instead of latest/newest as we do right now), to avoid that the position get ahead and lag is \"too small\" # We could also try to hook into the returned `ConsumerRecords` iterator, to track the position more fine grained on a per-record basis # We could track multiple positions, like \"processed positions\" and \"fetched position\" (not that \"fetched position\" might be even further ahead than the current position, as based on `max.poll.records` not all fetch records might be returned from `poll()`)","output":"Consider to distinguish between multiple \"positions\"","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Consider to distinguish between multiple \"positions\"\n\nKafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffset`, because \"position\" does not reflect the latest _processed_ record, but might be ahead of what the application code did process. If lag is computed as \"endOffset - committedOffset\", lag is always b","output":"Open","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Consider to distinguish between multiple \"positions\"\n\nKafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffset`, because \"position\" does not reflect the latest _processed_ record, but might be ahead of what the application code did process. If lag is computed as \"endOffset - committedOffset\", lag is always b","output":"Major","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Consider to distinguish between multiple \"positions\"\n\nKafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffset`, because \"position\" does not reflect the latest _processed_ record, but might be ahead of what the application code did process. If lag is computed as \"endOffset - committedOffset\", lag is always b","output":"KafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffse","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Consider to distinguish between multiple \"positions\"\n\nKafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is comput\n\nConversation:\nuser: Hi mjsax I would take over this issue. Thank you 😀","output":"Hi mjsax I would take over this issue. Thank you 😀","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"KafkaConsumer currently maintains a \"position\" which is the max offset of records returned via `poll()`. This \"position\" is used to compute the consumer \"lag metrics\". This implies, that lag is computed slightly different on the consumer, compared to other tools which use `endOffset - committedOffset`, because \"position\" does not reflect the latest _processed_ record, but might be ahead of what the application code did process. If lag is computed as \"endOffset - committedOffset\", lag is always behind, ie, larger than the real lag, what might actually provide better semantics. – It seems undesired that the consumer lag metric could be smaller and the actual lag... We should consider to update the position of the consumer differently: # A simple changes could be, to update the position to th","output":"Consider to distinguish between multiple \"positions\"","metadata":{"issue_id":"KAFKA-18344","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Matthias J. Sax"},"created":"2024-12-23T23:33:18.000+0000","updated":"2024-12-23T23:42:55.000+0000","resolved":null,"labels":["needs-kip"],"components":["clients","consumer"],"comment_count":1,"comments":[{"id":"17907963","author":{"displayName":"TengYao Chi"},"body":"Hi mjsax I would take over this issue. Thank you 😀","body_raw":"Hi [~mjsax] \r\n\r\nI would take over this issue. \r\n\r\nThank you 😀","created":"2024-12-23T23:42:55.399+0000","updated":"2024-12-23T23:42:55.399+0000","updateAuthor":{"displayName":"TengYao Chi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Use java_pids to implement pids\n\nexcept for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"Use java_pids to implement pids","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Use java_pids to implement pids\n\nexcept for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"Resolved","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Use java_pids to implement pids\n\nexcept for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"Minor","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Use java_pids to implement pids\n\nexcept for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"except for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Use java_pids to implement pids\n\nexcept for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"Fixed","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"except for streams and connect which having separate pid file, other classes can leverage ducktape java_pids to return pid https://github.com/confluentinc/ducktape/blob/6136efbb84710f7f7a200172ae88873ab154582d/ducktape/cluster/remoteaccount.py#L442","output":"Use java_pids to implement pids","metadata":{"issue_id":"KAFKA-18343","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T17:26:43.000+0000","updated":"2024-12-25T00:46:05.000+0000","resolved":"2024-12-25T00:46:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"Vagrantfile can't work with ruby 3.2","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"Resolved","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"Major","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagra","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"Fixed","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Vagrantfile can't work with ruby 3.2\n\n`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/comm\n\nConversation:\nuser: I'm working on this. Thanks:)\nassistant: trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit","output":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"`File.exists` was removed from ruby 3.2 [0], and the newest vagrant could use ruby 3.2 and 3.3 [1]. Hence, we should ensure Vagrantfile[2] can work with ruby 3.2+ [0] https://github.com/ruby/ruby/commit/bf97415c02b11a8949f715431aca9eeb6311add2 [1] https://github.com/hashicorp/vagrant/blob/main/vagrant.gemspec#L15 [2] https://github.com/apache/kafka/blob/trunk/Vagrantfile#L62","output":"Vagrantfile can't work with ruby 3.2","metadata":{"issue_id":"KAFKA-18342","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Jhen-Yung Hsu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-23T16:39:32.000+0000","updated":"2024-12-24T07:07:07.000+0000","resolved":"2024-12-24T07:07:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17907917","author":{"displayName":"Jhen-Yung Hsu"},"body":"I'm working on this. Thanks:)","body_raw":"I'm working on this. Thanks:)","created":"2024-12-23T16:43:09.539+0000","updated":"2024-12-23T16:43:09.539+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17908025","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3 4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea 3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d 3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","body_raw":"trunk: https://github.com/apache/kafka/commit/a940d520703c133e964f13d2ec864a78972978b3\r\n\r\n4.0: https://github.com/apache/kafka/commit/60b646c70cd3500a237abc65a2bd971329acc2ea\r\n\r\n3.9: https://github.com/apache/kafka/commit/430892654bcf45d644e66b532d83aab0f569cb7d\r\n\r\n3.8: https://github.com/apache/kafka/commit/77712d521865a1c5b70c4909b98db8c58404709b","created":"2024-12-24T07:07:07.803+0000","updated":"2024-12-24T07:07:07.803+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove KafkaConfig GroupType config check and warn log\n\nIn Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"Remove KafkaConfig GroupType config check and warn log","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove KafkaConfig GroupType config check and warn log\n\nIn Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"Resolved","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove KafkaConfig GroupType config check and warn log\n\nIn Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"Major","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove KafkaConfig GroupType config check and warn log\n\nIn Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"In Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove KafkaConfig GroupType config check and warn log\n\nIn Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"Fixed","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"In Kafka 4.0, the ZK will be removed. there are group type warning message about \"WARN The new 'consumer' rebalance protocol is only supported in KRaft cluster with the new group coordinator.\" we should remove this message in code base","output":"Remove KafkaConfig GroupType config check and warn log","metadata":{"issue_id":"KAFKA-18341","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T15:29:03.000+0000","updated":"2025-01-13T10:44:13.000+0000","resolved":"2025-01-13T10:42:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Change Dockerfile to use log4j2 yaml instead log4j properties\n\nWe already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"Change Dockerfile to use log4j2 yaml instead log4j properties","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Change Dockerfile to use log4j2 yaml instead log4j properties\n\nWe already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"Resolved","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Change Dockerfile to use log4j2 yaml instead log4j properties\n\nWe already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"Major","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Change Dockerfile to use log4j2 yaml instead log4j properties\n\nWe already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"We already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Change Dockerfile to use log4j2 yaml instead log4j properties\n\nWe already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"Fixed","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We already change to use log4j2 in Kafka version 4.0, we should also update the Dockerfile to use log4j2 instead log4j","output":"Change Dockerfile to use log4j2 yaml instead log4j properties","metadata":{"issue_id":"KAFKA-18340","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-23T13:48:58.000+0000","updated":"2025-01-13T08:59:54.000+0000","resolved":"2025-01-10T15:00:05.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove raw unversioned direct SASL protocol (KIP-896)\n\nIt preceded SaslHandshake and KIP-43.","output":"Resolved","metadata":{"issue_id":"KAFKA-18339","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-21T22:52:15.000+0000","updated":"2025-01-06T11:00:14.000+0000","resolved":"2024-12-27T22:56:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove raw unversioned direct SASL protocol (KIP-896)\n\nIt preceded SaslHandshake and KIP-43.","output":"Major","metadata":{"issue_id":"KAFKA-18339","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-21T22:52:15.000+0000","updated":"2025-01-06T11:00:14.000+0000","resolved":"2024-12-27T22:56:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove raw unversioned direct SASL protocol (KIP-896)\n\nIt preceded SaslHandshake and KIP-43.","output":"It preceded SaslHandshake and KIP-43.","metadata":{"issue_id":"KAFKA-18339","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-21T22:52:15.000+0000","updated":"2025-01-06T11:00:14.000+0000","resolved":"2024-12-27T22:56:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove raw unversioned direct SASL protocol (KIP-896)\n\nIt preceded SaslHandshake and KIP-43.","output":"Fixed","metadata":{"issue_id":"KAFKA-18339","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-21T22:52:15.000+0000","updated":"2025-01-06T11:00:14.000+0000","resolved":"2024-12-27T22:56:06.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"add log4j.yaml to test-common-api and remove unsed log4j.properties from test-common\n\nas title.","output":"Resolved","metadata":{"issue_id":"KAFKA-18338","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-21T16:14:11.000+0000","updated":"2024-12-24T08:26:21.000+0000","resolved":"2024-12-24T08:26:21.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908037","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","body_raw":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096\r\n\r\n4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","created":"2024-12-24T08:26:21.742+0000","updated":"2024-12-24T08:26:21.742+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"add log4j.yaml to test-common-api and remove unsed log4j.properties from test-common\n\nas title.","output":"Major","metadata":{"issue_id":"KAFKA-18338","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-21T16:14:11.000+0000","updated":"2024-12-24T08:26:21.000+0000","resolved":"2024-12-24T08:26:21.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908037","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","body_raw":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096\r\n\r\n4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","created":"2024-12-24T08:26:21.742+0000","updated":"2024-12-24T08:26:21.742+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: add log4j.yaml to test-common-api and remove unsed log4j.properties from test-common\n\nas title.","output":"as title.","metadata":{"issue_id":"KAFKA-18338","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-21T16:14:11.000+0000","updated":"2024-12-24T08:26:21.000+0000","resolved":"2024-12-24T08:26:21.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908037","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","body_raw":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096\r\n\r\n4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","created":"2024-12-24T08:26:21.742+0000","updated":"2024-12-24T08:26:21.742+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"add log4j.yaml to test-common-api and remove unsed log4j.properties from test-common\n\nas title.","output":"Fixed","metadata":{"issue_id":"KAFKA-18338","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-21T16:14:11.000+0000","updated":"2024-12-24T08:26:21.000+0000","resolved":"2024-12-24T08:26:21.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908037","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","body_raw":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096\r\n\r\n4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","created":"2024-12-24T08:26:21.742+0000","updated":"2024-12-24T08:26:21.742+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"add log4j.yaml to test-common-api and remove unsed log4j.properties from test-common\n\nas title.\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","output":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","metadata":{"issue_id":"KAFKA-18338","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-21T16:14:11.000+0000","updated":"2024-12-24T08:26:21.000+0000","resolved":"2024-12-24T08:26:21.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908037","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096 4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","body_raw":"trunk: https://github.com/apache/kafka/commit/5a4590c02016faeea3adeadb0d53e4388a558096\r\n\r\n4.0: https://github.com/apache/kafka/commit/1c2312e53fb6d1fa86d42d33655c6540cdd6b4b2","created":"2024-12-24T08:26:21.742+0000","updated":"2024-12-24T08:26:21.742+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"KafkaProducer Memory Leak in JmxReporter Class\n\nHello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { producer.close(); } }); {code} Using this code to close the KafkaProducer can cause the AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics); within KafkaProducer.close() to execute before the client.close() in Sender.run(). This leads to {{client.close()}} eventually calling {{{}JmxReporter.reregister(){}}}, which prevents the metrics from being completely cleaned up !image-2024-12-21-15-52-39-930.png! The RecordAccumulator is utilized by metrics in JMX and cannot be garbage collected, ultimately leading to a memory leak。","output":"KafkaProducer Memory Leak in JmxReporter Class","metadata":{"issue_id":"KAFKA-18337","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"MaoWei Tian"},"reporter":{"displayName":"MaoWei Tian"},"created":"2024-12-21T07:58:29.000+0000","updated":"2025-05-28T03:45:39.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"KafkaProducer Memory Leak in JmxReporter Class\n\nHello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { producer.close(); } }); {code} Using this code to close the KafkaProducer can cause the AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics); within KafkaProducer.close()","output":"Patch Available","metadata":{"issue_id":"KAFKA-18337","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"MaoWei Tian"},"reporter":{"displayName":"MaoWei Tian"},"created":"2024-12-21T07:58:29.000+0000","updated":"2025-05-28T03:45:39.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"KafkaProducer Memory Leak in JmxReporter Class\n\nHello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { producer.close(); } }); {code} Using this code to close the KafkaProducer can cause the AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics); within KafkaProducer.close()","output":"Major","metadata":{"issue_id":"KAFKA-18337","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"MaoWei Tian"},"reporter":{"displayName":"MaoWei Tian"},"created":"2024-12-21T07:58:29.000+0000","updated":"2025-05-28T03:45:39.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: KafkaProducer Memory Leak in JmxReporter Class\n\nHello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { producer.close(); } }); {code} Using this code to close the KafkaProducer can cause the AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics); within KafkaProducer.close()","output":"Hello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exc","metadata":{"issue_id":"KAFKA-18337","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"MaoWei Tian"},"reporter":{"displayName":"MaoWei Tian"},"created":"2024-12-21T07:58:29.000+0000","updated":"2025-05-28T03:45:39.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Hello！ If I use a Callback in the KafkaProducer.send() to call the KafkaProducer.close(), the metrics will not be completely cleaned up. {code:java} //代码占位符 producer.send((new ProducerRecord<>(\"topic\", \"message test\")), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { producer.close(); } }); {code} Using this code to close the KafkaProducer can cause the AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics); within KafkaProducer.close() to execute before the client.close() in Sender.run(). This leads to {{client.close()}} eventually calling {{{}JmxReporter.reregister(){}}}, which prevents the metrics from being completely cleaned up !image-2024-12-21-15-52-39-930.png! The RecordAccumulator is utilized by metrics in JMX and cannot ","output":"KafkaProducer Memory Leak in JmxReporter Class","metadata":{"issue_id":"KAFKA-18337","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Patch Available","priority":"Major","assignee":{"displayName":"MaoWei Tian"},"reporter":{"displayName":"MaoWei Tian"},"created":"2024-12-21T07:58:29.000+0000","updated":"2025-05-28T03:45:39.000+0000","resolved":null,"labels":[],"components":["clients"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Resolved","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Minor","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Now AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBe","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Fixed","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark\n\nNow AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark ha\n\nConversation:\nuser: Please assign it to me. I can't do it myself in Jira\nassistant: PR merged","output":"PR merged","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Now AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole. Compiler can eliminate returned value and benchmark would be incorrect StandardAuthorizerUpdateBenchmark has a big error For example Benchmark (aclCount) Mode Cnt Score Error Units StandardAuthorizerUpdateBenchmark.testAddAcl 25000 avgt 10 5.425 ± 1.723 ms/op","output":"Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark","metadata":{"issue_id":"KAFKA-18336","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Minor","assignee":null,"reporter":{"displayName":"Evgeny Kuvardin"},"created":"2024-12-20T20:18:02.000+0000","updated":"2025-10-10T20:29:51.000+0000","resolved":"2025-10-10T20:29:51.000+0000","resolution":"Fixed","labels":[],"components":["system tests"],"comment_count":2,"comments":[{"id":"17907479","author":{"displayName":"Evgeny Kuvardin"},"body":"Please assign it to me. I can't do it myself in Jira","body_raw":"Please assign it to me. I can't do it myself in Jira","created":"2024-12-20T20:19:52.413+0000","updated":"2024-12-20T20:19:52.413+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}},{"id":"18029103","author":{"displayName":"Evgeny Kuvardin"},"body":"PR merged","body_raw":"PR merged","created":"2025-10-10T20:29:51.652+0000","updated":"2025-10-10T20:29:51.652+0000","updateAuthor":{"displayName":"Evgeny Kuvardin"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove deprecated metrics as part of KIP-1109 in Kafka-5.0\n\nAs part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","output":"Remove deprecated metrics as part of KIP-1109 in Kafka-5.0","metadata":{"issue_id":"KAFKA-18335","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2024-12-20T19:59:46.000+0000","updated":"2024-12-20T20:00:07.000+0000","resolved":null,"labels":[],"components":["clients","consumer","metrics"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated metrics as part of KIP-1109 in Kafka-5.0\n\nAs part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","output":"Open","metadata":{"issue_id":"KAFKA-18335","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2024-12-20T19:59:46.000+0000","updated":"2024-12-20T20:00:07.000+0000","resolved":null,"labels":[],"components":["clients","consumer","metrics"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated metrics as part of KIP-1109 in Kafka-5.0\n\nAs part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","output":"Major","metadata":{"issue_id":"KAFKA-18335","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2024-12-20T19:59:46.000+0000","updated":"2024-12-20T20:00:07.000+0000","resolved":null,"labels":[],"components":["clients","consumer","metrics"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated metrics as part of KIP-1109 in Kafka-5.0\n\nAs part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","output":"As part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","metadata":{"issue_id":"KAFKA-18335","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2024-12-20T19:59:46.000+0000","updated":"2024-12-20T20:00:07.000+0000","resolved":null,"labels":[],"components":["clients","consumer","metrics"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"As part of KIP-1109 we deprecated metrics with replaced topic names. Remove tose Metrics in Kafka-5.0.","output":"Remove deprecated metrics as part of KIP-1109 in Kafka-5.0","metadata":{"issue_id":"KAFKA-18335","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Open","priority":"Major","assignee":{"displayName":"Apoorv Mittal"},"reporter":{"displayName":"Apoorv Mittal"},"created":"2024-12-20T19:59:46.000+0000","updated":"2024-12-20T20:00:07.000+0000","resolved":null,"labels":[],"components":["clients","consumer","metrics"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Produce v4-v6 should be undeprecated\n\nLibrdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Produce v4-v6 should be undeprecated","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Produce v4-v6 should be undeprecated\n\nLibrdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Resolved","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Produce v4-v6 should be undeprecated\n\nLibrdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Major","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Produce v4-v6 should be undeprecated\n\nLibrdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Librdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to ","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Produce v4-v6 should be undeprecated\n\nLibrdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Fixed","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Librdkafka totally breaks if produce v3 is removed - it starts sending records with record format v0. I will file an issue in the upstream project, but the immediate impact is that these api versions have to be undeprecated. I've already updated KIP-896 with this information and will send a note to the mailing list soon.","output":"Produce v4-v6 should be undeprecated","metadata":{"issue_id":"KAFKA-18334","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-20T19:35:32.000+0000","updated":"2024-12-21T02:16:44.000+0000","resolved":"2024-12-21T02:16:44.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Simulation test should include invariant that quorum state never loses state\n\ne.g. within same epoch, we don't lose votedKey or leaderId state","output":"Simulation test should include invariant that quorum state never loses state","metadata":{"issue_id":"KAFKA-18333","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"In Progress","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:52:09.000+0000","updated":"2025-01-17T14:55:20.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":2,"comments":[{"id":"17907522","author":{"displayName":"PoAn Yang"},"body":"Hi alyssahuang, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~alyssahuang], if you're not working on this, may I take it? Thank you.","created":"2024-12-21T01:01:32.082+0000","updated":"2024-12-21T01:01:32.082+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17911224","author":{"displayName":"Alyssa Huang"},"body":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","body_raw":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","created":"2025-01-08T20:16:32.339+0000","updated":"2025-01-08T20:16:32.339+0000","updateAuthor":{"displayName":"Alyssa Huang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Simulation test should include invariant that quorum state never loses state\n\ne.g. within same epoch, we don't lose votedKey or leaderId state","output":"In Progress","metadata":{"issue_id":"KAFKA-18333","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"In Progress","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:52:09.000+0000","updated":"2025-01-17T14:55:20.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":2,"comments":[{"id":"17907522","author":{"displayName":"PoAn Yang"},"body":"Hi alyssahuang, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~alyssahuang], if you're not working on this, may I take it? Thank you.","created":"2024-12-21T01:01:32.082+0000","updated":"2024-12-21T01:01:32.082+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17911224","author":{"displayName":"Alyssa Huang"},"body":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","body_raw":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","created":"2025-01-08T20:16:32.339+0000","updated":"2025-01-08T20:16:32.339+0000","updateAuthor":{"displayName":"Alyssa Huang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Simulation test should include invariant that quorum state never loses state\n\ne.g. within same epoch, we don't lose votedKey or leaderId state","output":"Major","metadata":{"issue_id":"KAFKA-18333","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"In Progress","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:52:09.000+0000","updated":"2025-01-17T14:55:20.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":2,"comments":[{"id":"17907522","author":{"displayName":"PoAn Yang"},"body":"Hi alyssahuang, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~alyssahuang], if you're not working on this, may I take it? Thank you.","created":"2024-12-21T01:01:32.082+0000","updated":"2024-12-21T01:01:32.082+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17911224","author":{"displayName":"Alyssa Huang"},"body":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","body_raw":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","created":"2025-01-08T20:16:32.339+0000","updated":"2025-01-08T20:16:32.339+0000","updateAuthor":{"displayName":"Alyssa Huang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Simulation test should include invariant that quorum state never loses state\n\ne.g. within same epoch, we don't lose votedKey or leaderId state","output":"e.g. within same epoch, we don't lose votedKey or leaderId state","metadata":{"issue_id":"KAFKA-18333","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"In Progress","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:52:09.000+0000","updated":"2025-01-17T14:55:20.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":2,"comments":[{"id":"17907522","author":{"displayName":"PoAn Yang"},"body":"Hi alyssahuang, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~alyssahuang], if you're not working on this, may I take it? Thank you.","created":"2024-12-21T01:01:32.082+0000","updated":"2024-12-21T01:01:32.082+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17911224","author":{"displayName":"Alyssa Huang"},"body":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","body_raw":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","created":"2025-01-08T20:16:32.339+0000","updated":"2025-01-08T20:16:32.339+0000","updateAuthor":{"displayName":"Alyssa Huang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Simulation test should include invariant that quorum state never loses state\n\ne.g. within same epoch, we don't lose votedKey or leaderId state\n\nConversation:\nuser: Hi alyssahuang, if you're not working on this, may I take it? Thank you.\nassistant: I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","output":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","metadata":{"issue_id":"KAFKA-18333","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"In Progress","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:52:09.000+0000","updated":"2025-01-17T14:55:20.000+0000","resolved":null,"labels":[],"components":["kraft"],"comment_count":2,"comments":[{"id":"17907522","author":{"displayName":"PoAn Yang"},"body":"Hi alyssahuang, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~alyssahuang], if you're not working on this, may I take it? Thank you.","created":"2024-12-21T01:01:32.082+0000","updated":"2024-12-21T01:01:32.082+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17911224","author":{"displayName":"Alyssa Huang"},"body":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","body_raw":"I'm not working on this currently, this may involve changing LeaderState first btw, currently votedKey is lost on the transition to Leader (which is fine for correctness, but not consistent with all other state transitions within the same epoch).","created":"2025-01-08T20:16:32.339+0000","updated":"2025-01-08T20:16:32.339+0000","updateAuthor":{"displayName":"Alyssa Huang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Fix KafkaRaftClient complexity","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Resolved","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Major","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Remove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Fixed","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix KafkaRaftClient complexity\n\nRemove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity\n\nConversation:\nuser: Hi can I take this issue?","output":"Hi can I take this issue?","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Remove suppressions for ClassDataAbstractionCoupling and JavaNCSS KafkaRaftClientTest also has suppressions for JavaNCSS (in suppressions.xml), ClassDataAbstractionCoupling, and ClassFanOutComplexity","output":"Fix KafkaRaftClient complexity","metadata":{"issue_id":"KAFKA-18332","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Jose Armando Garcia Sancio"},"reporter":{"displayName":"Alyssa Huang"},"created":"2024-12-20T17:32:29.000+0000","updated":"2025-04-17T14:41:41.000+0000","resolved":"2025-04-17T14:41:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17927585","author":{"displayName":"Ryan Ye"},"body":"Hi can I take this issue?","body_raw":"Hi can I take this issue?","created":"2025-02-17T02:11:33.564+0000","updated":"2025-02-17T02:11:33.564+0000","updateAuthor":{"displayName":"Ryan Ye"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Update process.roles to required configuration\n\nIn Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","output":"Update process.roles to required configuration","metadata":{"issue_id":"KAFKA-18331","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-20T15:16:32.000+0000","updated":"2025-02-20T10:32:23.000+0000","resolved":"2025-01-16T08:09:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update process.roles to required configuration\n\nIn Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","output":"Resolved","metadata":{"issue_id":"KAFKA-18331","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-20T15:16:32.000+0000","updated":"2025-02-20T10:32:23.000+0000","resolved":"2025-01-16T08:09:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update process.roles to required configuration\n\nIn Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","output":"Major","metadata":{"issue_id":"KAFKA-18331","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-20T15:16:32.000+0000","updated":"2025-02-20T10:32:23.000+0000","resolved":"2025-01-16T08:09:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update process.roles to required configuration\n\nIn Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","output":"In Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","metadata":{"issue_id":"KAFKA-18331","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-20T15:16:32.000+0000","updated":"2025-02-20T10:32:23.000+0000","resolved":"2025-01-16T08:09:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update process.roles to required configuration\n\nIn Kafka 4.0, the ZK will be removed. We should update process.roles as required configuration.","output":"Fixed","metadata":{"issue_id":"KAFKA-18331","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-20T15:16:32.000+0000","updated":"2025-02-20T10:32:23.000+0000","resolved":"2025-01-16T08:09:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"Update documentation to remove controller deployment limitations","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"Resolved","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"Major","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"This https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cl","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"Fixed","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Update documentation to remove controller deployment limitations\n\nThis https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommend\n\nConversation:\nuser: Hi jsancio I would take over this issue, thanks\nassistant: frankvicky thanks for working on this.\nassistant: frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9\nassistant: frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\nassistant: https://github.com/apache/kafka/pull/18281","output":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This https://kafka.apache.org/documentation/#kraft_deployment has the following bullet point: {quote} For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers is not recommended in critical environments. In the rare case of a partial network failure it is possible for the cluster metadata quorum to become unavailable. This limitation will be addressed in a future release of Kafka {quote} The limitation mentioned should be removed.","output":"Update documentation to remove controller deployment limitations","metadata":{"issue_id":"KAFKA-18330","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"José Armando García Sancio"},"created":"2024-12-19T20:30:45.000+0000","updated":"2025-02-25T10:01:28.000+0000","resolved":"2025-01-14T21:39:15.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":6,"comments":[{"id":"17907232","author":{"displayName":"TengYao Chi"},"body":"Hi jsancio I would take over this issue, thanks","body_raw":"Hi [~jsancio] \r\n\r\nI would take over this issue, thanks","created":"2024-12-20T00:08:57.280+0000","updated":"2024-12-20T00:08:57.280+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17907422","author":{"displayName":"José Armando García Sancio"},"body":"frankvicky thanks for working on this.","body_raw":"[~frankvicky] thanks for working on this.","created":"2024-12-20T15:59:32.010+0000","updated":"2024-12-20T15:59:32.010+0000","updateAuthor":{"displayName":"José Armando García Sancio"}},{"id":"17910013","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky opened a new pull request, #659: URL: https://github.com/apache/kafka-site/pull/659 JIRA: KAFKA-18330 This doc patch also needs be backported to 3.9","body_raw":"frankvicky opened a new pull request, #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n   JIRA: KAFKA-18330\r\n   This doc patch also needs be backported to 3.9\n\n\n","created":"2025-01-06T06:36:31.434+0000","updated":"2025-01-06T06:36:31.434+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17910014","author":{"displayName":"ASF GitHub Bot"},"body":"frankvicky commented on PR #659: URL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401 Preview: ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)","body_raw":"frankvicky commented on PR #659:\nURL: https://github.com/apache/kafka-site/pull/659#issuecomment-2572403401\n\n   Preview:\r\n   ![image](https://github.com/user-attachments/assets/a58caaf3-1bc1-4113-a3f2-d7ff4445611a)\r\n   \n\n\n","created":"2025-01-06T06:41:29.880+0000","updated":"2025-01-06T06:41:29.880+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}},{"id":"17911235","author":{"displayName":"Alyssa Huang"},"body":"https://github.com/apache/kafka/pull/18281","body_raw":"https://github.com/apache/kafka/pull/18281","created":"2025-01-08T20:53:26.860+0000","updated":"2025-01-08T20:53:26.860+0000","updateAuthor":{"displayName":"Alyssa Huang"}},{"id":"17930243","author":{"displayName":"ASF GitHub Bot"},"body":"chia7712 merged PR #659: URL: https://github.com/apache/kafka-site/pull/659","body_raw":"chia7712 merged PR #659:\nURL: https://github.com/apache/kafka-site/pull/659\n\n\n","created":"2025-02-25T10:01:28.726+0000","updated":"2025-02-25T10:01:28.726+0000","updateAuthor":{"displayName":"ASF GitHub Bot"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.","output":"Delete old group coordinator (KIP-848)","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.","output":"Resolved","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.","output":"Major","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.","output":"In 4.2.0, we should be able to delete the old group coordinator.","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.","output":"Fixed","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Delete old group coordinator (KIP-848)\n\nIn 4.2.0, we should be able to delete the old group coordinator.\n\nConversation:\nuser: Why 4.2.0? What's stopping us from doing it now?","output":"Why 4.2.0? What's stopping us from doing it now?","metadata":{"issue_id":"KAFKA-18329","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:56:23.000+0000","updated":"2025-03-21T15:08:01.000+0000","resolved":"2025-03-21T15:08:01.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17930718","author":{"displayName":"Ismael Juma"},"body":"Why 4.2.0? What's stopping us from doing it now?","body_raw":"Why 4.2.0? What's stopping us from doing it now?","created":"2025-02-26T15:07:35.673+0000","updated":"2025-02-26T15:07:35.673+0000","updateAuthor":{"displayName":"Ismael Juma"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)\n\nThis Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","output":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)\n\nThis Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","output":"Open","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)\n\nThis Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","output":"Major","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)\n\nThis Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","output":"This Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)\n\nThis Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.\n\nConversation:\nuser: It will be easier to introduce the new API versions when the old group coordinator is gone.","output":"It will be easier to introduce the new API versions when the old group coordinator is gone.","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This Jira tracks the implementation of the support for topic ids in the Offset Fetch/Commit APIs as defined in KIP-848.","output":"Support for Topic Ids in Offset Fetch/Commit APIs (KIP-848)","metadata":{"issue_id":"KAFKA-18328","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:50:56.000+0000","updated":"2025-03-21T16:39:20.000+0000","resolved":null,"labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907172","author":{"displayName":"David Jacot"},"body":"It will be easier to introduce the new API versions when the old group coordinator is gone.","body_raw":"It will be easier to introduce the new API versions when the old group coordinator is gone.","created":"2024-12-19T19:57:37.221+0000","updated":"2024-12-19T19:57:37.221+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Client Side Assignors (KIP-848)\n\nThis Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","output":"Client Side Assignors (KIP-848)","metadata":{"issue_id":"KAFKA-18327","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:46:57.000+0000","updated":"2024-12-19T19:55:15.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Client Side Assignors (KIP-848)\n\nThis Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","output":"Open","metadata":{"issue_id":"KAFKA-18327","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:46:57.000+0000","updated":"2024-12-19T19:55:15.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Client Side Assignors (KIP-848)\n\nThis Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","output":"Major","metadata":{"issue_id":"KAFKA-18327","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:46:57.000+0000","updated":"2024-12-19T19:55:15.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Client Side Assignors (KIP-848)\n\nThis Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","output":"This Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","metadata":{"issue_id":"KAFKA-18327","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:46:57.000+0000","updated":"2024-12-19T19:55:15.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This Jira tracks the tasks related to adding support for client side assignors as defined in KIP-848.","output":"Client Side Assignors (KIP-848)","metadata":{"issue_id":"KAFKA-18327","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T19:46:57.000+0000","updated":"2024-12-19T19:55:15.000+0000","resolved":null,"labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 bytes big endian val store: KeyValueStore[(Int, String), String] = ??? // store contents // (1, \"A\") -> \"A\" // (1, \"B\") -> \"B\" // using put instead of delete to avoid reading previous value store.put((1, \"B\"), null) // reading all key value pairs using key prefix val result: List[KeyValue[(Int, String), String]] = store.prefixScan(1, keyPrefixSerializer).asScala.toList // expected result // (1, \"A\") -> \"A\" // actual result // (1, \"A\") -> \"A\" // (1, \"B\") -> \"B\" (was previously deleted, but returned by the iterator){code} I tried to come up with a unit test for MergedSortedCacheKeyValueBytesStoreIterator (returned by KeyValueStore.prefixScan and other methods like range, all) in order to reproduce the behavior. And it also showed that the iterator returns more items than expected if I delete a larger key: {code:java} @Test public void shouldSkipAllDeletedFromCache1() { final byte[][] bytes = {{0}, {1}}; for (final byte[] aByte : bytes) { store.put(Bytes.wrap(aByte), aByte); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); // simulate key deletion from store that is cached try (final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator()) { assertArrayEquals(bytes[0], iterator.next().key.get()); assertFalse(iterator.hasNext()); // org.opentest4j.AssertionFailedError: expected: but was: } }{code} But if I delete a smaller key the test is successful: {code:java} @Test public void shouldSkipAllDeletedFromCache0() { final byte[][] bytes = {{0}, {1}}; for (final byte[] aByte : bytes) { store.put(Bytes.wrap(aByte), aByte); } cache.put(namespace, Bytes.wrap(bytes[0]), new LRUCacheEntry(null)); try (final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator()) { assertArrayEquals(bytes[1], iterator.next().key.get()); assertFalse(iterator.hasNext()); } } Could someone help me verify if it is a bug or am I missing something? Thank you.{code}","output":"Cached stores may return deleted values","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 ","output":"Resolved","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 ","output":"Critical","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 ","output":"Reported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected ","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 ","output":"Fixed","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Cached stores may return deleted values\n\nReported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basic\n\nConversation:\nuser: Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b\nassistant: I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.\nassistant: guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed","output":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Reported in community Slack by Stanislav Savulchik. I've attached a patch fix, and am waiting for the reporter to submit a PR if they want to since they were the first to identify it! It affects basically every version of Kafka Streams out there... ----- Hi everyone, I’m investigating an unexpected behavior of a KeyValueStore.prefixScan method that sometimes returns previously deleted keys if caching is enabled. Example pseudocode: {code:java} val keyPrefixSerializer: Serializer[Int] = ??? // 4 bytes big endian val store: KeyValueStore[(Int, String), String] = ??? // store contents // (1, \"A\") -> \"A\" // (1, \"B\") -> \"B\" // using put instead of delete to avoid reading previous value store.put((1, \"B\"), null) // reading all key value pairs using key prefix val result: List[KeyValue[(Int, Stri","output":"Cached stores may return deleted values","metadata":{"issue_id":"KAFKA-18326","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Critical","assignee":{"displayName":"Almog Gavra"},"reporter":{"displayName":"Almog Gavra"},"created":"2024-12-19T17:41:02.000+0000","updated":"2025-01-08T07:17:43.000+0000","resolved":"2025-01-08T07:17:43.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":3,"comments":[{"id":"17907138","author":{"displayName":"Almog Gavra"},"body":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail. {code:java} diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java index a678908b04..70e9ca8786 100644 --- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java +++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java @@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest { for (final byte[] aByte : bytes) { final Bytes aBytes = Bytes.wrap(aByte); store.put(aBytes, aByte); - cache.put(namespace, aBytes, new LRUCacheEntry(aByte)); } cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null)); cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","body_raw":"Note that existing tests did not capture this because they first inserted non-tombstone entries into the cache. The following change causes the test to fail.\r\n{code:java}\r\ndiff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\nindex a678908b04..70e9ca8786 100644\r\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java\r\n@@ -154,7 +154,6 @@ public class MergedSortedCacheKeyValueBytesStoreIteratorTest {\r\n         for (final byte[] aByte : bytes) {\r\n             final Bytes aBytes = Bytes.wrap(aByte);\r\n             store.put(aBytes, aByte);\r\n-            cache.put(namespace, aBytes, new LRUCacheEntry(aByte));\r\n         }\r\n         cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));\r\n         cache.put(namespace, Bytes.wrap(bytes[2]), new LRUCacheEntry(null)); {code}","created":"2024-12-19T17:42:57.205+0000","updated":"2024-12-19T17:42:57.205+0000","updateAuthor":{"displayName":"Almog Gavra"}},{"id":"17909090","author":{"displayName":"Guozhang Wang"},"body":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","body_raw":"I've just tested the modified unit test myself and confirmed it, thanks for reporting! Will take a look at the PR soon.","created":"2024-12-31T18:52:14.300+0000","updated":"2024-12-31T18:52:14.300+0000","updateAuthor":{"displayName":"Guozhang Wang"}},{"id":"17909114","author":{"displayName":"Stanislav Savulchik"},"body":"guozhang , thanks for reviewing the PR. Let me answer your https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288 here: {quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache. {quote} Indeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug. There is a topology with two custom processors and a shared key value state store connected to both processors. The first processor handles a very small rate of input records and does (in order): # store.put(key, value or null) # store.prefixScan(keyPrefix) The second processor handles a very large rate of input records (various keyPrefixes) and does (in order): # store.prefixScan(keyPrefix) # store.putAll(keyValues) I believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor. I hope that helps to clarify the scenario I had.","body_raw":"[~guozhang] , thanks for reviewing the PR.\r\n\r\nLet me answer your [question|https://github.com/apache/kafka/pull/18287#pullrequestreview-2526664288] here:\r\n{quote}For the original reported scenario where if deleting the second key \"1\" this bug gets exposed bug if deleting the first key \"0\" this bug will not be exposed, I'm a bit unclear how it would be the case. The only case I can think of is that the cache is small enough such that both \"0\" and \"1\" are flushed to store while \"1\" is in the cache.\r\n{quote}\r\nIndeed, my original example didn't explain how I created the state store records in the first place before deleting one of them to reproduce the bug.\r\n\r\nThere is a topology with two custom processors and a shared key value state store connected to both processors.\r\n\r\nThe first processor handles a very small rate of input records and does (in order):\r\n # store.put(key, value or null)\r\n # store.prefixScan(keyPrefix)\r\n\r\nThe second processor handles a very large rate of input records (various keyPrefixes) and does (in order):\r\n # store.prefixScan(keyPrefix)\r\n # store.putAll(keyValues)\r\n\r\nI believe that by the time the first processor did a delete in a form of store.put(key, null) and store.prefixScan(keyPrefix) that key wasn't in the cache (and other keys with its keyPrefix) because it had already been evicted from cache by other reads and updates in the second processor.\r\n\r\nI hope that helps to clarify the scenario I had. ","created":"2025-01-01T07:07:14.373+0000","updated":"2025-01-01T07:07:14.373+0000","updateAuthor":{"displayName":"Stanislav Savulchik"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add StickyTaskAssignor\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18322","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-19T16:24:10.000+0000","updated":"2025-04-22T06:54:23.000+0000","resolved":"2025-02-05T16:55:20.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17946318","author":{"displayName":"Alieh Saeedi"},"body":"PR: https://github.com/apache/kafka/pull/18652","body_raw":"PR: https://github.com/apache/kafka/pull/18652","created":"2025-04-22T06:54:23.866+0000","updated":"2025-04-22T06:54:23.866+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add StickyTaskAssignor\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18322","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-19T16:24:10.000+0000","updated":"2025-04-22T06:54:23.000+0000","resolved":"2025-02-05T16:55:20.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17946318","author":{"displayName":"Alieh Saeedi"},"body":"PR: https://github.com/apache/kafka/pull/18652","body_raw":"PR: https://github.com/apache/kafka/pull/18652","created":"2025-04-22T06:54:23.866+0000","updated":"2025-04-22T06:54:23.866+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add StickyTaskAssignor\n\n","output":"","metadata":{"issue_id":"KAFKA-18322","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-19T16:24:10.000+0000","updated":"2025-04-22T06:54:23.000+0000","resolved":"2025-02-05T16:55:20.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17946318","author":{"displayName":"Alieh Saeedi"},"body":"PR: https://github.com/apache/kafka/pull/18652","body_raw":"PR: https://github.com/apache/kafka/pull/18652","created":"2025-04-22T06:54:23.866+0000","updated":"2025-04-22T06:54:23.866+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add StickyTaskAssignor\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18322","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-19T16:24:10.000+0000","updated":"2025-04-22T06:54:23.000+0000","resolved":"2025-02-05T16:55:20.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17946318","author":{"displayName":"Alieh Saeedi"},"body":"PR: https://github.com/apache/kafka/pull/18652","body_raw":"PR: https://github.com/apache/kafka/pull/18652","created":"2025-04-22T06:54:23.866+0000","updated":"2025-04-22T06:54:23.866+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add StickyTaskAssignor\n\n\n\nConversation:\nuser: PR: https://github.com/apache/kafka/pull/18652","output":"PR: https://github.com/apache/kafka/pull/18652","metadata":{"issue_id":"KAFKA-18322","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-19T16:24:10.000+0000","updated":"2025-04-22T06:54:23.000+0000","resolved":"2025-02-05T16:55:20.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17946318","author":{"displayName":"Alieh Saeedi"},"body":"PR: https://github.com/apache/kafka/pull/18652","body_raw":"PR: https://github.com/apache/kafka/pull/18652","created":"2025-04-22T06:54:23.866+0000","updated":"2025-04-22T06:54:23.866+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"Ensure that assignors are at the right place","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"Resolved","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"Blocker","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"We should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"Fixed","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Ensure that assignors are at the right place\n\nWe should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.\n\nConversation:\nuser: That seems like a bad idea: why don't we have short names for the built in assigners?\nassistant: ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","output":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We should ensure that assignors are at the right place because we cannot change them anymore in the future as they full class names are references in the configs.","output":"Ensure that assignors are at the right place","metadata":{"issue_id":"KAFKA-18320","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Blocker","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-19T15:27:43.000+0000","updated":"2025-02-07T15:36:51.000+0000","resolved":"2025-01-31T15:52:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17922891","author":{"displayName":"Ismael Juma"},"body":"That seems like a bad idea: why don't we have short names for the built in assigners?","body_raw":"That seems like a bad idea: why don't we have short names for the built in assigners?","created":"2025-02-01T02:34:31.146+0000","updated":"2025-02-01T02:35:21.241+0000","updateAuthor":{"displayName":"Ismael Juma"}},{"id":"17924938","author":{"displayName":"David Jacot"},"body":"ijuma Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","body_raw":"[~ijuma] Thanks for the suggestion. I opened [https://github.com/apache/kafka/pull/18832] to improve it.","created":"2025-02-07T15:36:51.679+0000","updated":"2025-02-07T15:36:51.679+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.","output":"Remove zookeeper.connect from RemoteLogManagerTest","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.","output":"Resolved","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.","output":"Major","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.","output":"This test still uses zookeeper.connect in its configuration.","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.","output":"Fixed","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove zookeeper.connect from RemoteLogManagerTest\n\nThis test still uses zookeeper.connect in its configuration.\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","output":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","metadata":{"issue_id":"KAFKA-18317","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"Mickael Maison"},"created":"2024-12-19T13:31:23.000+0000","updated":"2024-12-28T13:55:42.000+0000","resolved":"2024-12-28T13:55:42.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908607","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433 4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","body_raw":"trunk: https://github.com/apache/kafka/commit/bc7a1a8969b234f080dfed7553860389ad901433\r\n\r\n4.0: https://github.com/apache/kafka/commit/27815fdb1a7ebcd32bde16603c6c6e69ecf5c9b6","created":"2024-12-28T13:55:42.548+0000","updated":"2024-12-28T13:55:42.548+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Resolved","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Major","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fixed","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","output":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in ConnectionQuotasTest","metadata":{"issue_id":"KAFKA-18316","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:37:37.000+0000","updated":"2024-12-29T18:24:33.000+0000","resolved":"2024-12-29T18:24:33.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908731","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a 4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","body_raw":"trunk: https://github.com/apache/kafka/commit/f2ae20a5c10c1f518a07b61d6c0e41daef41146a\r\n\r\n4.0: https://github.com/apache/kafka/commit/400d7d36bc430713a5cb4588b72cd95dac8d1b49","created":"2024-12-29T18:24:33.604+0000","updated":"2024-12-29T18:24:33.604+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Resolved","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Major","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fixed","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","output":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in DynamicBrokerConfigTest, ReplicaManagerTest, DescribeTopicPartitionsRequestHandlerTest, KafkaConfigTest","metadata":{"issue_id":"KAFKA-18315","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:36:52.000+0000","updated":"2024-12-29T16:51:17.000+0000","resolved":"2024-12-29T16:51:17.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908722","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d 4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","body_raw":"trunk: https://github.com/apache/kafka/commit/1156d5c29452eb9804db7521207566d02095107d\r\n\r\n4.0: https://github.com/apache/kafka/commit/6d6b206036907491b76510a18886e2a898bb2de3","created":"2024-12-29T16:51:17.795+0000","updated":"2024-12-29T16:51:17.795+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Resolved","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Major","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fixed","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest\n\nPlease check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","output":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Please check these fail tests in this CI [https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219]","output":"Fix to Kraft or remove tests associate with Zk Broker config in KafkaApisTest","metadata":{"issue_id":"KAFKA-18314","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:35:17.000+0000","updated":"2024-12-29T11:06:32.000+0000","resolved":"2024-12-29T11:06:32.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908692","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7 4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","body_raw":"trunk: https://github.com/apache/kafka/commit/e9a03b3a86b423beca4bef0aa52811452f056ca7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0864f8155a5214bc7bbd955ad456f664b709c77a","created":"2024-12-29T11:06:32.108+0000","updated":"2024-12-29T11:06:32.108+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Resolved","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Major","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Please check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Fixed","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest\n\nPlease check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","output":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Please check these fail tests in this CI https://github.com/apache/kafka/actions/runs/12392828301/job/34593103321?pr=18219","output":"Fix to Kraft or remove tests associate with Zk Broker config in SocketServerTest","metadata":{"issue_id":"KAFKA-18313","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-19T12:33:58.000+0000","updated":"2024-12-30T02:17:07.000+0000","resolved":"2024-12-30T02:17:07.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908733","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b 4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","body_raw":"trunk: https://github.com/apache/kafka/commit/03f16f368be0b8c6d043b136c0d1db120c7fdb4b\r\n\r\n4.0: https://github.com/apache/kafka/commit/baa492ad7c132992caa370a410e510b04540ba1a","created":"2024-12-29T18:32:04.250+0000","updated":"2024-12-29T18:32:04.250+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add \"entityType\": \"topicName\" to \"SubscribedTopicNames\" in ShareGroupHeartbeatRequest.json\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18312","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chirag Wadhwa"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2024-12-19T12:08:10.000+0000","updated":"2024-12-21T17:26:41.000+0000","resolved":"2024-12-21T17:26:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907628","author":{"displayName":"Chirag Wadhwa"},"body":"The required PR has been merged","body_raw":"The required PR has been merged","created":"2024-12-21T17:26:41.234+0000","updated":"2024-12-21T17:26:41.234+0000","updateAuthor":{"displayName":"Chirag Wadhwa"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add \"entityType\": \"topicName\" to \"SubscribedTopicNames\" in ShareGroupHeartbeatRequest.json\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18312","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chirag Wadhwa"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2024-12-19T12:08:10.000+0000","updated":"2024-12-21T17:26:41.000+0000","resolved":"2024-12-21T17:26:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907628","author":{"displayName":"Chirag Wadhwa"},"body":"The required PR has been merged","body_raw":"The required PR has been merged","created":"2024-12-21T17:26:41.234+0000","updated":"2024-12-21T17:26:41.234+0000","updateAuthor":{"displayName":"Chirag Wadhwa"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add \"entityType\": \"topicName\" to \"SubscribedTopicNames\" in ShareGroupHeartbeatRequest.json\n\n","output":"","metadata":{"issue_id":"KAFKA-18312","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chirag Wadhwa"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2024-12-19T12:08:10.000+0000","updated":"2024-12-21T17:26:41.000+0000","resolved":"2024-12-21T17:26:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907628","author":{"displayName":"Chirag Wadhwa"},"body":"The required PR has been merged","body_raw":"The required PR has been merged","created":"2024-12-21T17:26:41.234+0000","updated":"2024-12-21T17:26:41.234+0000","updateAuthor":{"displayName":"Chirag Wadhwa"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add \"entityType\": \"topicName\" to \"SubscribedTopicNames\" in ShareGroupHeartbeatRequest.json\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18312","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chirag Wadhwa"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2024-12-19T12:08:10.000+0000","updated":"2024-12-21T17:26:41.000+0000","resolved":"2024-12-21T17:26:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907628","author":{"displayName":"Chirag Wadhwa"},"body":"The required PR has been merged","body_raw":"The required PR has been merged","created":"2024-12-21T17:26:41.234+0000","updated":"2024-12-21T17:26:41.234+0000","updateAuthor":{"displayName":"Chirag Wadhwa"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add \"entityType\": \"topicName\" to \"SubscribedTopicNames\" in ShareGroupHeartbeatRequest.json\n\n\n\nConversation:\nuser: The required PR has been merged","output":"The required PR has been merged","metadata":{"issue_id":"KAFKA-18312","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chirag Wadhwa"},"reporter":{"displayName":"Chirag Wadhwa"},"created":"2024-12-19T12:08:10.000+0000","updated":"2024-12-21T17:26:41.000+0000","resolved":"2024-12-21T17:26:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907628","author":{"displayName":"Chirag Wadhwa"},"body":"The required PR has been merged","body_raw":"The required PR has been merged","created":"2024-12-21T17:26:41.234+0000","updated":"2024-12-21T17:26:41.234+0000","updateAuthor":{"displayName":"Chirag Wadhwa"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Flaky AbstractCoordinatorTest","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Resolved","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Major","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Three tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Fixed","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky AbstractCoordinatorTest\n\nThree tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion\n\nConversation:\nuser: The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. ","output":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. ","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Three tests are flaky with about 5% failure rate on trunk. * testWakeupAfterSyncGroupReceived * testWakeupAfterSyncGroupReceivedExternalCompletion * testWakeupAfterSyncGroupSentExternalCompletion","output":"Flaky AbstractCoordinatorTest","metadata":{"issue_id":"KAFKA-18310","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-18T21:46:02.000+0000","updated":"2025-03-10T15:58:12.000+0000","resolved":"2025-03-10T15:58:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17915738","author":{"displayName":"PoAn Yang"},"body":"The root cause of this Jira is like following: AbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently. {noformat} public boolean poll(RequestFuture future, Timer timer, boolean disableWakeup) { do { poll(timer, future, disableWakeup); try { System.err.println(\"sleeping.....\"); Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } while (!future.isDone() && timer.notExpired()); return future.isDone(); }{noformat} [0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481] [1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","body_raw":"The root cause of this Jira is like following:\r\n\r\nAbstractCoordinator#joinGroupIfNeeded calls ConsumerNetworkClient#poll(RequestFuture, Timer) [0]. It calls another method with disableWakeup = false. If joinFuture is not done, ConsumerNetworkClient does poll multiple times before timer is expired [1]. If joinFuture finishes too fast, the SyncGroupRequest can't be handled in AbstractCoordinator#ensureActiveGroup, so it doesn't throw WakeupException. I change ConsumerNetworkClient#poll(RequestFuture, Timer, boolean) like following and I can reproduce flaky frequently.\r\n{noformat}\r\npublic boolean poll(RequestFuture<?> future, Timer timer, boolean disableWakeup) {\r\n    do {\r\n        poll(timer, future, disableWakeup);\r\n        try {\r\n            System.err.println(\"sleeping.....\");\r\n            Thread.sleep(1000L);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    } while (!future.isDone() && timer.notExpired());\r\n    return future.isDone();\r\n}{noformat}\r\n[0] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L480-L481]\r\n[1] [https://github.com/apache/kafka/blob/3d49159c841e7653e3951af4ffc3524d17339295/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L230-L235]","created":"2025-01-21T15:36:42.591+0000","updated":"2025-01-21T15:36:42.591+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"Flaky test report includes disabled or removed tests.","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"Resolved","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"Major","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"I noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"Fixed","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Flaky test report includes disabled or removed tests.\n\nI noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows \n\nConversation:\nuser: cc santhoshct\nassistant: I will take a look at this.","output":"I will take a look at this.","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I noticed in this report [https://github.com/apache/kafka/actions/runs/12398964575] that two of the problematic tests have been removed or disabled on trunk. Following their links to Develocity shows that there is actually no recent data.","output":"Flaky test report includes disabled or removed tests.","metadata":{"issue_id":"KAFKA-18307","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-18T18:39:27.000+0000","updated":"2025-01-08T08:36:47.000+0000","resolved":"2025-01-06T15:55:22.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906823","author":{"displayName":"David Arthur"},"body":"cc santhoshct","body_raw":"cc [~santhoshct] ","created":"2024-12-18T18:39:42.130+0000","updated":"2024-12-18T18:39:42.130+0000","updateAuthor":{"displayName":"David Arthur"}},{"id":"17907284","author":{"displayName":"Santhosh C T"},"body":"I will take a look at this.","body_raw":"I will take a look at this.","created":"2024-12-20T06:36:33.391+0000","updated":"2024-12-20T06:36:33.391+0000","updateAuthor":{"displayName":"Santhosh C T"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers\n\nMake an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers\n\nMake an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"Resolved","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers\n\nMake an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"Major","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: validate controller.listener.names is not in inter.broker.listener.name for kcontrollers\n\nMake an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"Make an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers\n\nMake an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"Fixed","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Make an exception for when `inter.broker.listener.name` is not set and is instead inferred from the security protocol.","output":"validate controller.listener.names is not in inter.broker.listener.name for kcontrollers","metadata":{"issue_id":"KAFKA-18305","project":"KAFKA","project_name":"Kafka","issue_type":"Task","status":"Resolved","priority":"Major","assignee":{"displayName":"Kevin Wu"},"reporter":{"displayName":"Kevin Wu"},"created":"2024-12-18T16:21:51.000+0000","updated":"2024-12-20T16:10:48.000+0000","resolved":"2024-12-20T16:10:48.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Promote Kafka Connect data package as a dedicated module\n\nThe current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easier to create consistent data models across different applications * *Cleaner project structure* * ** Allows independent versioning of data format components ** Creates a clearer separation of different parts of the Kafka ecosystem * *Become standard for other tools and frameworks* * ** It makes it easier for third-party tools to work with Kafka's data models ** It can be used as a generic data format ** It helps different streaming platforms work together more smoothly","output":"Promote Kafka Connect data package as a dedicated module","metadata":{"issue_id":"KAFKA-18299","project":"KAFKA","project_name":"Kafka","issue_type":"Wish","status":"Patch Available","priority":"Major","assignee":{"displayName":"Mario Fiore Vitale"},"reporter":{"displayName":"Mario Fiore Vitale"},"created":"2024-12-18T13:21:28.000+0000","updated":"2025-06-04T03:46:32.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Promote Kafka Connect data package as a dedicated module\n\nThe current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easier to create consistent data models across different applications * *Cleaner project structure* * ** Allows independent versioning of data format components ** Creates a clearer separation of different","output":"Patch Available","metadata":{"issue_id":"KAFKA-18299","project":"KAFKA","project_name":"Kafka","issue_type":"Wish","status":"Patch Available","priority":"Major","assignee":{"displayName":"Mario Fiore Vitale"},"reporter":{"displayName":"Mario Fiore Vitale"},"created":"2024-12-18T13:21:28.000+0000","updated":"2025-06-04T03:46:32.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Promote Kafka Connect data package as a dedicated module\n\nThe current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easier to create consistent data models across different applications * *Cleaner project structure* * ** Allows independent versioning of data format components ** Creates a clearer separation of different","output":"Major","metadata":{"issue_id":"KAFKA-18299","project":"KAFKA","project_name":"Kafka","issue_type":"Wish","status":"Patch Available","priority":"Major","assignee":{"displayName":"Mario Fiore Vitale"},"reporter":{"displayName":"Mario Fiore Vitale"},"created":"2024-12-18T13:21:28.000+0000","updated":"2025-06-04T03:46:32.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Promote Kafka Connect data package as a dedicated module\n\nThe current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easier to create consistent data models across different applications * *Cleaner project structure* * ** Allows independent versioning of data format components ** Creates a clearer separation of different","output":"The current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easie","metadata":{"issue_id":"KAFKA-18299","project":"KAFKA","project_name":"Kafka","issue_type":"Wish","status":"Patch Available","priority":"Major","assignee":{"displayName":"Mario Fiore Vitale"},"reporter":{"displayName":"Mario Fiore Vitale"},"created":"2024-12-18T13:21:28.000+0000","updated":"2025-06-04T03:46:32.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The current implementation restricts the usage of data classes, which prevents broader adoption and integration. By moving these classes to a separate module, we can achieve: * *More flexible for developers* * ** Developers can now use these data classes outside of Kafka Connect ** It makes it easier to create consistent data models across different applications * *Cleaner project structure* * ** Allows independent versioning of data format components ** Creates a clearer separation of different parts of the Kafka ecosystem * *Become standard for other tools and frameworks* * ** It makes it easier for third-party tools to work with Kafka's data models ** It can be used as a generic data format ** It helps different streaming platforms work together more smoothly","output":"Promote Kafka Connect data package as a dedicated module","metadata":{"issue_id":"KAFKA-18299","project":"KAFKA","project_name":"Kafka","issue_type":"Wish","status":"Patch Available","priority":"Major","assignee":{"displayName":"Mario Fiore Vitale"},"reporter":{"displayName":"Mario Fiore Vitale"},"created":"2024-12-18T13:21:28.000+0000","updated":"2025-06-04T03:46:32.000+0000","resolved":null,"labels":[],"components":["connect"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"Resolved","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"Major","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"org.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGr","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"Fixed","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState\n\norg.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api\n\nConversation:\nuser: Just failed again on [https://github.com/apache/kafka/pull/18402]\nassistant: Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]\nassistant: Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?ex\nassistant: Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.\nassistant: Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","output":"Backporting to 4.0 is not worth it at our current state in the release.","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"org.opentest4j.AssertionFailedError: Expected the offset for partition 0 to eventually become 1. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroupsDeprecatedConsumerGroupState(PlaintextAdminIntegrationTest.scala:2360) at java.base@23.0.1/java.lang.reflect.Method.invoke(Method.java:580)","output":"Fix flaky PlaintextAdminIntegrationTest#testConsumerGroupsDeprecatedConsumerGroupState","metadata":{"issue_id":"KAFKA-18298","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T11:00:58.000+0000","updated":"2025-02-14T12:38:48.000+0000","resolved":"2025-02-14T07:20:30.000+0000","resolution":"Fixed","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":8,"comments":[{"id":"17910478","author":{"displayName":"Matthias J. Sax"},"body":"Just failed again on [https://github.com/apache/kafka/pull/18402]","body_raw":"Just failed again on [https://github.com/apache/kafka/pull/18402] ","created":"2025-01-07T02:24:20.434+0000","updated":"2025-01-07T02:24:20.434+0000","updateAuthor":{"displayName":"Matthias J. Sax"}},{"id":"17912252","author":{"displayName":"Yu-Lin Chen"},"body":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.) * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","body_raw":"Locally validated that the issue was fixed after this PR. (No error in 200 loops.) (The tests had previously failed within 20 loops.)\r\n * [https://github.com/apache/kafka/commit/48b522fe86984db32556e48b799e8797ae2236ba]","created":"2025-01-12T09:57:11.839+0000","updated":"2025-01-12T09:58:56.228+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912511","author":{"displayName":"Yu-Lin Chen"},"body":"Reopen this Jira becasue other flaky issues found in CI. # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2) # org.opentest4j.AssertionFailedError: expected: but was: (https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1) The flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch]. (Uncomment the sleep 1 sec) The root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","body_raw":"Reopen this Jira becasue other flaky issues found in CI.\r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3> ([Report|https://ge.apache.org/s/ndoj6s2stb446/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?expanded-stacktrace=WyIxIl0&top-execution=2])\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false> ([Report|https://ge.apache.org/s/kh3jze2tc5qeu/tests/task/:core:test/details/kafka.api.PlaintextAdminIntegrationTest/testConsumerGroupsDeprecatedConsumerGroupState(String%2C%20String)%5B1%5D?top-execution=1])\r\n\r\nThe flaky can't be reproduce in my local, but can be simulated with the attached patch [^0001-Reproduce-KAFKA-18298-and-KAFKA-18297-in-ClassicConsumer.patch].  (Uncomment the sleep 1 sec)\r\n\r\nThe root cause is that the member rejoining group after it was removed from group. Same issue occur on KAFKA-18297 too.","created":"2025-01-13T13:16:16.757+0000","updated":"2025-01-13T13:17:51.790+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912519","author":{"displayName":"Yu-Lin Chen"},"body":"Created a PR to fix the flaky in ClassicConsumer. * https://github.com/apache/kafka/pull/18506 I would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","body_raw":"Created a PR to fix the flaky in ClassicConsumer.\r\n * https://github.com/apache/kafka/pull/18506\r\n\r\nI would appreciate it if anyone has an idea to fix the same issue in AsyncConsumer.","created":"2025-01-13T13:34:25.639+0000","updated":"2025-01-13T13:34:25.639+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912628","author":{"displayName":"Yu-Lin Chen"},"body":"Create an improved PR to fully fix this flaky issue: * https://github.com/apache/kafka/pull/18513","body_raw":"Create an improved PR to fully fix this flaky issue:\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:01:29.656+0000","updated":"2025-01-13T18:01:29.656+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924806","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:13.018+0000","updated":"2025-02-07T08:16:13.018+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926995","author":{"displayName":"Chia-Ping Tsai"},"body":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","body_raw":"this PR is merged into trunk (https://github.com/apache/kafka/commit/2bbd25841e5b76d99e731286899b0c5aa07fd850). will backport it to 4.0","created":"2025-02-13T23:35:14.871+0000","updated":"2025-02-13T23:35:14.871+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17927056","author":{"displayName":"David Jacot"},"body":"Backporting to 4.0 is not worth it at our current state in the release.","body_raw":"Backporting to 4.0 is not worth it at our current state in the release.","created":"2025-02-14T07:19:40.662+0000","updated":"2025-02-14T07:19:40.662+0000","updateAuthor":{"displayName":"David Jacot"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18297","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T06:46:45.000+0000","updated":"2025-07-09T12:47:09.000+0000","resolved":"2025-02-13T23:34:10.000+0000","resolution":"Duplicate","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":14,"comments":[{"id":"17906620","author":{"displayName":"Chia-Ping Tsai"},"body":"case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0] at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065) Caused by: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","body_raw":"case_0\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]\r\n        at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345)\r\n        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440)\r\n        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117)\r\n        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)\r\n        at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065)\r\n\r\n        Caused by:\r\n        org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","created":"2024-12-18T06:46:57.942+0000","updated":"2024-12-18T06:46:57.942+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906621","author":{"displayName":"Chia-Ping Tsai"},"body":"case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31) at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","body_raw":"case 1\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)\r\n        at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","created":"2024-12-18T06:47:04.988+0000","updated":"2024-12-18T06:47:04.988+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910601","author":{"displayName":"David Jacot"},"body":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","body_raw":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","created":"2025-01-07T12:20:56.690+0000","updated":"2025-01-07T12:20:56.690+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17910634","author":{"displayName":"Mingdao Yang"},"body":"dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]","body_raw":"[~dajac] The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409] ","created":"2025-01-07T13:22:38.294+0000","updated":"2025-01-07T13:22:38.294+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17912404","author":{"displayName":"Jhen-Yung Hsu"},"body":"Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)","body_raw":"Case 1 still exists, and I found another case:\r\n\r\nGradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)\r\n        at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)\r\n ","created":"2025-01-13T08:34:20.692+0000","updated":"2025-01-13T08:34:20.692+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17912523","author":{"displayName":"Yu-Lin Chen"},"body":"[https://github.com/apache/kafka/pull/18506] Create a PR to fix below flaky errors in ClassicConsumer. # org.opentest4j.AssertionFailedError: expected: but was: # org.opentest4j.AssertionFailedError: expected: but was: The same issue also occurs with AsyncConsumer but I have no idea how to fix it. I would appreciate it if anyone come up with the solution. :(","body_raw":"[https://github.com/apache/kafka/pull/18506]\r\n\r\nCreate a PR to fix below flaky errors in ClassicConsumer. \r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\r\nThe same issue also occurs with AsyncConsumer but I have no idea how to fix it.\r\nI would appreciate it if anyone come up with the solution. :(","created":"2025-01-13T13:40:59.545+0000","updated":"2025-01-13T13:41:14.381+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912526","author":{"displayName":"David Jacot"},"body":"Yu-Lin Chen I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","body_raw":"[~Yu-Lin Chen] I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","created":"2025-01-13T13:45:11.041+0000","updated":"2025-01-13T13:45:11.041+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912533","author":{"displayName":"Yu-Lin Chen"},"body":"Hi dajac, Base on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario: * admin client remove member from consumer group * admin list group to check the member is removed. If we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing. Ref: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","body_raw":"Hi [~dajac], \r\n\r\nBase on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario:\r\n * admin client remove member from consumer group\r\n * admin list group to check the member is removed.\r\n\r\nIf we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing.\r\n\r\nRef: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","created":"2025-01-13T13:53:51.212+0000","updated":"2025-01-13T13:53:51.212+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912537","author":{"displayName":"David Jacot"},"body":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect. The member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","body_raw":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect.\r\n\r\nThe member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","created":"2025-01-13T13:59:26.498+0000","updated":"2025-01-13T13:59:26.498+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912546","author":{"displayName":"Yu-Lin Chen"},"body":"dajac Great idea, that make sense to me. I think I can rewirte the test to 1. Stop consumers before deleting 2. Validate the static members still in groups 3. Remove the static members through admin client 4. Validate the static members were removed I'll rewrite the PR. I really appreciate your suggestion. :)","body_raw":"[~dajac] Great idea, that make sense to me.\r\n\r\nI think I can rewirte the test to\r\n1. Stop consumers before deleting\r\n2. Validate the static members still in groups\r\n3. Remove the static members through admin client\r\n4. Validate the static members were removed\r\n\r\nI'll rewrite the PR. I really appreciate your suggestion. :)","created":"2025-01-13T14:13:44.842+0000","updated":"2025-01-13T14:13:44.842+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912631","author":{"displayName":"Yu-Lin Chen"},"body":"Created an improved PR based on the previous discussion * https://github.com/apache/kafka/pull/18513","body_raw":"Created an improved PR based on the previous discussion\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:02:41.697+0000","updated":"2025-01-13T18:02:41.697+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924807","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:23.175+0000","updated":"2025-02-07T08:16:23.175+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926994","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18298","body_raw":"see KAFKA-18298","created":"2025-02-13T23:34:10.741+0000","updated":"2025-02-13T23:34:10.741+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"18004142","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:47:09.934+0000","updated":"2025-07-09T12:47:09.934+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18297","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T06:46:45.000+0000","updated":"2025-07-09T12:47:09.000+0000","resolved":"2025-02-13T23:34:10.000+0000","resolution":"Duplicate","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":14,"comments":[{"id":"17906620","author":{"displayName":"Chia-Ping Tsai"},"body":"case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0] at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065) Caused by: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","body_raw":"case_0\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]\r\n        at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345)\r\n        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440)\r\n        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117)\r\n        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)\r\n        at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065)\r\n\r\n        Caused by:\r\n        org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","created":"2024-12-18T06:46:57.942+0000","updated":"2024-12-18T06:46:57.942+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906621","author":{"displayName":"Chia-Ping Tsai"},"body":"case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31) at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","body_raw":"case 1\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)\r\n        at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","created":"2024-12-18T06:47:04.988+0000","updated":"2024-12-18T06:47:04.988+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910601","author":{"displayName":"David Jacot"},"body":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","body_raw":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","created":"2025-01-07T12:20:56.690+0000","updated":"2025-01-07T12:20:56.690+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17910634","author":{"displayName":"Mingdao Yang"},"body":"dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]","body_raw":"[~dajac] The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409] ","created":"2025-01-07T13:22:38.294+0000","updated":"2025-01-07T13:22:38.294+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17912404","author":{"displayName":"Jhen-Yung Hsu"},"body":"Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)","body_raw":"Case 1 still exists, and I found another case:\r\n\r\nGradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)\r\n        at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)\r\n ","created":"2025-01-13T08:34:20.692+0000","updated":"2025-01-13T08:34:20.692+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17912523","author":{"displayName":"Yu-Lin Chen"},"body":"[https://github.com/apache/kafka/pull/18506] Create a PR to fix below flaky errors in ClassicConsumer. # org.opentest4j.AssertionFailedError: expected: but was: # org.opentest4j.AssertionFailedError: expected: but was: The same issue also occurs with AsyncConsumer but I have no idea how to fix it. I would appreciate it if anyone come up with the solution. :(","body_raw":"[https://github.com/apache/kafka/pull/18506]\r\n\r\nCreate a PR to fix below flaky errors in ClassicConsumer. \r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\r\nThe same issue also occurs with AsyncConsumer but I have no idea how to fix it.\r\nI would appreciate it if anyone come up with the solution. :(","created":"2025-01-13T13:40:59.545+0000","updated":"2025-01-13T13:41:14.381+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912526","author":{"displayName":"David Jacot"},"body":"Yu-Lin Chen I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","body_raw":"[~Yu-Lin Chen] I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","created":"2025-01-13T13:45:11.041+0000","updated":"2025-01-13T13:45:11.041+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912533","author":{"displayName":"Yu-Lin Chen"},"body":"Hi dajac, Base on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario: * admin client remove member from consumer group * admin list group to check the member is removed. If we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing. Ref: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","body_raw":"Hi [~dajac], \r\n\r\nBase on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario:\r\n * admin client remove member from consumer group\r\n * admin list group to check the member is removed.\r\n\r\nIf we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing.\r\n\r\nRef: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","created":"2025-01-13T13:53:51.212+0000","updated":"2025-01-13T13:53:51.212+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912537","author":{"displayName":"David Jacot"},"body":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect. The member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","body_raw":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect.\r\n\r\nThe member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","created":"2025-01-13T13:59:26.498+0000","updated":"2025-01-13T13:59:26.498+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912546","author":{"displayName":"Yu-Lin Chen"},"body":"dajac Great idea, that make sense to me. I think I can rewirte the test to 1. Stop consumers before deleting 2. Validate the static members still in groups 3. Remove the static members through admin client 4. Validate the static members were removed I'll rewrite the PR. I really appreciate your suggestion. :)","body_raw":"[~dajac] Great idea, that make sense to me.\r\n\r\nI think I can rewirte the test to\r\n1. Stop consumers before deleting\r\n2. Validate the static members still in groups\r\n3. Remove the static members through admin client\r\n4. Validate the static members were removed\r\n\r\nI'll rewrite the PR. I really appreciate your suggestion. :)","created":"2025-01-13T14:13:44.842+0000","updated":"2025-01-13T14:13:44.842+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912631","author":{"displayName":"Yu-Lin Chen"},"body":"Created an improved PR based on the previous discussion * https://github.com/apache/kafka/pull/18513","body_raw":"Created an improved PR based on the previous discussion\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:02:41.697+0000","updated":"2025-01-13T18:02:41.697+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924807","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:23.175+0000","updated":"2025-02-07T08:16:23.175+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926994","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18298","body_raw":"see KAFKA-18298","created":"2025-02-13T23:34:10.741+0000","updated":"2025-02-13T23:34:10.741+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"18004142","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:47:09.934+0000","updated":"2025-07-09T12:47:09.934+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups\n\n","output":"","metadata":{"issue_id":"KAFKA-18297","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T06:46:45.000+0000","updated":"2025-07-09T12:47:09.000+0000","resolved":"2025-02-13T23:34:10.000+0000","resolution":"Duplicate","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":14,"comments":[{"id":"17906620","author":{"displayName":"Chia-Ping Tsai"},"body":"case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0] at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065) Caused by: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","body_raw":"case_0\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]\r\n        at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345)\r\n        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440)\r\n        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117)\r\n        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)\r\n        at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065)\r\n\r\n        Caused by:\r\n        org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","created":"2024-12-18T06:46:57.942+0000","updated":"2024-12-18T06:46:57.942+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906621","author":{"displayName":"Chia-Ping Tsai"},"body":"case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31) at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","body_raw":"case 1\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)\r\n        at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","created":"2024-12-18T06:47:04.988+0000","updated":"2024-12-18T06:47:04.988+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910601","author":{"displayName":"David Jacot"},"body":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","body_raw":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","created":"2025-01-07T12:20:56.690+0000","updated":"2025-01-07T12:20:56.690+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17910634","author":{"displayName":"Mingdao Yang"},"body":"dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]","body_raw":"[~dajac] The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409] ","created":"2025-01-07T13:22:38.294+0000","updated":"2025-01-07T13:22:38.294+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17912404","author":{"displayName":"Jhen-Yung Hsu"},"body":"Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)","body_raw":"Case 1 still exists, and I found another case:\r\n\r\nGradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)\r\n        at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)\r\n ","created":"2025-01-13T08:34:20.692+0000","updated":"2025-01-13T08:34:20.692+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17912523","author":{"displayName":"Yu-Lin Chen"},"body":"[https://github.com/apache/kafka/pull/18506] Create a PR to fix below flaky errors in ClassicConsumer. # org.opentest4j.AssertionFailedError: expected: but was: # org.opentest4j.AssertionFailedError: expected: but was: The same issue also occurs with AsyncConsumer but I have no idea how to fix it. I would appreciate it if anyone come up with the solution. :(","body_raw":"[https://github.com/apache/kafka/pull/18506]\r\n\r\nCreate a PR to fix below flaky errors in ClassicConsumer. \r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\r\nThe same issue also occurs with AsyncConsumer but I have no idea how to fix it.\r\nI would appreciate it if anyone come up with the solution. :(","created":"2025-01-13T13:40:59.545+0000","updated":"2025-01-13T13:41:14.381+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912526","author":{"displayName":"David Jacot"},"body":"Yu-Lin Chen I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","body_raw":"[~Yu-Lin Chen] I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","created":"2025-01-13T13:45:11.041+0000","updated":"2025-01-13T13:45:11.041+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912533","author":{"displayName":"Yu-Lin Chen"},"body":"Hi dajac, Base on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario: * admin client remove member from consumer group * admin list group to check the member is removed. If we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing. Ref: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","body_raw":"Hi [~dajac], \r\n\r\nBase on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario:\r\n * admin client remove member from consumer group\r\n * admin list group to check the member is removed.\r\n\r\nIf we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing.\r\n\r\nRef: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","created":"2025-01-13T13:53:51.212+0000","updated":"2025-01-13T13:53:51.212+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912537","author":{"displayName":"David Jacot"},"body":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect. The member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","body_raw":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect.\r\n\r\nThe member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","created":"2025-01-13T13:59:26.498+0000","updated":"2025-01-13T13:59:26.498+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912546","author":{"displayName":"Yu-Lin Chen"},"body":"dajac Great idea, that make sense to me. I think I can rewirte the test to 1. Stop consumers before deleting 2. Validate the static members still in groups 3. Remove the static members through admin client 4. Validate the static members were removed I'll rewrite the PR. I really appreciate your suggestion. :)","body_raw":"[~dajac] Great idea, that make sense to me.\r\n\r\nI think I can rewirte the test to\r\n1. Stop consumers before deleting\r\n2. Validate the static members still in groups\r\n3. Remove the static members through admin client\r\n4. Validate the static members were removed\r\n\r\nI'll rewrite the PR. I really appreciate your suggestion. :)","created":"2025-01-13T14:13:44.842+0000","updated":"2025-01-13T14:13:44.842+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912631","author":{"displayName":"Yu-Lin Chen"},"body":"Created an improved PR based on the previous discussion * https://github.com/apache/kafka/pull/18513","body_raw":"Created an improved PR based on the previous discussion\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:02:41.697+0000","updated":"2025-01-13T18:02:41.697+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924807","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:23.175+0000","updated":"2025-02-07T08:16:23.175+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926994","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18298","body_raw":"see KAFKA-18298","created":"2025-02-13T23:34:10.741+0000","updated":"2025-02-13T23:34:10.741+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"18004142","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:47:09.934+0000","updated":"2025-07-09T12:47:09.934+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups\n\n","output":"Duplicate","metadata":{"issue_id":"KAFKA-18297","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T06:46:45.000+0000","updated":"2025-07-09T12:47:09.000+0000","resolved":"2025-02-13T23:34:10.000+0000","resolution":"Duplicate","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":14,"comments":[{"id":"17906620","author":{"displayName":"Chia-Ping Tsai"},"body":"case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0] at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065) Caused by: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","body_raw":"case_0\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]\r\n        at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345)\r\n        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440)\r\n        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117)\r\n        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)\r\n        at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065)\r\n\r\n        Caused by:\r\n        org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","created":"2024-12-18T06:46:57.942+0000","updated":"2024-12-18T06:46:57.942+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906621","author":{"displayName":"Chia-Ping Tsai"},"body":"case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31) at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","body_raw":"case 1\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)\r\n        at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","created":"2024-12-18T06:47:04.988+0000","updated":"2024-12-18T06:47:04.988+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910601","author":{"displayName":"David Jacot"},"body":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","body_raw":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","created":"2025-01-07T12:20:56.690+0000","updated":"2025-01-07T12:20:56.690+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17910634","author":{"displayName":"Mingdao Yang"},"body":"dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]","body_raw":"[~dajac] The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409] ","created":"2025-01-07T13:22:38.294+0000","updated":"2025-01-07T13:22:38.294+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17912404","author":{"displayName":"Jhen-Yung Hsu"},"body":"Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)","body_raw":"Case 1 still exists, and I found another case:\r\n\r\nGradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)\r\n        at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)\r\n ","created":"2025-01-13T08:34:20.692+0000","updated":"2025-01-13T08:34:20.692+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17912523","author":{"displayName":"Yu-Lin Chen"},"body":"[https://github.com/apache/kafka/pull/18506] Create a PR to fix below flaky errors in ClassicConsumer. # org.opentest4j.AssertionFailedError: expected: but was: # org.opentest4j.AssertionFailedError: expected: but was: The same issue also occurs with AsyncConsumer but I have no idea how to fix it. I would appreciate it if anyone come up with the solution. :(","body_raw":"[https://github.com/apache/kafka/pull/18506]\r\n\r\nCreate a PR to fix below flaky errors in ClassicConsumer. \r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\r\nThe same issue also occurs with AsyncConsumer but I have no idea how to fix it.\r\nI would appreciate it if anyone come up with the solution. :(","created":"2025-01-13T13:40:59.545+0000","updated":"2025-01-13T13:41:14.381+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912526","author":{"displayName":"David Jacot"},"body":"Yu-Lin Chen I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","body_raw":"[~Yu-Lin Chen] I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","created":"2025-01-13T13:45:11.041+0000","updated":"2025-01-13T13:45:11.041+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912533","author":{"displayName":"Yu-Lin Chen"},"body":"Hi dajac, Base on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario: * admin client remove member from consumer group * admin list group to check the member is removed. If we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing. Ref: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","body_raw":"Hi [~dajac], \r\n\r\nBase on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario:\r\n * admin client remove member from consumer group\r\n * admin list group to check the member is removed.\r\n\r\nIf we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing.\r\n\r\nRef: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","created":"2025-01-13T13:53:51.212+0000","updated":"2025-01-13T13:53:51.212+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912537","author":{"displayName":"David Jacot"},"body":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect. The member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","body_raw":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect.\r\n\r\nThe member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","created":"2025-01-13T13:59:26.498+0000","updated":"2025-01-13T13:59:26.498+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912546","author":{"displayName":"Yu-Lin Chen"},"body":"dajac Great idea, that make sense to me. I think I can rewirte the test to 1. Stop consumers before deleting 2. Validate the static members still in groups 3. Remove the static members through admin client 4. Validate the static members were removed I'll rewrite the PR. I really appreciate your suggestion. :)","body_raw":"[~dajac] Great idea, that make sense to me.\r\n\r\nI think I can rewirte the test to\r\n1. Stop consumers before deleting\r\n2. Validate the static members still in groups\r\n3. Remove the static members through admin client\r\n4. Validate the static members were removed\r\n\r\nI'll rewrite the PR. I really appreciate your suggestion. :)","created":"2025-01-13T14:13:44.842+0000","updated":"2025-01-13T14:13:44.842+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912631","author":{"displayName":"Yu-Lin Chen"},"body":"Created an improved PR based on the previous discussion * https://github.com/apache/kafka/pull/18513","body_raw":"Created an improved PR based on the previous discussion\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:02:41.697+0000","updated":"2025-01-13T18:02:41.697+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924807","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:23.175+0000","updated":"2025-02-07T08:16:23.175+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926994","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18298","body_raw":"see KAFKA-18298","created":"2025-02-13T23:34:10.741+0000","updated":"2025-02-13T23:34:10.741+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"18004142","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:47:09.934+0000","updated":"2025-07-09T12:47:09.934+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups\n\n\n\nConversation:\nuser: case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdExcep\nassistant: case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.Assert\nassistant: I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testC\nassistant: dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]\nassistant: Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but w","output":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","metadata":{"issue_id":"KAFKA-18297","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Yu-Lin Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T06:46:45.000+0000","updated":"2025-07-09T12:47:09.000+0000","resolved":"2025-02-13T23:34:10.000+0000","resolution":"Duplicate","labels":["flaky-test","integration-test","kip-848-client-support"],"components":["clients","consumer"],"comment_count":14,"comments":[{"id":"17906620","author":{"displayName":"Chia-Ping Tsai"},"body":"case_0 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0] at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345) at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155) at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065) Caused by: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","body_raw":"case_0\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]\r\n        at java.base/java.util.concurrent.CompletableFuture.wrapInExecutionException(CompletableFuture.java:345)\r\n        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:440)\r\n        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2117)\r\n        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)\r\n        at kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2065)\r\n\r\n        Caused by:\r\n        org.apache.kafka.common.errors.UnknownMemberIdException: Failed altering consumer group offsets for the following partitions: [test_topic-0]","created":"2024-12-18T06:46:57.942+0000","updated":"2024-12-18T06:46:57.942+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906621","author":{"displayName":"Chia-Ping Tsai"},"body":"case 1 Gradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36) at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31) at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","body_raw":"case 1\r\n\r\n \r\n\r\nGradle Test Run :core:test > Gradle Test Executor 6 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=consumer FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n        at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)\r\n        at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:183)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2053)","created":"2024-12-18T06:47:04.988+0000","updated":"2024-12-18T06:47:04.988+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17910601","author":{"displayName":"David Jacot"},"body":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","body_raw":"I have a PR for fix one case: [https://github.com/apache/kafka/pull/18409.] I think that this is the most common one according to https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Europe%2FZurich&tests.container=kafka.api.PlaintextAdminIntegrationTest&tests.test=testConsumerGroups(String%2C%20String)%5B2%5D.","created":"2025-01-07T12:20:56.690+0000","updated":"2025-01-07T12:20:56.690+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17910634","author":{"displayName":"Mingdao Yang"},"body":"dajac The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409]","body_raw":"[~dajac] The GitHub URL has an extra period(.) which would not take us to [https://github.com/apache/kafka/pull/18409] ","created":"2025-01-07T13:22:38.294+0000","updated":"2025-01-07T13:22:38.294+0000","updateAuthor":{"displayName":"Mingdao Yang"}},{"id":"17912404","author":{"displayName":"Jhen-Yung Hsu"},"body":"Case 1 still exists, and I found another case: Gradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED org.opentest4j.AssertionFailedError: expected: but was: at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)","body_raw":"Case 1 still exists, and I found another case:\r\n\r\nGradle Test Run :core:test > Gradle Test Executor 33 > PlaintextAdminIntegrationTest > testConsumerGroups(String, String) > testConsumerGroups(String, String).quorum=kraft.groupProtocol=classic FAILED\r\n    org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n        at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n        at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)\r\n        at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)\r\n        at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)\r\n        at app//kafka.api.PlaintextAdminIntegrationTest.testConsumerGroups(PlaintextAdminIntegrationTest.scala:2047)\r\n ","created":"2025-01-13T08:34:20.692+0000","updated":"2025-01-13T08:34:20.692+0000","updateAuthor":{"displayName":"Jhen-Yung Hsu"}},{"id":"17912523","author":{"displayName":"Yu-Lin Chen"},"body":"[https://github.com/apache/kafka/pull/18506] Create a PR to fix below flaky errors in ClassicConsumer. # org.opentest4j.AssertionFailedError: expected: but was: # org.opentest4j.AssertionFailedError: expected: but was: The same issue also occurs with AsyncConsumer but I have no idea how to fix it. I would appreciate it if anyone come up with the solution. :(","body_raw":"[https://github.com/apache/kafka/pull/18506]\r\n\r\nCreate a PR to fix below flaky errors in ClassicConsumer. \r\n # org.opentest4j.AssertionFailedError: expected: <2> but was: <3>\r\n # org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\r\nThe same issue also occurs with AsyncConsumer but I have no idea how to fix it.\r\nI would appreciate it if anyone come up with the solution. :(","created":"2025-01-13T13:40:59.545+0000","updated":"2025-01-13T13:41:14.381+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912526","author":{"displayName":"David Jacot"},"body":"Yu-Lin Chen I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","body_raw":"[~Yu-Lin Chen] I suppose that the fix is the same for the AsyncConsumer because the source of the flakiness is the same. Consumers must be stopped before deleting members and the group.","created":"2025-01-13T13:45:11.041+0000","updated":"2025-01-13T13:45:11.041+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912533","author":{"displayName":"Yu-Lin Chen"},"body":"Hi dajac, Base on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario: * admin client remove member from consumer group * admin list group to check the member is removed. If we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing. Ref: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","body_raw":"Hi [~dajac], \r\n\r\nBase on my understanding, we can't stop consumer before deleting members. This test was designed to validate this scenario:\r\n * admin client remove member from consumer group\r\n * admin list group to check the member is removed.\r\n\r\nIf we stop consumer before removing member, a leave group event will be sent and there is no way to validate the admin client trigger the member removing.\r\n\r\nRef: https://github.com/apache/kafka/blob/cbbeccad632d37c131bfa494fdb2eabc5df7610d/core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala#L2030-L2033","created":"2025-01-13T13:53:51.212+0000","updated":"2025-01-13T13:53:51.212+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912537","author":{"displayName":"David Jacot"},"body":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect. The member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","body_raw":"Yeah, the issue is that there is a race condition with this approach as there is a chance for the member to rejoin before the removal is verified. This is the main source of the flakiness. Similarly, when we remove all members before deleting the group, there is the same race condition. In my opinion, the test is incorrect.\r\n\r\nThe member removal is mainly meant to remove static members. For the context, when a static member shutdowns, it does not leave the group. Hence having a way to remove it makes sense.","created":"2025-01-13T13:59:26.498+0000","updated":"2025-01-13T13:59:26.498+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17912546","author":{"displayName":"Yu-Lin Chen"},"body":"dajac Great idea, that make sense to me. I think I can rewirte the test to 1. Stop consumers before deleting 2. Validate the static members still in groups 3. Remove the static members through admin client 4. Validate the static members were removed I'll rewrite the PR. I really appreciate your suggestion. :)","body_raw":"[~dajac] Great idea, that make sense to me.\r\n\r\nI think I can rewirte the test to\r\n1. Stop consumers before deleting\r\n2. Validate the static members still in groups\r\n3. Remove the static members through admin client\r\n4. Validate the static members were removed\r\n\r\nI'll rewrite the PR. I really appreciate your suggestion. :)","created":"2025-01-13T14:13:44.842+0000","updated":"2025-01-13T14:13:44.842+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17912631","author":{"displayName":"Yu-Lin Chen"},"body":"Created an improved PR based on the previous discussion * https://github.com/apache/kafka/pull/18513","body_raw":"Created an improved PR based on the previous discussion\r\n * https://github.com/apache/kafka/pull/18513","created":"2025-01-13T18:02:41.697+0000","updated":"2025-01-13T18:02:41.697+0000","updateAuthor":{"displayName":"Yu-Lin Chen"}},{"id":"17924807","author":{"displayName":"David Jacot"},"body":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","body_raw":"The PR is still ongoing. It is clear that the issue is in the test itself now so we can downgrade the priority. It is nice to have in 4.0 but not mandatory.","created":"2025-02-07T08:16:23.175+0000","updated":"2025-02-07T08:16:23.175+0000","updateAuthor":{"displayName":"David Jacot"}},{"id":"17926994","author":{"displayName":"Chia-Ping Tsai"},"body":"see KAFKA-18298","body_raw":"see KAFKA-18298","created":"2025-02-13T23:34:10.741+0000","updated":"2025-02-13T23:34:10.741+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"18004142","author":{"displayName":"Mickael Maison"},"body":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","body_raw":"Removed 4.1.0 from the fixed version as it's marked as Duplicate and the release script does not handle these.","created":"2025-07-09T12:47:09.934+0000","updated":"2025-07-09T12:47:09.934+0000","updateAuthor":{"displayName":"Mickael Maison"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"Remove deprecated KafkaBasedLog constructor","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"Resolved","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"Major","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : nu","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"Fixed","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated KafkaBasedLog constructor\n\n{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consum\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","output":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"{code:java} @Deprecated public KafkaBasedLog(String topic, Map producerConfigs, Map consumerConfigs, Callback > consumedCallback, Time time, Runnable initializer) { this(topic, producerConfigs, consumerConfigs, () -> null, consumedCallback, time, initializer != null ? admin -> initializer.run() : null); } {code} this constructor was deprecated in commit [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02] (since 3.0.0)","output":"Remove deprecated KafkaBasedLog constructor","metadata":{"issue_id":"KAFKA-18296","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Kuan Po Tseng"},"reporter":{"displayName":"Kuan Po Tseng"},"created":"2024-12-18T02:14:55.000+0000","updated":"2024-12-20T07:18:50.000+0000","resolved":"2024-12-20T07:18:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907286","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768] 4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","body_raw":"trunk: [https://github.com/apache/kafka/commit/e9d4aa4cfeeecb56eac9b9543d5d403b691f1768]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/cb193efc2f8203a5505e1b429e9afb729fba388f","created":"2024-12-20T07:18:50.160+0000","updated":"2024-12-20T07:18:50.160+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Remove deprecated function Partitioner#onNewBatch","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Resolved","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Blocker","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"It has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Fixed","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated function Partitioner#onNewBatch\n\nIt has been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.\n\nConversation:\nuser: trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","output":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","metadata":{"issue_id":"KAFKA-18295","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"PoAn Yang"},"created":"2024-12-18T01:50:27.000+0000","updated":"2024-12-27T10:45:19.000+0000","resolved":"2024-12-27T10:45:19.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908429","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7 4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","body_raw":"trunk:https://github.com/apache/kafka/commit/e6d242113625f20108b64bf54e796fcbcff6d8a7\r\n\r\n4.0: https://github.com/apache/kafka/commit/0a6028d3483f3e110468fd3e46028c000b11295e","created":"2024-12-27T10:45:19.686+0000","updated":"2024-12-27T10:45:19.686+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated SourceTask#commitRecord\n\nrelated to KAFKA-9780 (2.6.0, 2020)","output":"Resolved","metadata":{"issue_id":"KAFKA-18294","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:11:08.000+0000","updated":"2024-12-18T17:10:16.000+0000","resolved":"2024-12-18T17:10:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17906808","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","body_raw":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","created":"2024-12-18T17:10:16.731+0000","updated":"2024-12-18T17:10:16.731+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated SourceTask#commitRecord\n\nrelated to KAFKA-9780 (2.6.0, 2020)","output":"Blocker","metadata":{"issue_id":"KAFKA-18294","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:11:08.000+0000","updated":"2024-12-18T17:10:16.000+0000","resolved":"2024-12-18T17:10:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17906808","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","body_raw":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","created":"2024-12-18T17:10:16.731+0000","updated":"2024-12-18T17:10:16.731+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated SourceTask#commitRecord\n\nrelated to KAFKA-9780 (2.6.0, 2020)","output":"related to KAFKA-9780 (2.6.0, 2020)","metadata":{"issue_id":"KAFKA-18294","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:11:08.000+0000","updated":"2024-12-18T17:10:16.000+0000","resolved":"2024-12-18T17:10:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17906808","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","body_raw":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","created":"2024-12-18T17:10:16.731+0000","updated":"2024-12-18T17:10:16.731+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated SourceTask#commitRecord\n\nrelated to KAFKA-9780 (2.6.0, 2020)","output":"Fixed","metadata":{"issue_id":"KAFKA-18294","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:11:08.000+0000","updated":"2024-12-18T17:10:16.000+0000","resolved":"2024-12-18T17:10:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17906808","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","body_raw":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","created":"2024-12-18T17:10:16.731+0000","updated":"2024-12-18T17:10:16.731+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated SourceTask#commitRecord\n\nrelated to KAFKA-9780 (2.6.0, 2020)\n\nConversation:\nuser: trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","output":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","metadata":{"issue_id":"KAFKA-18294","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:11:08.000+0000","updated":"2024-12-18T17:10:16.000+0000","resolved":"2024-12-18T17:10:16.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17906808","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd] 4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","body_raw":"trunk: [https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/0163fa2d06559d4e7a6348d92434a5c21661e4cd","created":"2024-12-18T17:10:16.731+0000","updated":"2024-12-18T17:10:16.731+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler and org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler\n\ndeprecated by KAFKA-13725 (3.4.0, 2022)","output":"Resolved","metadata":{"issue_id":"KAFKA-18293","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:08:50.000+0000","updated":"2024-12-18T06:58:49.000+0000","resolved":"2024-12-18T06:58:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906577","author":{"displayName":"xuanzhang gong"},"body":"Hi,I will handle this issue, plz assign to me","body_raw":"Hi,I will handle this issue, plz assign to  me ","created":"2024-12-18T01:22:21.253+0000","updated":"2024-12-18T01:22:21.253+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17906622","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","body_raw":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945]\r\n\r\n4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","created":"2024-12-18T06:58:45.189+0000","updated":"2024-12-18T06:58:45.189+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler and org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler\n\ndeprecated by KAFKA-13725 (3.4.0, 2022)","output":"Blocker","metadata":{"issue_id":"KAFKA-18293","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:08:50.000+0000","updated":"2024-12-18T06:58:49.000+0000","resolved":"2024-12-18T06:58:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906577","author":{"displayName":"xuanzhang gong"},"body":"Hi,I will handle this issue, plz assign to me","body_raw":"Hi,I will handle this issue, plz assign to  me ","created":"2024-12-18T01:22:21.253+0000","updated":"2024-12-18T01:22:21.253+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17906622","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","body_raw":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945]\r\n\r\n4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","created":"2024-12-18T06:58:45.189+0000","updated":"2024-12-18T06:58:45.189+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler and org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler\n\ndeprecated by KAFKA-13725 (3.4.0, 2022)","output":"deprecated by KAFKA-13725 (3.4.0, 2022)","metadata":{"issue_id":"KAFKA-18293","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:08:50.000+0000","updated":"2024-12-18T06:58:49.000+0000","resolved":"2024-12-18T06:58:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906577","author":{"displayName":"xuanzhang gong"},"body":"Hi,I will handle this issue, plz assign to me","body_raw":"Hi,I will handle this issue, plz assign to  me ","created":"2024-12-18T01:22:21.253+0000","updated":"2024-12-18T01:22:21.253+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17906622","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","body_raw":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945]\r\n\r\n4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","created":"2024-12-18T06:58:45.189+0000","updated":"2024-12-18T06:58:45.189+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler and org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler\n\ndeprecated by KAFKA-13725 (3.4.0, 2022)","output":"Fixed","metadata":{"issue_id":"KAFKA-18293","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:08:50.000+0000","updated":"2024-12-18T06:58:49.000+0000","resolved":"2024-12-18T06:58:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906577","author":{"displayName":"xuanzhang gong"},"body":"Hi,I will handle this issue, plz assign to me","body_raw":"Hi,I will handle this issue, plz assign to  me ","created":"2024-12-18T01:22:21.253+0000","updated":"2024-12-18T01:22:21.253+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17906622","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","body_raw":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945]\r\n\r\n4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","created":"2024-12-18T06:58:45.189+0000","updated":"2024-12-18T06:58:45.189+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler and org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler\n\ndeprecated by KAFKA-13725 (3.4.0, 2022)\n\nConversation:\nuser: Hi,I will handle this issue, plz assign to me\nassistant: trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","output":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","metadata":{"issue_id":"KAFKA-18293","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"xuanzhang gong"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:08:50.000+0000","updated":"2024-12-18T06:58:49.000+0000","resolved":"2024-12-18T06:58:49.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17906577","author":{"displayName":"xuanzhang gong"},"body":"Hi,I will handle this issue, plz assign to me","body_raw":"Hi,I will handle this issue, plz assign to  me ","created":"2024-12-18T01:22:21.253+0000","updated":"2024-12-18T01:22:21.253+0000","updateAuthor":{"displayName":"xuanzhang gong"}},{"id":"17906622","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945] 4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","body_raw":"trunk: [https://github.com/apache/kafka/commit/346e5dc32267e6b00eaae6a20a604dd41ab27945]\r\n\r\n4.0: https://github.com/apache/kafka/commit/d90ad2a7bf8cc416894e60fe721de03f7b242ed9","created":"2024-12-18T06:58:45.189+0000","updated":"2024-12-18T06:58:45.189+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated methods of UpdateFeaturesOptions\n\ndeprecated by KAFKA-13823 (3.3.0, 2022)","output":"Resolved","metadata":{"issue_id":"KAFKA-18292","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Wei-Ting Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:04:22.000+0000","updated":"2024-12-20T03:30:50.000+0000","resolved":"2024-12-20T03:30:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906576","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-18T01:14:31.244+0000","updated":"2024-12-18T01:14:31.244+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17906582","author":{"displayName":"Wei-Ting Chen"},"body":"Hi~ chia7712, Could you give me the issue? I can do it.","body_raw":"Hi~ [~chia7712],\r\nCould you give me the issue? I can do it.","created":"2024-12-18T02:07:01.366+0000","updated":"2024-12-18T02:07:01.366+0000","updateAuthor":{"displayName":"Wei-Ting Chen"}},{"id":"17907267","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","body_raw":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","created":"2024-12-20T03:30:50.523+0000","updated":"2024-12-20T03:30:50.523+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated methods of UpdateFeaturesOptions\n\ndeprecated by KAFKA-13823 (3.3.0, 2022)","output":"Blocker","metadata":{"issue_id":"KAFKA-18292","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Wei-Ting Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:04:22.000+0000","updated":"2024-12-20T03:30:50.000+0000","resolved":"2024-12-20T03:30:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906576","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-18T01:14:31.244+0000","updated":"2024-12-18T01:14:31.244+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17906582","author":{"displayName":"Wei-Ting Chen"},"body":"Hi~ chia7712, Could you give me the issue? I can do it.","body_raw":"Hi~ [~chia7712],\r\nCould you give me the issue? I can do it.","created":"2024-12-18T02:07:01.366+0000","updated":"2024-12-18T02:07:01.366+0000","updateAuthor":{"displayName":"Wei-Ting Chen"}},{"id":"17907267","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","body_raw":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","created":"2024-12-20T03:30:50.523+0000","updated":"2024-12-20T03:30:50.523+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated methods of UpdateFeaturesOptions\n\ndeprecated by KAFKA-13823 (3.3.0, 2022)","output":"deprecated by KAFKA-13823 (3.3.0, 2022)","metadata":{"issue_id":"KAFKA-18292","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Wei-Ting Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:04:22.000+0000","updated":"2024-12-20T03:30:50.000+0000","resolved":"2024-12-20T03:30:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906576","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-18T01:14:31.244+0000","updated":"2024-12-18T01:14:31.244+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17906582","author":{"displayName":"Wei-Ting Chen"},"body":"Hi~ chia7712, Could you give me the issue? I can do it.","body_raw":"Hi~ [~chia7712],\r\nCould you give me the issue? I can do it.","created":"2024-12-18T02:07:01.366+0000","updated":"2024-12-18T02:07:01.366+0000","updateAuthor":{"displayName":"Wei-Ting Chen"}},{"id":"17907267","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","body_raw":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","created":"2024-12-20T03:30:50.523+0000","updated":"2024-12-20T03:30:50.523+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated methods of UpdateFeaturesOptions\n\ndeprecated by KAFKA-13823 (3.3.0, 2022)","output":"Fixed","metadata":{"issue_id":"KAFKA-18292","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Wei-Ting Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:04:22.000+0000","updated":"2024-12-20T03:30:50.000+0000","resolved":"2024-12-20T03:30:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906576","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-18T01:14:31.244+0000","updated":"2024-12-18T01:14:31.244+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17906582","author":{"displayName":"Wei-Ting Chen"},"body":"Hi~ chia7712, Could you give me the issue? I can do it.","body_raw":"Hi~ [~chia7712],\r\nCould you give me the issue? I can do it.","created":"2024-12-18T02:07:01.366+0000","updated":"2024-12-18T02:07:01.366+0000","updateAuthor":{"displayName":"Wei-Ting Chen"}},{"id":"17907267","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","body_raw":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","created":"2024-12-20T03:30:50.523+0000","updated":"2024-12-20T03:30:50.523+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated methods of UpdateFeaturesOptions\n\ndeprecated by KAFKA-13823 (3.3.0, 2022)\n\nConversation:\nuser: Hello, chia7712 , if you wont work on this, may I take the issue? Thank you\nassistant: Hi~ chia7712, Could you give me the issue? I can do it.\nassistant: trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","output":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","metadata":{"issue_id":"KAFKA-18292","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Wei-Ting Chen"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T01:04:22.000+0000","updated":"2024-12-20T03:30:50.000+0000","resolved":"2024-12-20T03:30:50.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906576","author":{"displayName":"黃竣陽"},"body":"Hello, chia7712 , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~chia7712] , if you wont work on this, may I take the issue? Thank you","created":"2024-12-18T01:14:31.244+0000","updated":"2024-12-18T01:14:31.244+0000","updateAuthor":{"displayName":"黃竣陽"}},{"id":"17906582","author":{"displayName":"Wei-Ting Chen"},"body":"Hi~ chia7712, Could you give me the issue? I can do it.","body_raw":"Hi~ [~chia7712],\r\nCould you give me the issue? I can do it.","created":"2024-12-18T02:07:01.366+0000","updated":"2024-12-18T02:07:01.366+0000","updateAuthor":{"displayName":"Wei-Ting Chen"}},{"id":"17907267","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9 4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","body_raw":"trunk: https://github.com/apache/kafka/commit/bd27e34f2d61735e4be144e075bca25ca04eacc9\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/df82240096a4f3cfa60e679ef21ada06021cdb95","created":"2024-12-20T03:30:50.523+0000","updated":"2024-12-20T03:30:50.523+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated methods of ListConsumerGroupOffsetsOptions\n\nthey are deprecated by KAFKA-13043 (3.3.0, 2022)","output":"Resolved","metadata":{"issue_id":"KAFKA-18291","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:59:45.000+0000","updated":"2024-12-23T17:41:46.000+0000","resolved":"2024-12-23T17:41:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907923","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","body_raw":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08\r\n\r\n4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","created":"2024-12-23T17:41:46.946+0000","updated":"2024-12-23T17:41:46.946+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated methods of ListConsumerGroupOffsetsOptions\n\nthey are deprecated by KAFKA-13043 (3.3.0, 2022)","output":"Blocker","metadata":{"issue_id":"KAFKA-18291","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:59:45.000+0000","updated":"2024-12-23T17:41:46.000+0000","resolved":"2024-12-23T17:41:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907923","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","body_raw":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08\r\n\r\n4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","created":"2024-12-23T17:41:46.946+0000","updated":"2024-12-23T17:41:46.946+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated methods of ListConsumerGroupOffsetsOptions\n\nthey are deprecated by KAFKA-13043 (3.3.0, 2022)","output":"they are deprecated by KAFKA-13043 (3.3.0, 2022)","metadata":{"issue_id":"KAFKA-18291","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:59:45.000+0000","updated":"2024-12-23T17:41:46.000+0000","resolved":"2024-12-23T17:41:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907923","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","body_raw":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08\r\n\r\n4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","created":"2024-12-23T17:41:46.946+0000","updated":"2024-12-23T17:41:46.946+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated methods of ListConsumerGroupOffsetsOptions\n\nthey are deprecated by KAFKA-13043 (3.3.0, 2022)","output":"Fixed","metadata":{"issue_id":"KAFKA-18291","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:59:45.000+0000","updated":"2024-12-23T17:41:46.000+0000","resolved":"2024-12-23T17:41:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907923","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","body_raw":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08\r\n\r\n4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","created":"2024-12-23T17:41:46.946+0000","updated":"2024-12-23T17:41:46.946+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated methods of ListConsumerGroupOffsetsOptions\n\nthey are deprecated by KAFKA-13043 (3.3.0, 2022)\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","output":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","metadata":{"issue_id":"KAFKA-18291","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:59:45.000+0000","updated":"2024-12-23T17:41:46.000+0000","resolved":"2024-12-23T17:41:46.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907923","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08 4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","body_raw":"trunk: https://github.com/apache/kafka/commit/1cf514313e87cee5b8611c3765ba55692ce68e08\r\n\r\n4.0: https://github.com/apache/kafka/commit/916caccbf688970c533ad787367767e6fe6909e7","created":"2024-12-23T17:41:46.946+0000","updated":"2024-12-23T17:41:46.946+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated methods of FeatureUpdate\n\nthey are deprecated by KAFKA-13823 (3.3.0, 2022)","output":"Resolved","metadata":{"issue_id":"KAFKA-18290","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:57:52.000+0000","updated":"2024-12-24T07:01:31.000+0000","resolved":"2024-12-24T07:01:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908023","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","body_raw":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370\r\n\r\n4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","created":"2024-12-24T07:01:31.289+0000","updated":"2024-12-24T07:01:31.289+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated methods of FeatureUpdate\n\nthey are deprecated by KAFKA-13823 (3.3.0, 2022)","output":"Blocker","metadata":{"issue_id":"KAFKA-18290","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:57:52.000+0000","updated":"2024-12-24T07:01:31.000+0000","resolved":"2024-12-24T07:01:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908023","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","body_raw":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370\r\n\r\n4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","created":"2024-12-24T07:01:31.289+0000","updated":"2024-12-24T07:01:31.289+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated methods of FeatureUpdate\n\nthey are deprecated by KAFKA-13823 (3.3.0, 2022)","output":"they are deprecated by KAFKA-13823 (3.3.0, 2022)","metadata":{"issue_id":"KAFKA-18290","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:57:52.000+0000","updated":"2024-12-24T07:01:31.000+0000","resolved":"2024-12-24T07:01:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908023","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","body_raw":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370\r\n\r\n4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","created":"2024-12-24T07:01:31.289+0000","updated":"2024-12-24T07:01:31.289+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated methods of FeatureUpdate\n\nthey are deprecated by KAFKA-13823 (3.3.0, 2022)","output":"Fixed","metadata":{"issue_id":"KAFKA-18290","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:57:52.000+0000","updated":"2024-12-24T07:01:31.000+0000","resolved":"2024-12-24T07:01:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908023","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","body_raw":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370\r\n\r\n4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","created":"2024-12-24T07:01:31.289+0000","updated":"2024-12-24T07:01:31.289+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated methods of FeatureUpdate\n\nthey are deprecated by KAFKA-13823 (3.3.0, 2022)\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","output":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","metadata":{"issue_id":"KAFKA-18290","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:57:52.000+0000","updated":"2024-12-24T07:01:31.000+0000","resolved":"2024-12-24T07:01:31.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908023","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370 4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","body_raw":"trunk: https://github.com/apache/kafka/commit/4d6535e60ddf6c1f04e9eeb6b83fcbe251332370\r\n\r\n4.0: https://github.com/apache/kafka/commit/750af8f06555384480cb31739f2c2679cf494796","created":"2024-12-24T07:01:31.289+0000","updated":"2024-12-24T07:01:31.289+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","output":"Remove deprecated methods of DescribeTopicsResult","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","output":"Resolved","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","output":"Blocker","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","output":"they are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0","output":"Fixed","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove deprecated methods of DescribeTopicsResult\n\nthey are deprecated by KAFKA-10774 (3.1.0), so we can remove them from 4.0.0\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","output":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","metadata":{"issue_id":"KAFKA-18289","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Blocker","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-18T00:52:57.000+0000","updated":"2025-10-13T20:59:52.000+0000","resolved":"2024-12-23T17:52:02.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17907925","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73 4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","body_raw":"trunk: https://github.com/apache/kafka/commit/1a3dce72fa4c8b90ab203d42861602a6b78a0b73\r\n\r\n4.0: https://github.com/apache/kafka/commit/20cf548e88fbb4831faa32a368970c4c8190f771","created":"2024-12-23T17:52:02.749+0000","updated":"2024-12-23T17:52:02.749+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Add support kafka-streams-groups.sh --describe","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Resolved","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Major","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Implement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Fixed","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add support kafka-streams-groups.sh --describe\n\nImplement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trun\n\nConversation:\nuser: AK PR: https://github.com/apache/kafka/pull/19433","output":"AK PR: https://github.com/apache/kafka/pull/19433","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Implement --describe and its options: (--state, --offset, --members and the combination of them with --verbose) This is implemented in the kip1071 feature branch already and needs to be ported to trunk.","output":"Add support kafka-streams-groups.sh --describe","metadata":{"issue_id":"KAFKA-18288","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:32.000+0000","updated":"2025-07-08T13:29:00.000+0000","resolved":"2025-04-30T06:30:49.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17943230","author":{"displayName":"Alieh Saeedi"},"body":"AK PR: https://github.com/apache/kafka/pull/19433","body_raw":"AK PR: https://github.com/apache/kafka/pull/19433","created":"2025-04-10T14:07:48.223+0000","updated":"2025-04-10T14:07:48.223+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Add support for kafka-streams-groups.sh --list","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Resolved","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Major","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Implement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Fixed","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add support for kafka-streams-groups.sh --list\n\nImplement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.\n\nConversation:\nuser: BLOCKED for now!\nassistant: https://github.com/apache/kafka/pull/19422","output":"https://github.com/apache/kafka/pull/19422","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Implement kafka-streams-group.sh --list. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Add support for kafka-streams-groups.sh --list","metadata":{"issue_id":"KAFKA-18287","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Alieh Saeedi"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:12:07.000+0000","updated":"2025-04-10T08:55:31.000+0000","resolved":"2025-04-10T08:55:24.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17913706","author":{"displayName":"Alieh Saeedi"},"body":"BLOCKED for now!","body_raw":"BLOCKED for now!","created":"2025-01-16T12:33:02.753+0000","updated":"2025-01-16T12:33:02.753+0000","updateAuthor":{"displayName":"Alieh Saeedi"}},{"id":"17942412","author":{"displayName":"Alieh Saeedi"},"body":"https://github.com/apache/kafka/pull/19422","body_raw":"https://github.com/apache/kafka/pull/19422","created":"2025-04-10T08:55:24.185+0000","updated":"2025-04-10T08:55:24.185+0000","updateAuthor":{"displayName":"Alieh Saeedi"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Add support for streams groups to kafka-group.sh","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Resolved","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Major","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Add support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Fixed","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add support for streams groups to kafka-group.sh\n\nAdd support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.\n\nConversation:\nuser: We'll need KAFAK-18613, to be able to write a useful integration test","output":"We'll need KAFAK-18613, to be able to write a useful integration test","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Add support for streams groups in kafka-group.sh. This is already present on the kip1071 feature branch and needs to be added to trunk.","output":"Add support for streams groups to kafka-group.sh","metadata":{"issue_id":"KAFKA-18286","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T18:11:33.000+0000","updated":"2025-04-16T14:55:53.000+0000","resolved":"2025-04-16T14:55:53.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17934539","author":{"displayName":"Lucas Brutschy"},"body":"We'll need KAFAK-18613, to be able to write a useful integration test","body_raw":"We'll need KAFAK-18613, to be able to write a useful integration test","created":"2025-03-12T13:55:09.161+0000","updated":"2025-03-12T13:55:09.161+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Add Admin.describeStreamsGroups","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Resolved","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Major","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Adds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Fixed","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add Admin.describeStreamsGroups\n\nAdds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported\n\nConversation:\nuser: This is currently blocked, since this includes an integration test, so we need to implement the broker side first","output":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Adds {{describeStreamsGroup}} to Admin API. This exposes the result of the {{DESCRIBE_STREAMS_GROUP}} RPC in the Admin API. This is already present in the kip1071 feature branch and needs to be ported to trunk.","output":"Add Admin.describeStreamsGroups","metadata":{"issue_id":"KAFKA-18285","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T14:01:42.000+0000","updated":"2025-03-07T14:57:02.000+0000","resolved":"2025-03-07T14:57:02.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":1,"comments":[{"id":"17915782","author":{"displayName":"Lucas Brutschy"},"body":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","body_raw":"This is currently blocked, since this includes an integration test, so we need to implement the broker side first","created":"2025-01-21T17:38:57.090+0000","updated":"2025-01-21T17:38:57.090+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add StreamsGroupDescribe RPC definitions\n\n","output":"Resolved","metadata":{"issue_id":"KAFKA-18283","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T12:51:54.000+0000","updated":"2025-01-03T10:10:12.000+0000","resolved":"2024-12-19T09:45:47.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17906417","author":{"displayName":"Chu Cheng Li"},"body":"Hi lucasbru, could I give it a try? Thanks!","body_raw":"Hi [~lucasbru], could I give it a try? Thanks!","created":"2024-12-17T13:34:39.781+0000","updated":"2024-12-17T13:34:58.539+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17906418","author":{"displayName":"Lucas Brutschy"},"body":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","body_raw":"Hi [~peterxcli]! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","created":"2024-12-17T13:37:07.952+0000","updated":"2024-12-17T13:37:07.952+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add StreamsGroupDescribe RPC definitions\n\n","output":"Major","metadata":{"issue_id":"KAFKA-18283","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T12:51:54.000+0000","updated":"2025-01-03T10:10:12.000+0000","resolved":"2024-12-19T09:45:47.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17906417","author":{"displayName":"Chu Cheng Li"},"body":"Hi lucasbru, could I give it a try? Thanks!","body_raw":"Hi [~lucasbru], could I give it a try? Thanks!","created":"2024-12-17T13:34:39.781+0000","updated":"2024-12-17T13:34:58.539+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17906418","author":{"displayName":"Lucas Brutschy"},"body":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","body_raw":"Hi [~peterxcli]! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","created":"2024-12-17T13:37:07.952+0000","updated":"2024-12-17T13:37:07.952+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add StreamsGroupDescribe RPC definitions\n\n","output":"","metadata":{"issue_id":"KAFKA-18283","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T12:51:54.000+0000","updated":"2025-01-03T10:10:12.000+0000","resolved":"2024-12-19T09:45:47.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17906417","author":{"displayName":"Chu Cheng Li"},"body":"Hi lucasbru, could I give it a try? Thanks!","body_raw":"Hi [~lucasbru], could I give it a try? Thanks!","created":"2024-12-17T13:34:39.781+0000","updated":"2024-12-17T13:34:58.539+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17906418","author":{"displayName":"Lucas Brutschy"},"body":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","body_raw":"Hi [~peterxcli]! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","created":"2024-12-17T13:37:07.952+0000","updated":"2024-12-17T13:37:07.952+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add StreamsGroupDescribe RPC definitions\n\n","output":"Fixed","metadata":{"issue_id":"KAFKA-18283","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T12:51:54.000+0000","updated":"2025-01-03T10:10:12.000+0000","resolved":"2024-12-19T09:45:47.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17906417","author":{"displayName":"Chu Cheng Li"},"body":"Hi lucasbru, could I give it a try? Thanks!","body_raw":"Hi [~lucasbru], could I give it a try? Thanks!","created":"2024-12-17T13:34:39.781+0000","updated":"2024-12-17T13:34:58.539+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17906418","author":{"displayName":"Lucas Brutschy"},"body":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","body_raw":"Hi [~peterxcli]! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","created":"2024-12-17T13:37:07.952+0000","updated":"2024-12-17T13:37:07.952+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add StreamsGroupDescribe RPC definitions\n\n\n\nConversation:\nuser: Hi lucasbru, could I give it a try? Thanks!\nassistant: Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","output":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","metadata":{"issue_id":"KAFKA-18283","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Lucas Brutschy"},"reporter":{"displayName":"Lucas Brutschy"},"created":"2024-12-17T12:51:54.000+0000","updated":"2025-01-03T10:10:12.000+0000","resolved":"2024-12-19T09:45:47.000+0000","resolution":"Fixed","labels":[],"components":["streams"],"comment_count":2,"comments":[{"id":"17906417","author":{"displayName":"Chu Cheng Li"},"body":"Hi lucasbru, could I give it a try? Thanks!","body_raw":"Hi [~lucasbru], could I give it a try? Thanks!","created":"2024-12-17T13:34:39.781+0000","updated":"2024-12-17T13:34:58.539+0000","updateAuthor":{"displayName":"Chu Cheng Li"}},{"id":"17906418","author":{"displayName":"Lucas Brutschy"},"body":"Hi peterxcli! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","body_raw":"Hi [~peterxcli]! Thanks wanting to help with this. However, we already have the code on a feature branch, and just created this ticket to track merging the code to trunk. So there is nothing really to do at the moment","created":"2024-12-17T13:37:07.952+0000","updated":"2024-12-17T13:37:07.952+0000","updateAuthor":{"displayName":"Lucas Brutschy"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which are configured on a per-controller basis via `advertised.listeners` (previously, controller listeners were prohibited in `advertised.listeners`). As part of https://github.com/confluentinc/ce-kafka/blob/78066ca74e7964bc7f9dfb0f7660814ee91ef149/core/src/main/scala/kafka/server/KafkaConfig.scala#L3445, Kafka added two things: * When a controller listener is not in `advertised.listeners`, the (non-advertised) controller listener from `listeners` is added to `effectiveAdvertisedBrokerListeners` * We validate that all effective advertised broker listeners are routable (i.e. not `0.0.0.0`) This is incorrect logic, because the listeners property indicates what IPs a listener binds on, not how clients should connect to the listener (which may be a static configuration that is transparent to the controller) In 3.8, both of these config combinations are valid: _Implicit \"bind on all IP addresses\"_ {code:java} controller.quorum.voters=9991@localhost:9094 listeners=BROKER://:9093,CONTROLLER://:9094 advertised.listeners=BROKER://hostname:9093{code} _Explicit \"bind on all addresses\" (specifically, IPv4)_ {code:java} controller.quorum.voters=9991@localhost:9094 listeners=BROKER://0.0.0.0:9093,CONTROLLER://0.0.0.0:9094 advertised.listeners=BROKER:// :9093{code} Formatting a controller with this configuration fails with an error like this: {code:java} broker | Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: advertised.listeners cannot use the nonroutable meta-address 0.0.0.0. Use a routable IP address. at scala.Predef$.require(Predef.scala:337) at kafka.server.KafkaConfig.validateValues(KafkaConfig.scala:1022) at kafka.server.KafkaConfig. (KafkaConfig.scala:852) at kafka.server.KafkaConfig. (KafkaConfig.scala:184) at kafka.tools.StorageTool$.$anonfun$execute$1(StorageTool.scala:79) at scala.Option.flatMap(Option.scala:283) at kafka.tools.StorageTool$.execute(StorageTool.scala:79) at kafka.tools.StorageTool$.main(StorageTool.scala:46) at kafka.docker.KafkaDockerWrapper$.main(KafkaDockerWrapper.scala:48) at kafka.docker.KafkaDockerWrapper.main(KafkaDockerWrapper.scala) at java.base@21.0.2/java.lang.invoke.LambdaForm$DMH/sa346b79c.invokeStaticInit(LambdaForm$DMH) {code} In 3.9.0, the implicit configuration works, but the explicit configuration does not. This is a breaking change because a configuration that previously worked no longer works.","output":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which ar","output":"Resolved","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which ar","output":"Blocker","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which ar","output":"Kafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introd","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which ar","output":"Fixed","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses\n\nKafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* htt\n\nConversation:\nuser: Hi justinrlee, if you're not working on this, may I take it? Thank you.\nassistant: Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!\nassistant: justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","output":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Kafka 3.9.0 is currently validating whether all listeners (including non-advertised controller listeners) are routable. Non-advertised controller listeners don't need to be routable. *Background:* https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes introduced dynamic controller quorums. When a cluster is configured with a dynamic controller quorum, KRaft clients request (via a metadata request) the currently-valid set of controller endpoints, which are configured on a per-controller basis via `advertised.listeners` (previously, controller listeners were prohibited in `advertised.listeners`). As part of https://github.com/confluentinc/ce-kafka/blob/78066ca74e7964bc7f9dfb0f7660814ee91ef149/core/src/main/scala/kafka/server/KafkaConfig.scala#L3445, ","output":"Kafka (3.9.0) is improperly validating non-advertised listeners for routable controller addresses","metadata":{"issue_id":"KAFKA-18281","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Blocker","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Justin Lee"},"created":"2024-12-17T06:19:26.000+0000","updated":"2025-03-25T08:03:46.000+0000","resolved":"2025-02-25T03:24:10.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17909610","author":{"displayName":"PoAn Yang"},"body":"Hi justinrlee, if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~justinrlee], if you're not working on this, may I take it? Thank you.","created":"2025-01-03T15:24:26.869+0000","updated":"2025-01-03T15:24:26.869+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17909818","author":{"displayName":"Justin Lee"},"body":"Hey yangpoan, I'm not actively working on it. Go ahead and take this. Thanks!","body_raw":"Hey [~yangpoan], I'm not actively working on it. Go ahead and take this. Thanks!","created":"2025-01-05T05:25:43.176+0000","updated":"2025-01-05T05:25:43.176+0000","updateAuthor":{"displayName":"Justin Lee"}},{"id":"17909819","author":{"displayName":"PoAn Yang"},"body":"justinrlee Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","body_raw":"[~justinrlee] Thanks. I already created a PR for it. If you have time, could you help to review it? Thank you.","created":"2025-01-05T05:27:43.892+0000","updated":"2025-01-05T05:27:43.892+0000","updateAuthor":{"displayName":"PoAn Yang"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one.new_client_sasl_mechanism=PLAIN.metadata_quorum=ISOLATED_KRAFT status: FAIL run time: 5 minutes 55.502 seconds TimeoutError('Expected producer to still be producing.') Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run data = self.run_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test return self.test_context.function(self.test) File \"/usr/local/lib/python3.7/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) File \"/opt/kafka-dev/tests/kafkatest/tests/core/security_rolling_upgrade_test.py\", line 140, in test_rolling_upgrade_sasl_mechanism_phase_one self.run_produce_consume_validate(self.add_sasl_mechanism, new_client_sasl_mechanism) File \"/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py\", line 107, in run_produce_consume_validate self.stop_producer_and_consumer() File \"/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py\", line 95, in stop_producer_and_consumer self.check_producing() File \"/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py\", line 91, in check_producing err_msg=\"Expected producer to still be producing.\") File \"/usr/local/lib/python3.7/dist-packages/ducktape/utils/util.py\", line 58, in wait_until raise TimeoutError(err_msg() if callable(err_msg) else err_msg) from last_exception ducktape.errors.TimeoutError: Expected producer to still be producing. ----------------------------------------------------------------------------------------------------","output":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_o","output":"Resolved","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_o","output":"Blocker","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_o","output":"==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ===============================","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_o","output":"Fixed","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one\n\n==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes\n\nConversation:\nuser: trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","output":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"==================================================================================================== SESSION REPORT (ALL TESTS) ducktape version: 0.12.0 session_id: 2024-12-16--001 run time: 5 minutes 55.742 seconds tests run: 1 passed: 0 flaky: 0 failed: 1 ignored: 0 ==================================================================================================== test_id: kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one.new_client_sasl_mechanism=PLAIN.metadata_quorum=ISOLATED_KRAFT status: FAIL run time: 5 minutes 55.502 seconds TimeoutError('Expected producer to still be producing.') Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 351, in _d","output":"fix e2e TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one","metadata":{"issue_id":"KAFKA-18280","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:41:45.000+0000","updated":"2025-04-08T08:26:04.000+0000","resolved":"2024-12-30T14:37:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":1,"comments":[{"id":"17908875","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0 4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","body_raw":"trunk: https://github.com/apache/kafka/commit/6737178c12fbd90e9d8fc7cb7a17b7d2c4eb0ad0\r\n\r\n4.0: https://github.com/apache/kafka/commit/a0be42e7390190e15aa446b29b697e9374f63f6b","created":"2024-12-30T14:37:47.862+0000","updated":"2024-12-30T14:37:47.862+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"add JDK 11 check and test for clients and streams module\n\nsee discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","output":"add JDK 11 check and test for clients and streams module","metadata":{"issue_id":"KAFKA-18279","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:24:19.000+0000","updated":"2025-07-19T03:48:27.000+0000","resolved":null,"labels":[],"components":["admin","clients","consumer","producer ","streams"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"add JDK 11 check and test for clients and streams module\n\nsee discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","output":"Open","metadata":{"issue_id":"KAFKA-18279","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:24:19.000+0000","updated":"2025-07-19T03:48:27.000+0000","resolved":null,"labels":[],"components":["admin","clients","consumer","producer ","streams"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"add JDK 11 check and test for clients and streams module\n\nsee discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","output":"Major","metadata":{"issue_id":"KAFKA-18279","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:24:19.000+0000","updated":"2025-07-19T03:48:27.000+0000","resolved":null,"labels":[],"components":["admin","clients","consumer","producer ","streams"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: add JDK 11 check and test for clients and streams module\n\nsee discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","output":"see discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","metadata":{"issue_id":"KAFKA-18279","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:24:19.000+0000","updated":"2025-07-19T03:48:27.000+0000","resolved":null,"labels":[],"components":["admin","clients","consumer","producer ","streams"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"see discussion: https://github.com/apache/kafka/pull/17861#issuecomment-2547515965 This jira will add a post-commit job to trunk branch - it should run check and test for clients and streams module under JDK 11","output":"add JDK 11 check and test for clients and streams module","metadata":{"issue_id":"KAFKA-18279","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:24:19.000+0000","updated":"2025-07-19T03:48:27.000+0000","resolved":null,"labels":[],"components":["admin","clients","consumer","producer ","streams"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"the name and description of `run-gradle` is incorrect\n\nsee https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","output":"the name and description of `run-gradle` is incorrect","metadata":{"issue_id":"KAFKA-18278","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:20:55.000+0000","updated":"2024-12-27T10:32:23.000+0000","resolved":"2024-12-27T10:32:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"the name and description of `run-gradle` is incorrect\n\nsee https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","output":"Resolved","metadata":{"issue_id":"KAFKA-18278","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:20:55.000+0000","updated":"2024-12-27T10:32:23.000+0000","resolved":"2024-12-27T10:32:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"the name and description of `run-gradle` is incorrect\n\nsee https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","output":"Trivial","metadata":{"issue_id":"KAFKA-18278","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:20:55.000+0000","updated":"2024-12-27T10:32:23.000+0000","resolved":"2024-12-27T10:32:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: the name and description of `run-gradle` is incorrect\n\nsee https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","output":"see https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","metadata":{"issue_id":"KAFKA-18278","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:20:55.000+0000","updated":"2024-12-27T10:32:23.000+0000","resolved":"2024-12-27T10:32:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"the name and description of `run-gradle` is incorrect\n\nsee https://github.com/apache/kafka/blob/trunk/.github/actions/run-gradle/action.yml#L19","output":"Fixed","metadata":{"issue_id":"KAFKA-18278","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Trivial","assignee":{"displayName":"Logan Zhu"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T05:20:55.000+0000","updated":"2024-12-27T10:32:23.000+0000","resolved":"2024-12-27T10:32:23.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-12-17 04:27:01,370] INFO Starting REST server (org.apache.kafka.trogdor.rest.JsonRestServer) [2024-12-17 04:27:01,429] INFO Registered resource org.apache.kafka.trogdor.agent.AgentRestResource@5167f57d (org.apache.kafka.trogdor.rest.JsonRestServer) [2024-12-17 04:27:01,489] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.2+8-86 (org.eclipse.jetty.server.Server) *stdout, stderr:* Dec 17, 2024 4:27:01 AM org.glassfish.jersey.server.wadl.WadlFeature configure WARNING: JAXBContext implementation could not be found. WADL feature is disabled. Dec 17, 2024 4:27:01 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime WARNING: A provider org.apache.kafka.trogdor.agent.AgentRestResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.trogdor.agent.AgentRestResource will be ignored. *callstack* Traceback (most recent call last): File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 350, in _do_run self.setup_test() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/runner_client.py\", line 402, in setup_test self.test.setup() File \"/usr/local/lib/python3.7/dist-packages/ducktape/tests/test.py\", line 74, in setup self.setUp() File \"/opt/kafka-dev/tests/kafkatest/tests/core/network_degrade_test.py\", line 42, in setUp self.trogdor.start() File \"/usr/local/lib/python3.7/dist-packages/ducktape/services/service.py\", line 265, in start self.start_node(node, **kwargs) File \"/opt/kafka-dev/tests/kafkatest/services/trogdor/trogdor.py\", line 141, in start_node self._start_agent_node(node) File \"/opt/kafka-dev/tests/kafkatest/services/trogdor/trogdor.py\", line 158, in _start_agent_node TrogdorService.AGENT_LOG, node) File \"/opt/kafka-dev/tests/kafkatest/services/trogdor/trogdor.py\", line 175, in _start_trogdor_daemon err_msg=(\"%s on %s didn't finish startup\" % (daemon_name, node.name))) File \"/usr/local/lib/python3.7/dist-packages/ducktape/cluster/remoteaccount.py\", line 754, in wait_until allow_fail=True) == 0, **kwargs) File \"/usr/local/lib/python3.7/dist-packages/ducktape/utils/util.py\", line 58, in wait_until raise TimeoutError(err_msg() if callable(err_msg) else err_msg) from last_exception ducktape.errors.TimeoutError: agent on ducker02 didn't finish startup","output":"Convert network_degrade_test to Kraft mode","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-1","output":"Resolved","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-1","output":"Blocker","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-1","output":"[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] I","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-1","output":"Fixed","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Convert network_degrade_test to Kraft mode\n\n[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting\n\nConversation:\nuser: It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.\nassistant: chia7712 Thanks for comfirm, I will try migrating first.\nassistant: trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","output":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"[https://github.com/apache/kafka/pull/18000/files] make output as console instead of log so follow error will be reported. Even if apply KAFKA-18218, {{NetworkDegradeTest}} still fail since {{Starting agent process}} appear in {{log}} instead of {{{}stdout{}}}. *log show* [2024-12-17 04:27:01,329] INFO Logging initialized @664ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [2024-12-17 04:27:01,365] INFO Starting agent process. (org.apache.kafka.trogdor.agent.Agent) [2024-12-17 04:27:01,370] INFO Starting REST server (org.apache.kafka.trogdor.rest.JsonRestServer) [2024-12-17 04:27:01,429] INFO Registered resource org.apache.kafka.trogdor.agent.AgentRestResource@5167f57d (org.apache.kafka.trogdor.rest.JsonRestServer) [2024-12-17 04:27:01,489] INFO jetty-9.4.56.v2024082","output":"Convert network_degrade_test to Kraft mode","metadata":{"issue_id":"KAFKA-18277","project":"KAFKA","project_name":"Kafka","issue_type":"Test","status":"Resolved","priority":"Blocker","assignee":{"displayName":"TaiJuWu"},"reporter":{"displayName":"TaiJuWu"},"created":"2024-12-17T05:11:35.000+0000","updated":"2025-01-09T17:52:19.000+0000","resolved":"2025-01-09T17:52:18.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":3,"comments":[{"id":"17906584","author":{"displayName":"Chia-Ping Tsai"},"body":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working. We can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","body_raw":"It is using zk to run test, so the \"output\" should be based on 3.9 version rather than trunk version. That is why the fix of KAFKA-18218 is not working.\r\n\r\nWe can simplify revert https://github.com/apache/kafka/pull/18000 - or we can migrate the test to use kraft - I prefer the later.","created":"2024-12-18T02:24:03.429+0000","updated":"2024-12-18T02:24:03.429+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}},{"id":"17906586","author":{"displayName":"TaiJuWu"},"body":"chia7712 Thanks for comfirm, I will try migrating first.","body_raw":"[~chia7712] Thanks for comfirm, I will try migrating first.","created":"2024-12-18T02:28:09.946+0000","updated":"2024-12-18T02:28:09.946+0000","updateAuthor":{"displayName":"TaiJuWu"}},{"id":"17911603","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e 4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","body_raw":"trunk: https://github.com/apache/kafka/commit/5acbd42dd755081b0ce5d526e5155d8d56a11a3e\r\n\r\n4.0: https://github.com/apache/kafka/commit/403ee54cc2c85392e70e32cb3a829f7dcfc22da9","created":"2025-01-09T17:52:19.003+0000","updated":"2025-01-09T17:52:19.003+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Restarting broker in testing should use the same port\n\nWe have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"Restarting broker in testing should use the same port","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Restarting broker in testing should use the same port\n\nWe have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"Resolved","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Restarting broker in testing should use the same port\n\nWe have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"Major","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Restarting broker in testing should use the same port\n\nWe have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"We have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Restarting broker in testing should use the same port\n\nWe have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"Fixed","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We have already implemented this for the controller (though it has some issues—KAFKA-18274). Additionally, the broker also needs this feature to enable us to run related tests.","output":"Restarting broker in testing should use the same port","metadata":{"issue_id":"KAFKA-18275","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:28:49.000+0000","updated":"2025-02-06T19:02:41.000+0000","resolved":"2025-02-06T19:02:41.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Failed to restart controller in testing due to closed socket channel\n\nIn fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPort()); } ``` [0] https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ControllerServer.scala#L161 [1] https://github.com/apache/kafka/blob/trunk/test-common/src/main/java/org/apache/kafka/common/test/PreboundSocketFactoryManager.java#L52","output":"Failed to restart controller in testing due to closed socket channel","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Failed to restart controller in testing due to closed socket channel\n\nIn fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPor","output":"Resolved","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Failed to restart controller in testing due to closed socket channel\n\nIn fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPor","output":"Major","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Failed to restart controller in testing due to closed socket channel\n\nIn fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPor","output":"In fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen())","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Failed to restart controller in testing due to closed socket channel\n\nIn fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPor","output":"Fixed","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"In fact, there are three issues of restarting controller in testing. 1. we don't rebuild the metrics of `SharedServer` [0] - it causes NPE 2. we return the closed server socket [1] 3. we should return the new server socket bound with same port ``` if (socketChannel != null && socketChannel.isOpen()) { return socketChannel; } // bind the server socket with same port if (socketChannel != null) { socketAddress = new InetSocketAddress(socketAddress.getHostString(), socketChannel.socket().getLocalPort()); } ``` [0] https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ControllerServer.scala#L161 [1] https://github.com/apache/kafka/blob/trunk/test-common/src/main/java/org/apache/kafka/common/test/PreboundSocketFactoryManager.java#L52","output":"Failed to restart controller in testing due to closed socket channel","metadata":{"issue_id":"KAFKA-18274","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-17T04:25:20.000+0000","updated":"2025-01-13T19:12:47.000+0000","resolved":"2025-01-13T19:12:47.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Implement kafka-share-groups.sh --describe --verbose\n\nThis is the share groups part of KIP-1099.","output":"Resolved","metadata":{"issue_id":"KAFKA-18273","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-16T22:24:35.000+0000","updated":"2025-01-02T09:15:30.000+0000","resolved":"2025-01-02T09:15:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Implement kafka-share-groups.sh --describe --verbose\n\nThis is the share groups part of KIP-1099.","output":"Major","metadata":{"issue_id":"KAFKA-18273","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-16T22:24:35.000+0000","updated":"2025-01-02T09:15:30.000+0000","resolved":"2025-01-02T09:15:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Implement kafka-share-groups.sh --describe --verbose\n\nThis is the share groups part of KIP-1099.","output":"This is the share groups part of KIP-1099.","metadata":{"issue_id":"KAFKA-18273","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-16T22:24:35.000+0000","updated":"2025-01-02T09:15:30.000+0000","resolved":"2025-01-02T09:15:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Implement kafka-share-groups.sh --describe --verbose\n\nThis is the share groups part of KIP-1099.","output":"Fixed","metadata":{"issue_id":"KAFKA-18273","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Andrew Schofield"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-16T22:24:35.000+0000","updated":"2025-01-02T09:15:30.000+0000","resolved":"2025-01-02T09:15:30.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Deprecated protocol api usage should be logged at info level\n\nThis would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"Deprecated protocol api usage should be logged at info level","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Deprecated protocol api usage should be logged at info level\n\nThis would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"Resolved","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Deprecated protocol api usage should be logged at info level\n\nThis would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"Major","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Deprecated protocol api usage should be logged at info level\n\nThis would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"This would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Deprecated protocol api usage should be logged at info level\n\nThis would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"Fixed","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"This would still be disabled by default while making it possible to enable them without enabling request logging for non deprecated apis.","output":"Deprecated protocol api usage should be logged at info level","metadata":{"issue_id":"KAFKA-18272","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Ismael Juma"},"reporter":{"displayName":"Ismael Juma"},"created":"2024-12-16T21:18:00.000+0000","updated":"2024-12-28T17:21:37.000+0000","resolved":"2024-12-28T17:21:37.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Trigger CI when a PR is marked \"ready for review\"\n\n","output":"Open","metadata":{"issue_id":"KAFKA-18271","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-16T19:39:46.000+0000","updated":"2024-12-16T22:51:38.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":1,"comments":[{"id":"17906215","author":{"displayName":"黃竣陽"},"body":"Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~davidarthur]  , if you wont work on this, may I take the issue? Thank you","created":"2024-12-16T22:51:38.961+0000","updated":"2024-12-16T22:51:38.961+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Trigger CI when a PR is marked \"ready for review\"\n\n","output":"Minor","metadata":{"issue_id":"KAFKA-18271","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-16T19:39:46.000+0000","updated":"2024-12-16T22:51:38.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":1,"comments":[{"id":"17906215","author":{"displayName":"黃竣陽"},"body":"Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~davidarthur]  , if you wont work on this, may I take the issue? Thank you","created":"2024-12-16T22:51:38.961+0000","updated":"2024-12-16T22:51:38.961+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Trigger CI when a PR is marked \"ready for review\"\n\n","output":"","metadata":{"issue_id":"KAFKA-18271","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-16T19:39:46.000+0000","updated":"2024-12-16T22:51:38.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":1,"comments":[{"id":"17906215","author":{"displayName":"黃竣陽"},"body":"Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~davidarthur]  , if you wont work on this, may I take the issue? Thank you","created":"2024-12-16T22:51:38.961+0000","updated":"2024-12-16T22:51:38.961+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Trigger CI when a PR is marked \"ready for review\"\n\n\n\nConversation:\nuser: Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","output":"Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","metadata":{"issue_id":"KAFKA-18271","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Minor","assignee":null,"reporter":{"displayName":"David Arthur"},"created":"2024-12-16T19:39:46.000+0000","updated":"2024-12-16T22:51:38.000+0000","resolved":null,"labels":[],"components":["build"],"comment_count":1,"comments":[{"id":"17906215","author":{"displayName":"黃竣陽"},"body":"Hello, davidarthur , if you wont work on this, may I take the issue? Thank you","body_raw":"Hello, [~davidarthur]  , if you wont work on this, may I take the issue? Thank you","created":"2024-12-16T22:51:38.961+0000","updated":"2024-12-16T22:51:38.961+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add metric for log cleaner thread busy percentage\n\n*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, the user will notice an increase in `max-compaction-delay-secs` metric. However, an increase in this metric does not necessarily mean that the threads are overloaded. For example, this metric could be increasing due to all cleaner threads getting throttled. *Requirement* We want a mechanism to determine when the cleaner threads should be increased. *Proposal* Add a thread pool utilization metric for log cleaner thread busy percentage. This is similar to how to we have thread pool utilization metrics for io-threads, network-threads etc. When the metric is emitted, the metric will emit the number of threads which are actively doing some work ie are not sleeping at https://github.com/apache/kafka/blob/4aee33d6ab1345243e426e05388f6fc512970e93/core/src/main/scala/kafka/log/LogCleaner.scala#L387 *Note that this Jira required a KIP since we are adding a new metric*","output":"Add metric for log cleaner thread busy percentage","metadata":{"issue_id":"KAFKA-18268","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jason Taylor"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T16:30:42.000+0000","updated":"2024-12-16T16:56:03.000+0000","resolved":null,"labels":["kip"],"components":["log cleaner"],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add metric for log cleaner thread busy percentage\n\n*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, the user will notice an increase in `max-compaction-delay-secs` metric. However, an increase in this metric does not necessarily mean that the threads are overloaded. For example, this metric could be ","output":"Open","metadata":{"issue_id":"KAFKA-18268","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jason Taylor"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T16:30:42.000+0000","updated":"2024-12-16T16:56:03.000+0000","resolved":null,"labels":["kip"],"components":["log cleaner"],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add metric for log cleaner thread busy percentage\n\n*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, the user will notice an increase in `max-compaction-delay-secs` metric. However, an increase in this metric does not necessarily mean that the threads are overloaded. For example, this metric could be ","output":"Major","metadata":{"issue_id":"KAFKA-18268","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jason Taylor"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T16:30:42.000+0000","updated":"2024-12-16T16:56:03.000+0000","resolved":null,"labels":["kip"],"components":["log cleaner"],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add metric for log cleaner thread busy percentage\n\n*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, the user will notice an increase in `max-compaction-delay-secs` metric. However, an increase in this metric does not necessarily mean that the threads are overloaded. For example, this metric could be ","output":"*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, t","metadata":{"issue_id":"KAFKA-18268","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jason Taylor"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T16:30:42.000+0000","updated":"2024-12-16T16:56:03.000+0000","resolved":null,"labels":["kip"],"components":["log cleaner"],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"*Background* The number of cleaner threads (responsible for cleaning up/compacting topics which contains \"compact\") is configured using [https://kafka.apache.org/documentation.html#brokerconfigs_log.cleaner.threads] *Problem* When the number of threads is in-adequate to handle the compaction load, the user will notice an increase in `max-compaction-delay-secs` metric. However, an increase in this metric does not necessarily mean that the threads are overloaded. For example, this metric could be increasing due to all cleaner threads getting throttled. *Requirement* We want a mechanism to determine when the cleaner threads should be increased. *Proposal* Add a thread pool utilization metric for log cleaner thread busy percentage. This is similar to how to we have thread pool utilization metr","output":"Add metric for log cleaner thread busy percentage","metadata":{"issue_id":"KAFKA-18268","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Open","priority":"Major","assignee":{"displayName":"Jason Taylor"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T16:30:42.000+0000","updated":"2024-12-16T16:56:03.000+0000","resolved":null,"labels":["kip"],"components":["log cleaner"],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L83-L88] because the former validation performs a read from mmap and hence, is more expensive compared to the others. In best case scenarios, mmap is completely mapped to memory and hence, the lookup is cheap but in worst case scenario, OS will load the data we want to read from mmap on-demand. Hence, the expense.","output":"Re-order validation for TimeIndex sanity check","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/st","output":"Resolved","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/st","output":"Minor","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/st","output":"Currently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/st","output":"Not A Problem","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Re-order validation for TimeIndex sanity check\n\nCurrently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., \n\nConversation:\nuser: Hi jaytee , if you're not working on this, may I take it? Thank you.\nassistant: Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","output":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Currently, when validating the sanity of TimeIndex, we perform multiple validations. With this change, we want to re-order the validations such that the expensive ones are performed at the end. i.e., we want to do [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L79] after [https://github.com/apache/kafka/blob/9cc1547672a8b261c08f453f45277265dfb44808/storage/src/main/java/org/apache/kafka/storage/internals/log/TimeIndex.java#L83-L88] because the former validation performs a read from mmap and hence, is more expensive compared to the others. In best case scenarios, mmap is completely mapped to memory and hence, the lookup is cheap but in worst case scenario, OS will load the data we wan","output":"Re-order validation for TimeIndex sanity check","metadata":{"issue_id":"KAFKA-18266","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Pramithas Dhakal"},"reporter":{"displayName":"Divij Vaidya"},"created":"2024-12-16T15:32:17.000+0000","updated":"2025-01-20T15:26:56.000+0000","resolved":"2025-01-20T15:26:56.000+0000","resolution":"Not A Problem","labels":["newbie"],"components":[],"comment_count":2,"comments":[{"id":"17911776","author":{"displayName":"Pramithas Dhakal"},"body":"Hi jaytee , if you're not working on this, may I take it? Thank you.","body_raw":"Hi [~jaytee] , if you're not working on this, may I take it? Thank you.\r\n \r\n ","created":"2025-01-10T05:37:56.073+0000","updated":"2025-01-10T05:37:56.073+0000","updateAuthor":{"displayName":"Pramithas Dhakal"}},{"id":"17912012","author":{"displayName":"Jason Taylor"},"body":"Sure pramithas i'll reassign it to you as I still have other open PR's i need to prioritise. Weirdly it won't let me assign it to you. divijvaidya can u reassign please?","body_raw":"Sure [~pramithas] i'll reassign it to you as I still have other open PR's i need to prioritise.\r\n\r\nWeirdly it won't let me assign it to you. [~divijvaidya] can u reassign please?","created":"2025-01-10T14:54:16.156+0000","updated":"2025-01-10T14:56:12.899+0000","updateAuthor":{"displayName":"Jason Taylor"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove NotLeaderForPartitionException\n\nIt is deprecated by KAFKA-10223 (4 years ago)","output":"Resolved","metadata":{"issue_id":"KAFKA-18264","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T09:36:50.000+0000","updated":"2024-12-18T17:00:26.000+0000","resolved":"2024-12-18T17:00:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905956","author":{"displayName":"Nick Guo"},"body":"Hi chia7712 ,if you are not working on this,may I take over this?","body_raw":"Hi [~chia7712] ,if you are not working on this,may I take over this?","created":"2024-12-16T09:42:00.270+0000","updated":"2024-12-16T09:42:00.270+0000","updateAuthor":{"displayName":"Nick Guo"}},{"id":"17906804","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","body_raw":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6]\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","created":"2024-12-18T17:00:26.318+0000","updated":"2024-12-18T17:00:26.318+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove NotLeaderForPartitionException\n\nIt is deprecated by KAFKA-10223 (4 years ago)","output":"Minor","metadata":{"issue_id":"KAFKA-18264","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T09:36:50.000+0000","updated":"2024-12-18T17:00:26.000+0000","resolved":"2024-12-18T17:00:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905956","author":{"displayName":"Nick Guo"},"body":"Hi chia7712 ,if you are not working on this,may I take over this?","body_raw":"Hi [~chia7712] ,if you are not working on this,may I take over this?","created":"2024-12-16T09:42:00.270+0000","updated":"2024-12-16T09:42:00.270+0000","updateAuthor":{"displayName":"Nick Guo"}},{"id":"17906804","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","body_raw":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6]\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","created":"2024-12-18T17:00:26.318+0000","updated":"2024-12-18T17:00:26.318+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove NotLeaderForPartitionException\n\nIt is deprecated by KAFKA-10223 (4 years ago)","output":"It is deprecated by KAFKA-10223 (4 years ago)","metadata":{"issue_id":"KAFKA-18264","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T09:36:50.000+0000","updated":"2024-12-18T17:00:26.000+0000","resolved":"2024-12-18T17:00:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905956","author":{"displayName":"Nick Guo"},"body":"Hi chia7712 ,if you are not working on this,may I take over this?","body_raw":"Hi [~chia7712] ,if you are not working on this,may I take over this?","created":"2024-12-16T09:42:00.270+0000","updated":"2024-12-16T09:42:00.270+0000","updateAuthor":{"displayName":"Nick Guo"}},{"id":"17906804","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","body_raw":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6]\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","created":"2024-12-18T17:00:26.318+0000","updated":"2024-12-18T17:00:26.318+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove NotLeaderForPartitionException\n\nIt is deprecated by KAFKA-10223 (4 years ago)","output":"Fixed","metadata":{"issue_id":"KAFKA-18264","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T09:36:50.000+0000","updated":"2024-12-18T17:00:26.000+0000","resolved":"2024-12-18T17:00:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905956","author":{"displayName":"Nick Guo"},"body":"Hi chia7712 ,if you are not working on this,may I take over this?","body_raw":"Hi [~chia7712] ,if you are not working on this,may I take over this?","created":"2024-12-16T09:42:00.270+0000","updated":"2024-12-16T09:42:00.270+0000","updateAuthor":{"displayName":"Nick Guo"}},{"id":"17906804","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","body_raw":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6]\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","created":"2024-12-18T17:00:26.318+0000","updated":"2024-12-18T17:00:26.318+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove NotLeaderForPartitionException\n\nIt is deprecated by KAFKA-10223 (4 years ago)\n\nConversation:\nuser: Hi chia7712 ,if you are not working on this,may I take over this?\nassistant: trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","output":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","metadata":{"issue_id":"KAFKA-18264","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Minor","assignee":{"displayName":"Nick Guo"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T09:36:50.000+0000","updated":"2024-12-18T17:00:26.000+0000","resolved":"2024-12-18T17:00:26.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905956","author":{"displayName":"Nick Guo"},"body":"Hi chia7712 ,if you are not working on this,may I take over this?","body_raw":"Hi [~chia7712] ,if you are not working on this,may I take over this?","created":"2024-12-16T09:42:00.270+0000","updated":"2024-12-16T09:42:00.270+0000","updateAuthor":{"displayName":"Nick Guo"}},{"id":"17906804","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6] 4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","body_raw":"trunk: [https://github.com/apache/kafka/commit/21b7bb2265951d55efb8c0cb93340b664a5783a6]\r\n\r\n4.0: https://github.com/apache/kafka/commit/5a6e1204dd533af9f6f8072fe80330c5ea9f6fa7","created":"2024-12-18T17:00:26.318+0000","updated":"2024-12-18T17:00:26.318+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Group lock must be acquired when reverting static membership rejoin\n\nWhen a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"Group lock must be acquired when reverting static membership rejoin","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Group lock must be acquired when reverting static membership rejoin\n\nWhen a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"Resolved","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Group lock must be acquired when reverting static membership rejoin\n\nWhen a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"Major","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Group lock must be acquired when reverting static membership rejoin\n\nWhen a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"When a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from th","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Group lock must be acquired when reverting static membership rejoin\n\nWhen a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"Fixed","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"When a static member rejoins the group, the group state is rewritten to the partition in order to persist the change. If the write fails, the change is reverted. However, this is done without acquiring the group lock. This is only try in the old group coordinator. The new one does not suffer from this issue.","output":"Group lock must be acquired when reverting static membership rejoin","metadata":{"issue_id":"KAFKA-18263","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Major","assignee":{"displayName":"David Jacot"},"reporter":{"displayName":"David Jacot"},"created":"2024-12-16T08:49:24.000+0000","updated":"2024-12-16T17:25:24.000+0000","resolved":"2024-12-16T17:25:24.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Remove DefaultPartitioner and UniformStickyPartitioner","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Resolved","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Major","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"They have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.","output":"Fixed","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove DefaultPartitioner and UniformStickyPartitioner\n\nThey have been deprecated by KAFKA-10888 since 2022, so we should remove them from version 4.0.\n\nConversation:\nuser: Hi chia7712, if you're not working on this, may I take this Jira? Thanks.\nassistant: trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","output":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","metadata":{"issue_id":"KAFKA-18262","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"PoAn Yang"},"reporter":{"displayName":"Chia-Ping Tsai"},"created":"2024-12-16T03:15:57.000+0000","updated":"2024-12-20T07:40:45.000+0000","resolved":"2024-12-20T07:40:45.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905878","author":{"displayName":"PoAn Yang"},"body":"Hi chia7712, if you're not working on this, may I take this Jira? Thanks.","body_raw":"Hi [~chia7712], if you're not working on this, may I take this Jira? Thanks.","created":"2024-12-16T03:16:47.438+0000","updated":"2024-12-16T03:16:47.438+0000","updateAuthor":{"displayName":"PoAn Yang"}},{"id":"17907290","author":{"displayName":"Chia-Ping Tsai"},"body":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b] 4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","body_raw":"trunk: [https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b]\r\n\r\n \r\n\r\n4.0: https://github.com/apache/kafka/commit/753a00348057e43002dd205ec3ada4044edf145b","created":"2024-12-20T07:40:45.911+0000","updated":"2024-12-20T07:40:45.911+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"Remove TestUtils#createBrokerConfigs zkConnect argument","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"Resolved","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"Major","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"We already change all test to Kraft, thus we don't need this argument when we create the broker config","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"Fixed","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Remove TestUtils#createBrokerConfigs zkConnect argument\n\nWe already change all test to Kraft, thus we don't need this argument when we create the broker config\n\nConversation:\nuser: Hi m1a2st if you are not working on this,may I?\nassistant: Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","output":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"We already change all test to Kraft, thus we don't need this argument when we create the broker config","output":"Remove TestUtils#createBrokerConfigs zkConnect argument","metadata":{"issue_id":"KAFKA-18261","project":"KAFKA","project_name":"Kafka","issue_type":"Improvement","status":"Resolved","priority":"Major","assignee":{"displayName":"黃竣陽"},"reporter":{"displayName":"黃竣陽"},"created":"2024-12-16T01:24:15.000+0000","updated":"2025-02-05T11:54:03.000+0000","resolved":"2025-02-05T11:54:03.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905974","author":{"displayName":"Shonali K S"},"body":"Hi m1a2st if you are not working on this,may I?","body_raw":"Hi [~m1a2st] if you are not working on this,may I?","created":"2024-12-16T10:06:38.455+0000","updated":"2024-12-16T10:06:38.455+0000","updateAuthor":{"displayName":"Shonali K S"}},{"id":"17905975","author":{"displayName":"黃竣陽"},"body":"Hello shonali, I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","body_raw":"Hello [~shonali], I will work on this, I'm waiting for https://issues.apache.org/jira/browse/KAFKA-18226 merge to trunk","created":"2024-12-16T10:12:27.183+0000","updated":"2024-12-16T10:12:27.183+0000","updateAuthor":{"displayName":"黃竣陽"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"Resolved","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"Major","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"I suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"Fixed","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration\n\nI suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a \n\nConversation:\nuser: Hi schofielaj I would like to give it a try 😀\nassistant: discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","output":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"I suggest the test sets the duration to a value which is known to be the earliest, such as `PT1H`. We could try more subtle durations but the tests are likely to be flaky, so I favour starting with a known earliest duration.","output":"Add integration test to ShareConsumerTest for share.auto.offset.reset=by_duration","metadata":{"issue_id":"KAFKA-18260","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Chu Cheng Li"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:17:19.000+0000","updated":"2024-12-18T21:48:12.000+0000","resolved":"2024-12-18T21:48:12.000+0000","resolution":"Fixed","labels":[],"components":[],"comment_count":2,"comments":[{"id":"17905848","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj I would like to give it a try 😀","body_raw":"Hi [~schofielaj] \r\n\r\nI would like to give it a try 😀","created":"2024-12-15T23:30:26.948+0000","updated":"2024-12-15T23:30:26.948+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905851","author":{"displayName":"Chia-Ping Tsai"},"body":"discussed with frankvicky and peterxcli - it is up to peterxcli to complete the test cases :)","body_raw":"discussed with [~frankvicky] and [~peterxcli] - it is up to [~peterxcli] to complete the test cases :)","created":"2024-12-16T00:56:33.785+0000","updated":"2024-12-16T00:56:33.785+0000","updateAuthor":{"displayName":"Chia-Ping Tsai"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"Documentation for consumer auto.offset.reset contains invalid HTML","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"Resolved","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"Minor","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"The auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` c","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"Fixed","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"conversation","instruction":"Continue the conversation about this technical issue:","input":"Documentation for consumer auto.offset.reset contains invalid HTML\n\nThe auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be\n\nConversation:\nuser: Hi schofielaj May I take over this issue ?\nassistant: frankvicky Please go ahead. Thanks.","output":"frankvicky Please go ahead. Thanks.","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"The auto.offset.reset config now permits `by_duration: `. However, the documentation for this config needs some care because it is turned into HTML. In particular, the ` ` is not allowed and should be ` {noformat} &lt;duration&gt; {noformat} ` instead. Also, the `AutoOffsetResetStrategy.Validator` class needs a `toString()` method since the documentation will contain the result of calling that method on the validator.","output":"Documentation for consumer auto.offset.reset contains invalid HTML","metadata":{"issue_id":"KAFKA-18259","project":"KAFKA","project_name":"Kafka","issue_type":"Bug","status":"Resolved","priority":"Minor","assignee":{"displayName":"TengYao Chi"},"reporter":{"displayName":"Andrew Schofield"},"created":"2024-12-15T22:04:04.000+0000","updated":"2024-12-16T17:48:11.000+0000","resolved":"2024-12-16T17:48:11.000+0000","resolution":"Fixed","labels":[],"components":["clients"],"comment_count":2,"comments":[{"id":"17905847","author":{"displayName":"TengYao Chi"},"body":"Hi schofielaj May I take over this issue ?","body_raw":"Hi [~schofielaj] \r\n\r\nMay I take over this issue ?","created":"2024-12-15T23:28:43.083+0000","updated":"2024-12-15T23:28:43.083+0000","updateAuthor":{"displayName":"TengYao Chi"}},{"id":"17905893","author":{"displayName":"Manikumar"},"body":"frankvicky Please go ahead. Thanks.","body_raw":"[~frankvicky] Please go ahead. Thanks. ","created":"2024-12-16T05:43:40.217+0000","updated":"2024-12-16T05:43:40.217+0000","updateAuthor":{"displayName":"Manikumar"}}]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Code Review\n\nConduct a code review of all changes related to the RocksDB upgrade.","output":"Code Review","metadata":{"issue_id":"KAFKA-18258","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:48:07.000+0000","updated":"2025-01-04T04:42:16.000+0000","resolved":"2025-01-04T04:40:26.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Code Review\n\nConduct a code review of all changes related to the RocksDB upgrade.","output":"Resolved","metadata":{"issue_id":"KAFKA-18258","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:48:07.000+0000","updated":"2025-01-04T04:42:16.000+0000","resolved":"2025-01-04T04:40:26.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Code Review\n\nConduct a code review of all changes related to the RocksDB upgrade.","output":"Major","metadata":{"issue_id":"KAFKA-18258","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:48:07.000+0000","updated":"2025-01-04T04:42:16.000+0000","resolved":"2025-01-04T04:40:26.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Code Review\n\nConduct a code review of all changes related to the RocksDB upgrade.","output":"Conduct a code review of all changes related to the RocksDB upgrade.","metadata":{"issue_id":"KAFKA-18258","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:48:07.000+0000","updated":"2025-01-04T04:42:16.000+0000","resolved":"2025-01-04T04:40:26.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Code Review\n\nConduct a code review of all changes related to the RocksDB upgrade.","output":"Done","metadata":{"issue_id":"KAFKA-18258","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Matthias J. Sax"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:48:07.000+0000","updated":"2025-01-04T04:42:16.000+0000","resolved":"2025-01-04T04:40:26.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"summarization","instruction":"Summarize the following technical issue in 2-3 sentences:","input":"Update Documentation\n\nUpdate any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Update Documentation","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"classification","instruction":"Classify the status of this issue based on its description:","input":"Update Documentation\n\nUpdate any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Resolved","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"priority_classification","instruction":"Determine the priority level of this issue:","input":"Update Documentation\n\nUpdate any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Major","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"question_answering","instruction":"Answer the following question about this issue:","input":"What is the main problem described in this issue?\n\nIssue: Update Documentation\n\nUpdate any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Update any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"resolution_prediction","instruction":"Based on the issue description, predict if it would be resolved:","input":"Update Documentation\n\nUpdate any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Done","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
{"task_type":"extraction","instruction":"Extract key technical details from this issue:","input":"Update any relevant Kafka documentation to reflect the RocksDB upgrade and any API changes that might affect users or developers.","output":"Update Documentation","metadata":{"issue_id":"KAFKA-18257","project":"KAFKA","project_name":"Kafka","issue_type":"Sub-task","status":"Resolved","priority":"Major","assignee":{"displayName":"Swikar Patel"},"reporter":{"displayName":"Swikar Patel"},"created":"2024-12-15T17:47:34.000+0000","updated":"2025-01-04T04:39:42.000+0000","resolved":"2025-01-04T04:39:42.000+0000","resolution":"Done","labels":[],"components":[],"comment_count":0,"comments":[]}}
